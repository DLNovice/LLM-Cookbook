{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Part 5: 解码器 (Decoder)\n",
    "\n",
    "本notebook实现Transformer的解码器结构，包括：\n",
    "1. 解码器层（Decoder Layer）\n",
    "2. 带掩码的多头自注意力\n",
    "3. 编码器-解码器注意力\n",
    "4. 完整解码器（Decoder）\n",
    "5. 与PyTorch官方实现的对比\n",
    "6. 注意力权重可视化\n",
    "\n",
    "解码器是Transformer的另一个核心组件，负责生成输出序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 解码器层 (Decoder Layer)\n",
    "\n",
    "**原论文描述**：\n",
    "\"In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\"\n",
    "\n",
    "**架构**：\n",
    "1. 带掩码的多头自注意力（Masked Multi-Head Self-Attention）\n",
    "2. 编码器-解码器注意力（Encoder-Decoder Attention）\n",
    "3. 位置前馈网络（Position-wise Feed-Forward Network）\n",
    "\n",
    "**数学公式**（来自原论文）：\n",
    "\n",
    "1. 掩码自注意力：$LayerNorm(x + MaskedMultiHeadAttention(x, x, x))$\n",
    "2. 编码器-解码器注意力：$LayerNorm(x + MultiHeadAttention(x, encoder_output, encoder_output))$\n",
    "3. 前馈子层：$LayerNorm(x + FeedForward(x))$\n",
    "\n",
    "**特点**：\n",
    "- 使用掩码防止信息泄露\n",
    "- 三个子层，每个都有残差连接和层归一化\n",
    "- 编码器-解码器注意力让解码器关注输入序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试解码器层...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 8, 8, 64]' is invalid for input of size 10240",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 222\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, self_attn_weights, encoder_attn_weights\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# 运行测试\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m output, self_attn_weights, encoder_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtest_decoder_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 195\u001b[0m, in \u001b[0;36mtest_decoder_layer\u001b[1;34m()\u001b[0m\n\u001b[0;32m    192\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m tgt_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [1, 1, tgt_len, tgt_len]\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m output, self_attn_weights, encoder_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m目标输入形状: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m编码器输出形状: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 155\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m    152\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attn_output1)  \u001b[38;5;66;03m# 残差连接 + 层归一化\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# 2. 编码器-解码器注意力子层\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m attn_output2, encoder_attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_decoder_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m attn_output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output2)\n\u001b[0;32m    159\u001b[0m out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(out1 \u001b[38;5;241m+\u001b[39m attn_output2)  \u001b[38;5;66;03m# 残差连接 + 层归一化\u001b[39;00m\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 线性变换\u001b[39;00m\n\u001b[0;32m     50\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_q(query)\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_k\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     52\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_v(value)\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 注意力计算\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 8, 8, 64]' is invalid for input of size 10240"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力机制（简化版，用于测试）\n",
    "    \n",
    "    参数:\n",
    "        d_model: 模型维度\n",
    "        n_heads: 注意力头数\n",
    "        dropout: dropout概率\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # 线性变换层\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        缩放点积注意力\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # 线性变换\n",
    "        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 注意力计算\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 合并多头\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # 输出线性变换\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    位置前馈网络实现\n",
    "    \n",
    "    参数:\n",
    "        d_model: 模型维度\n",
    "        d_ff: 前馈网络隐藏层维度\n",
    "        dropout: dropout概率\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        # 第一层线性变换：d_model -> d_ff\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # 第二层线性变换：d_ff -> d_model\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量，形状为 [batch_size, seq_len, d_model]\n",
    "        \n",
    "        返回:\n",
    "            输出张量，形状与输入相同\n",
    "        \"\"\"\n",
    "        # 线性变换 + ReLU激活 + dropout + 线性变换\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    解码器层实现\n",
    "    \n",
    "    参数:\n",
    "        d_model: 模型维度\n",
    "        n_heads: 注意力头数\n",
    "        d_ff: 前馈网络隐藏层维度\n",
    "        dropout: dropout概率\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # 掩码多头自注意力\n",
    "        self.masked_self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # 编码器-解码器注意力\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # 位置前馈网络\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def create_padding_mask(self, seq_len):\n",
    "        \"\"\"\n",
    "        创建填充掩码\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        return mask == 0\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "            x: 目标序列输入，形状为 [batch_size, tgt_len, d_model]\n",
    "            encoder_output: 编码器输出，形状为 [batch_size, src_len, d_model]\n",
    "            src_mask: 源序列掩码\n",
    "            tgt_mask: 目标序列掩码（用于防止信息泄露）\n",
    "        \n",
    "        返回:\n",
    "            输出张量和注意力权重\n",
    "        \"\"\"\n",
    "        # 1. 掩码自注意力子层\n",
    "        attn_output1, self_attention_weights = self.masked_self_attention(x, x, x, tgt_mask)\n",
    "        attn_output1 = self.dropout(attn_output1)\n",
    "        out1 = self.norm1(x + attn_output1)  # 残差连接 + 层归一化\n",
    "        \n",
    "        # 2. 编码器-解码器注意力子层\n",
    "        attn_output2, encoder_attention_weights = self.encoder_decoder_attention(\n",
    "            out1, encoder_output, encoder_output, src_mask\n",
    "        )\n",
    "        attn_output2 = self.dropout(attn_output2)\n",
    "        out2 = self.norm2(out1 + attn_output2)  # 残差连接 + 层归一化\n",
    "        \n",
    "        # 3. 前馈子层\n",
    "        ff_output = self.feed_forward(out2)\n",
    "        ff_output = self.dropout(ff_output)\n",
    "    \n",
    "        output = self.norm3(out2 + ff_output)  # 残差连接 + 层归一化\n",
    "        \n",
    "        return output, self_attention_weights, encoder_attention_weights\n",
    "\n",
    "def test_decoder_layer():\n",
    "    \"\"\"\n",
    "    测试解码器层\n",
    "    \"\"\"\n",
    "    print(\"测试解码器层...\")\n",
    "    \n",
    "    # 参数设置\n",
    "    batch_size = 2\n",
    "    tgt_len = 8  # 目标序列长度\n",
    "    src_len = 10  # 源序列长度\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    \n",
    "    # 创建解码器层\n",
    "    decoder_layer = DecoderLayer(d_model, n_heads, d_ff)\n",
    "    \n",
    "    # 创建测试输入\n",
    "    tgt_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "    \n",
    "    # 创建目标序列掩码（防止信息泄露）\n",
    "    tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1) == 0\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, tgt_len, tgt_len]\n",
    "    \n",
    "    # 前向传播\n",
    "    output, self_attn_weights, encoder_attn_weights = decoder_layer(\n",
    "        tgt_input, encoder_output, tgt_mask=tgt_mask\n",
    "    )\n",
    "    \n",
    "    print(f\"目标输入形状: {tgt_input.shape}\")\n",
    "    print(f\"编码器输出形状: {encoder_output.shape}\")\n",
    "    print(f\"解码器输出形状: {output.shape}\")\n",
    "    print(f\"自注意力权重形状: {self_attn_weights.shape}\")\n",
    "    print(f\"编码器注意力权重形状: {encoder_attn_weights.shape}\")\n",
    "    \n",
    "    # 验证输出形状\n",
    "    assert output.shape == tgt_input.shape, f\"输出形状错误: {output.shape}\"\n",
    "    print(\"✓ 形状测试通过\")\n",
    "    \n",
    "    # 验证注意力权重形状\n",
    "    expected_self_attn_shape = (batch_size, n_heads, tgt_len, tgt_len)\n",
    "    expected_encoder_attn_shape = (batch_size, n_heads, tgt_len, src_len)\n",
    "    \n",
    "    assert self_attn_weights.shape == expected_self_attn_shape, f\"自注意力权重形状错误: {self_attn_weights.shape}\"\n",
    "    assert encoder_attn_weights.shape == expected_encoder_attn_shape, f\"编码器注意力权重形状错误: {encoder_attn_weights.shape}\"\n",
    "    print(\"✓ 注意力权重形状正确\")\n",
    "    \n",
    "    print(\"✓ 解码器层测试完成\")\n",
    "    \n",
    "    return output, self_attn_weights, encoder_attn_weights\n",
    "\n",
    "# 运行测试\n",
    "output, self_attn_weights, encoder_attn_weights = test_decoder_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 掩码机制详解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可视化掩码机制...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 26597 (\\N{CJK UNIFIED IDEOGRAPH-67E5}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 35810 (\\N{CJK UNIFIED IDEOGRAPH-8BE2}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 20301 (\\N{CJK UNIFIED IDEOGRAPH-4F4D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 30446 (\\N{CJK UNIFIED IDEOGRAPH-76EE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 24207 (\\N{CJK UNIFIED IDEOGRAPH-5E8F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 21015 (\\N{CJK UNIFIED IDEOGRAPH-5217}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 25513 (\\N{CJK UNIFIED IDEOGRAPH-63A9}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 30721 (\\N{CJK UNIFIED IDEOGRAPH-7801}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 38450 (\\N{CJK UNIFIED IDEOGRAPH-9632}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 27490 (\\N{CJK UNIFIED IDEOGRAPH-6B62}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 20449 (\\N{CJK UNIFIED IDEOGRAPH-4FE1}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 24687 (\\N{CJK UNIFIED IDEOGRAPH-606F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 27844 (\\N{CJK UNIFIED IDEOGRAPH-6CC4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 38706 (\\N{CJK UNIFIED IDEOGRAPH-9732}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 38190 (\\N{CJK UNIFIED IDEOGRAPH-952E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 38190 (\\N{CJK UNIFIED IDEOGRAPH-952E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 20301 (\\N{CJK UNIFIED IDEOGRAPH-4F4D}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 26410 (\\N{CJK UNIFIED IDEOGRAPH-672A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 20449 (\\N{CJK UNIFIED IDEOGRAPH-4FE1}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 24687 (\\N{CJK UNIFIED IDEOGRAPH-606F}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 26597 (\\N{CJK UNIFIED IDEOGRAPH-67E5}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 35810 (\\N{CJK UNIFIED IDEOGRAPH-8BE2}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 24403 (\\N{CJK UNIFIED IDEOGRAPH-5F53}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 21069 (\\N{CJK UNIFIED IDEOGRAPH-524D}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 25513 (\\N{CJK UNIFIED IDEOGRAPH-63A9}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 30721 (\\N{CJK UNIFIED IDEOGRAPH-7801}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 29702 (\\N{CJK UNIFIED IDEOGRAPH-7406}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 35828 (\\N{CJK UNIFIED IDEOGRAPH-8BF4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12104\\1624701367.py:69: UserWarning: Glyph 26126 (\\N{CJK UNIFIED IDEOGRAPH-660E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 26597 (\\N{CJK UNIFIED IDEOGRAPH-67E5}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 35810 (\\N{CJK UNIFIED IDEOGRAPH-8BE2}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 20301 (\\N{CJK UNIFIED IDEOGRAPH-4F4D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 30446 (\\N{CJK UNIFIED IDEOGRAPH-76EE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 24207 (\\N{CJK UNIFIED IDEOGRAPH-5E8F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 21015 (\\N{CJK UNIFIED IDEOGRAPH-5217}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 25513 (\\N{CJK UNIFIED IDEOGRAPH-63A9}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 30721 (\\N{CJK UNIFIED IDEOGRAPH-7801}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 38450 (\\N{CJK UNIFIED IDEOGRAPH-9632}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 27490 (\\N{CJK UNIFIED IDEOGRAPH-6B62}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 20449 (\\N{CJK UNIFIED IDEOGRAPH-4FE1}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 24687 (\\N{CJK UNIFIED IDEOGRAPH-606F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 27844 (\\N{CJK UNIFIED IDEOGRAPH-6CC4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 38706 (\\N{CJK UNIFIED IDEOGRAPH-9732}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 38190 (\\N{CJK UNIFIED IDEOGRAPH-952E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 24403 (\\N{CJK UNIFIED IDEOGRAPH-5F53}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 21069 (\\N{CJK UNIFIED IDEOGRAPH-524D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 29702 (\\N{CJK UNIFIED IDEOGRAPH-7406}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 35828 (\\N{CJK UNIFIED IDEOGRAPH-8BF4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 26126 (\\N{CJK UNIFIED IDEOGRAPH-660E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 26410 (\\N{CJK UNIFIED IDEOGRAPH-672A}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "掩码机制说明:\n",
      "- 白色/浅色: 允许关注 (值为1)\n",
      "- 深色: 屏蔽 (值为0)\n",
      "- 对角线及以上: 当前位置和未来位置\n",
      "- 对角线以下: 过去位置\n",
      "- 在生成任务中，模型只能关注已生成的部分\n",
      "\n",
      "掩码验证 (序列长度 8):\n",
      "位置 1 可以关注的键位置: [1]\n",
      "位置 2 可以关注的键位置: [1 2]\n",
      "位置 3 可以关注的键位置: [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "def create_masks(tgt_seq_len, src_seq_len, batch_size=1):\n",
    "    \"\"\"\n",
    "    创建各种掩码\n",
    "    \n",
    "    参数:\n",
    "        tgt_seq_len: 目标序列长度\n",
    "        src_seq_len: 源序列长度\n",
    "        batch_size: 批次大小\n",
    "    \n",
    "    返回:\n",
    "        tgt_mask: 目标序列掩码\n",
    "        src_mask: 源序列掩码\n",
    "    \"\"\"\n",
    "    # 创建目标序列掩码（下三角矩阵，防止信息泄露）\n",
    "    tgt_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len), diagonal=1) == 0\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, tgt_seq_len, tgt_seq_len]\n",
    "    \n",
    "    # 创建源序列掩码（这里假设没有填充，全部保留）\n",
    "    src_mask = torch.ones(batch_size, 1, 1, src_seq_len) == 1  # [batch_size, 1, 1, src_seq_len]\n",
    "    \n",
    "    return tgt_mask, src_mask\n",
    "\n",
    "def visualize_masks():\n",
    "    \"\"\"\n",
    "    可视化掩码\n",
    "    \"\"\"\n",
    "    print(\"可视化掩码机制...\")\n",
    "    \n",
    "    seq_len = 8\n",
    "    \n",
    "    # 创建目标序列掩码\n",
    "    tgt_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) == 0\n",
    "    \n",
    "    # 创建可视化\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # 子图1: 目标序列掩码\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(tgt_mask.numpy(), \n",
    "                cmap='Blues', \n",
    "                cbar=True, \n",
    "                square=True,\n",
    "                xticklabels=range(1, seq_len + 1),\n",
    "                yticklabels=range(1, seq_len + 1))\n",
    "    plt.title('目标序列掩码 (防止信息泄露)')\n",
    "    plt.xlabel('键位置')\n",
    "    plt.ylabel('查询位置')\n",
    "    \n",
    "    # 子图2: 说明\n",
    "    plt.subplot(1, 2, 2)\n",
    "    mask_explanation = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j <= i:\n",
    "                mask_explanation[i, j] = 1  # 允许关注\n",
    "            else:\n",
    "                mask_explanation[i, j] = 0  # 屏蔽\n",
    "    \n",
    "    sns.heatmap(mask_explanation, \n",
    "                cmap='RdYlBu', \n",
    "                cbar=True, \n",
    "                square=True,\n",
    "                xticklabels=range(1, seq_len + 1),\n",
    "                yticklabels=range(1, seq_len + 1))\n",
    "    plt.title('掩码原理说明')\n",
    "    plt.xlabel('键位置 (未来信息)')\n",
    "    plt.ylabel('查询位置 (当前位置)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 解释掩码机制\n",
    "    print(\"掩码机制说明:\")\n",
    "    print(\"- 白色/浅色: 允许关注 (值为1)\")\n",
    "    print(\"- 深色: 屏蔽 (值为0)\")\n",
    "    print(\"- 对角线及以上: 当前位置和未来位置\")\n",
    "    print(\"- 对角线以下: 过去位置\")\n",
    "    print(\"- 在生成任务中，模型只能关注已生成的部分\")\n",
    "    \n",
    "    # 验证掩码效果\n",
    "    print(f\"\\n掩码验证 (序列长度 {seq_len}):\")\n",
    "    for i in range(min(3, seq_len)):\n",
    "        row = tgt_mask[i].numpy()\n",
    "        allowed_positions = np.where(row == 1)[0] + 1\n",
    "        print(f\"位置 {i+1} 可以关注的键位置: {allowed_positions}\")\n",
    "\n",
    "# 运行可视化\n",
    "visualize_masks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 完整解码器 (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试完整解码器...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 8, 8, 64]' is invalid for input of size 10240",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, self_attn_weights_list, encoder_attn_weights_list\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# 运行测试\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m output, self_attn_weights_list, encoder_attn_weights_list \u001b[38;5;241m=\u001b[39m \u001b[43mtest_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 82\u001b[0m, in \u001b[0;36mtest_decoder\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m tgt_mask, src_mask \u001b[38;5;241m=\u001b[39m create_masks(tgt_len, src_len, batch_size)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m output, self_attn_weights_list, encoder_attn_weights_list \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m目标输入形状: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m编码器输出形状: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 45\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, tgt, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m tgt\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 45\u001b[0m     x, self_attn_weights, encoder_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     self_attention_weights_list\u001b[38;5;241m.\u001b[39mappend(self_attn_weights)\n\u001b[0;32m     49\u001b[0m     encoder_attention_weights_list\u001b[38;5;241m.\u001b[39mappend(encoder_attn_weights)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 155\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m    152\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attn_output1)  \u001b[38;5;66;03m# 残差连接 + 层归一化\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# 2. 编码器-解码器注意力子层\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m attn_output2, encoder_attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_decoder_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m attn_output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output2)\n\u001b[0;32m    159\u001b[0m out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(out1 \u001b[38;5;241m+\u001b[39m attn_output2)  \u001b[38;5;66;03m# 残差连接 + 层归一化\u001b[39;00m\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Application\\Anaconda\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 线性变换\u001b[39;00m\n\u001b[0;32m     50\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_q(query)\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_k\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     52\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_v(value)\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 注意力计算\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 8, 8, 64]' is invalid for input of size 10240"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    完整解码器实现\n",
    "    \n",
    "    参数:\n",
    "        d_model: 模型维度\n",
    "        n_heads: 注意力头数\n",
    "        d_ff: 前馈网络隐藏层维度\n",
    "        n_layers: 解码器层数\n",
    "        dropout: dropout概率\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, d_ff=2048, n_layers=6, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 解码器层堆叠\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "            tgt: 目标序列输入，形状为 [batch_size, tgt_len, d_model]\n",
    "            encoder_output: 编码器输出，形状为 [batch_size, src_len, d_model]\n",
    "            src_mask: 源序列掩码\n",
    "            tgt_mask: 目标序列掩码\n",
    "        \n",
    "        返回:\n",
    "            输出张量和所有层的注意力权重\n",
    "        \"\"\"\n",
    "        # 通过所有解码器层\n",
    "        self_attention_weights_list = []\n",
    "        encoder_attention_weights_list = []\n",
    "        \n",
    "        x = tgt\n",
    "        for layer in self.layers:\n",
    "            x, self_attn_weights, encoder_attn_weights = layer(\n",
    "                x, encoder_output, src_mask, tgt_mask\n",
    "            )\n",
    "            self_attention_weights_list.append(self_attn_weights)\n",
    "            encoder_attention_weights_list.append(encoder_attn_weights)\n",
    "        \n",
    "        # 最终层归一化\n",
    "        output = self.norm(x)\n",
    "        \n",
    "        return output, self_attention_weights_list, encoder_attention_weights_list\n",
    "\n",
    "def test_decoder():\n",
    "    \"\"\"\n",
    "    测试完整解码器\n",
    "    \"\"\"\n",
    "    print(\"测试完整解码器...\")\n",
    "    \n",
    "    # 参数设置\n",
    "    batch_size = 2\n",
    "    tgt_len = 8\n",
    "    src_len = 10\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    n_layers = 6\n",
    "    \n",
    "    # 创建解码器\n",
    "    decoder = Decoder(d_model, n_heads, d_ff, n_layers)\n",
    "    \n",
    "    # 创建测试输入\n",
    "    tgt_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "    \n",
    "    # 创建掩码\n",
    "    tgt_mask, src_mask = create_masks(tgt_len, src_len, batch_size)\n",
    "    \n",
    "    # 前向传播\n",
    "    output, self_attn_weights_list, encoder_attn_weights_list = decoder(\n",
    "        tgt_input, encoder_output, src_mask, tgt_mask\n",
    "    )\n",
    "    \n",
    "    print(f\"目标输入形状: {tgt_input.shape}\")\n",
    "    print(f\"编码器输出形状: {encoder_output.shape}\")\n",
    "    print(f\"解码器输出形状: {output.shape}\")\n",
    "    print(f\"自注意力权重层数: {len(self_attn_weights_list)}\")\n",
    "    print(f\"编码器注意力权重层数: {len(encoder_attn_weights_list)}\")\n",
    "    \n",
    "    # 验证输出形状\n",
    "    assert output.shape == tgt_input.shape, f\"输出形状错误: {output.shape}\"\n",
    "    print(\"✓ 形状测试通过\")\n",
    "    \n",
    "    # 验证注意力权重数量\n",
    "    assert len(self_attn_weights_list) == n_layers, f\"自注意力权重层数错误: {len(self_attn_weights_list)}\"\n",
    "    assert len(encoder_attn_weights_list) == n_layers, f\"编码器注意力权重层数错误: {len(encoder_attn_weights_list)}\"\n",
    "    print(\"✓ 注意力权重数量正确\")\n",
    "    \n",
    "    # 验证每层注意力权重形状\n",
    "    expected_self_attn_shape = (batch_size, n_heads, tgt_len, tgt_len)\n",
    "    expected_encoder_attn_shape = (batch_size, n_heads, tgt_len, src_len)\n",
    "    \n",
    "    for i, (self_attn, encoder_attn) in enumerate(zip(self_attn_weights_list, encoder_attn_weights_list)):\n",
    "        assert self_attn.shape == expected_self_attn_shape, f\"第{i}层自注意力权重形状错误: {self_attn.shape}\"\n",
    "        assert encoder_attn.shape == expected_encoder_attn_shape, f\"第{i}层编码器注意力权重形状错误: {encoder_attn.shape}\"\n",
    "    print(\"✓ 所有层注意力权重形状正确\")\n",
    "    \n",
    "    print(\"✓ 完整解码器测试完成\")\n",
    "    \n",
    "    return output, self_attn_weights_list, encoder_attn_weights_list\n",
    "\n",
    "# 运行测试\n",
    "output, self_attn_weights_list, encoder_attn_weights_list = test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 注意力权重可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decoder_attention():\n",
    "    \"\"\"\n",
    "    可视化解码器的注意力权重\n",
    "    \"\"\"\n",
    "    print(\"可视化解码器注意力权重...\")\n",
    "    \n",
    "    # 参数设置\n",
    "    batch_size = 1\n",
    "    tgt_len = 12\n",
    "    src_len = 16\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    n_layers = 1  # 只使用一层便于可视化\n",
    "    \n",
    "    # 创建单层解码器\n",
    "    decoder = Decoder(d_model, n_heads, d_ff, n_layers)\n",
    "    \n",
    "    # 创建测试输入\n",
    "    torch.manual_seed(42)\n",
    "    tgt_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "    \n",
    "    # 创建掩码\n",
    "    tgt_mask, src_mask = create_masks(tgt_len, src_len, batch_size)\n",
    "    \n",
    "    # 前向传播\n",
    "    output, self_attn_weights_list, encoder_attn_weights_list = decoder(\n",
    "        tgt_input, encoder_output, src_mask, tgt_mask\n",
    "    )\n",
    "    \n",
    "    # 获取注意力权重\n",
    "    self_attention_weights = self_attn_weights_list[0]  # [batch_size, n_heads, tgt_len, tgt_len]\n",
    "    encoder_attention_weights = encoder_attn_weights_list[0]  # [batch_size, n_heads, tgt_len, src_len]\n",
    "    \n",
    "    print(f\"自注意力权重形状: {self_attention_weights.shape}\")\n",
    "    print(f\"编码器注意力权重形状: {encoder_attention_weights.shape}\")\n",
    "    \n",
    "    # 绘制自注意力权重热力图\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 显示所有注意力头的自注意力\n",
    "    for head in range(n_heads):\n",
    "        plt.subplot(2, 4, head + 1)\n",
    "        \n",
    "        # 获取当前头的自注意力权重\n",
    "        head_self_attention = self_attention_weights[0, head].detach().numpy()\n",
    "        \n",
    "        # 绘制热力图\n",
    "        sns.heatmap(head_self_attention, \n",
    "                   cmap='Blues', \n",
    "                   cbar=True, \n",
    "                   square=True,\n",
    "                   xticklabels=range(1, tgt_len + 1),\n",
    "                   yticklabels=range(1, tgt_len + 1))\n",
    "        \n",
    "        plt.title(f'自注意力头 {head + 1}')\n",
    "        plt.xlabel('键位置')\n",
    "        plt.ylabel('查询位置')\n",
    "    \n",
    "    plt.suptitle('解码器自注意力权重', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 绘制编码器注意力权重热力图\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 显示所有注意力头的编码器注意力\n",
    "    for head in range(n_heads):\n",
    "        plt.subplot(2, 4, head + 1)\n",
    "        \n",
    "        # 获取当前头的编码器注意力权重\n",
    "        head_encoder_attention = encoder_attention_weights[0, head].detach().numpy()\n",
    "        \n",
    "        # 绘制热力图\n",
    "        sns.heatmap(head_encoder_attention, \n",
    "                   cmap='Reds', \n",
    "                   cbar=True, \n",
    "                   square=True,\n",
    "                   xticklabels=range(1, src_len + 1),\n",
    "                   yticklabels=range(1, tgt_len + 1))\n",
    "        \n",
    "        plt.title(f'编码器注意力头 {head + 1}')\n",
    "        plt.xlabel('源序列位置')\n",
    "        plt.ylabel('目标序列位置')\n",
    "    \n",
    "    plt.suptitle('解码器编码器注意力权重', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 分析注意力模式\n",
    "    print(\"分析注意力模式...\")\n",
    "    \n",
    "    # 分析自注意力\n",
    "    print(\"自注意力分析:\")\n",
    "    for head in range(n_heads):\n",
    "        head_attention = self_attention_weights[0, head].detach()\n",
    "        \n",
    "        # 计算注意力权重的统计特性\n",
    "        mean_attention = head_attention.mean().item()\n",
    "        std_attention = head_attention.std().item()\n",
    "        \n",
    "        # 检查掩码效果\n",
    "        upper_triangle = torch.triu(head_attention, diagonal=1)\n",
    "        upper_mean = upper_triangle.mean().item()\n",
    "        \n",
    "        print(f\"自注意力头 {head + 1}:\")\n",
    "        print(f\"  均值: {mean_attention:.4f}\")\n",
    "        print(f\"  标准差: {std_attention:.4f}\")\n",
    "        print(f\"  上三角均值 (应接近0): {upper_mean:.6f}\")\n",
    "    \n",
    "    # 分析编码器注意力\n",
    "    print(\"\\n编码器注意力分析:\")\n",
    "    for head in range(n_heads):\n",
    "        head_attention = encoder_attention_weights[0, head].detach()\n",
    "        \n",
    "        # 计算注意力权重的统计特性\n",
    "        mean_attention = head_attention.mean().item()\n",
    "        std_attention = head_attention.std().item()\n",
    "        \n",
    "        print(f\"编码器注意力头 {head + 1}:\")\n",
    "        print(f\"  均值: {mean_attention:.4f}\")\n",
    "        print(f\"  标准差: {std_attention:.4f}\")\n",
    "    \n",
    "    print(\"✓ 解码器注意力权重可视化完成\")\n",
    "\n",
    "# 运行可视化\n",
    "visualize_decoder_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 与PyTorch官方实现对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_pytorch_transformer():\n",
    "    \"\"\"\n",
    "    与PyTorch官方Transformer解码器对比\n",
    "    \"\"\"\n",
    "    print(\"与PyTorch官方Transformer解码器对比...\")\n",
    "    \n",
    "    # 参数设置\n",
    "    batch_size = 2\n",
    "    tgt_len = 8\n",
    "    src_len = 10\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    n_layers = 6\n",
    "    \n",
    "    # 创建我们的解码器\n",
    "    our_decoder = Decoder(d_model, n_heads, d_ff, n_layers)\n",
    "    \n",
    "    # 创建PyTorch官方Transformer解码器\n",
    "    pytorch_decoder_layer = nn.TransformerDecoderLayer(\n",
    "        d_model=d_model,\n",
    "        nhead=n_heads,\n",
    "        dim_feedforward=d_ff,\n",
    "        dropout=0.1,\n",
    "        activation='relu'\n",
    "    )\n",
    "    pytorch_decoder = nn.TransformerDecoder(pytorch_decoder_layer, num_layers=n_layers)\n",
    "    \n",
    "    # 创建测试输入\n",
    "    torch.manual_seed(42)\n",
    "    tgt_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "    \n",
    "    print(f\"目标输入形状: {tgt_input.shape}\")\n",
    "    print(f\"编码器输出形状: {encoder_output.shape}\")\n",
    "    \n",
    "    # 创建掩码\n",
    "    tgt_mask, src_mask = create_masks(tgt_len, src_len, batch_size)\n",
    "    \n",
    "    # 我们的解码器前向传播\n",
    "    our_output, our_self_attn_weights, our_encoder_attn_weights = our_decoder(\n",
    "        tgt_input, encoder_output, src_mask, tgt_mask\n",
    "    )\n",
    "    \n",
    "    # PyTorch解码器前向传播\n",
    "    # 注意：PyTorch的Transformer期望输入形状为 (seq_len, batch_size, d_model)\n",
    "    pytorch_tgt_input = tgt_input.transpose(0, 1)  # 调整维度\n",
    "    pytorch_encoder_output = encoder_output.transpose(0, 1)  # 调整维度\n",
    "    \n",
    "    # 创建PyTorch风格的掩码\n",
    "    pytorch_tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1) == 0\n",
    "    pytorch_memory_mask = None  # 这里简化处理\n",
    "    \n",
    "    pytorch_output = pytorch_decoder(\n",
    "        pytorch_tgt_input,\n",
    "        pytorch_encoder_output,\n",
    "        tgt_mask=pytorch_tgt_mask,\n",
    "        memory_mask=pytorch_memory_mask\n",
    "    )\n",
    "    pytorch_output = pytorch_output.transpose(0, 1)  # 调整回 (batch_size, tgt_len, d_model)\n",
    "    \n",
    "    print(f\"我们的输出形状: {our_output.shape}\")\n",
    "    print(f\"PyTorch输出形状: {pytorch_output.shape}\")\n",
    "    \n",
    "    # 比较参数量\n",
    "    our_params = sum(p.numel() for p in our_decoder.parameters())\n",
    "    pytorch_params = sum(p.numel() for p in pytorch_decoder.parameters())\n",
    "    \n",
    "    print(f\"我们的参数量: {our_params:,}\")\n",
    "    print(f\"PyTorch参数量: {pytorch_params:,}\")\n",
    "    print(f\"参数差异: {abs(our_params - pytorch_params):,}\")\n",
    "    \n",
    "    # 比较输出范围\n",
    "    print(f\"我们的输出范围: [{our_output.min().item():.3f}, {our_output.max().item():.3f}]\")\n",
    "    print(f\"PyTorch输出范围: [{pytorch_output.min().item():.3f}, {pytorch_output.max().item():.3f}]\")\n",
    "    \n",
    "    # 由于初始化不同，输出会有差异，但应该在一个合理的范围内\n",
    "    # 这里我们主要比较架构和参数量的相似性\n",
    "    \n",
    "    print(\"✓ 与PyTorch官方实现对比完成\")\n",
    "    \n",
    "    return our_output, pytorch_output\n",
    "\n",
    "# 运行对比\n",
    "our_output, pytorch_output = compare_with_pytorch_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 序列生成示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence_example():\n",
    "    \"\"\"\n",
    "    序列生成示例\n",
    "    \"\"\"\n",
    "    print(\"序列生成示例...\")\n",
    "    \n",
    "    # 参数设置\n",
    "    batch_size = 1\n",
    "    max_len = 10\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    n_layers = 6\n",
    "    vocab_size = 1000\n",
    "    \n",
    "    # 创建解码器\n",
    "    decoder = Decoder(d_model, n_heads, d_ff, n_layers)\n",
    "    \n",
    "    # 创建输出投影层（将解码器输出转换为词汇表概率）\n",
    "    output_projection = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    # 模拟编码器输出（假设已经通过编码器处理）\n",
    "    src_len = 8\n",
    "    encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "    \n",
    "    # 模拟开始符号\n",
    "    start_token = torch.randn(1, 1, d_model)\n",
    "    \n",
    "    # 逐步生成序列\n",
    "    generated_tokens = [start_token]\n",
    "    all_attention_weights = []\n",
    "    \n",
    "    print(\"逐步生成序列:\")\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        # 构建当前输入\n",
    "        current_input = torch.cat(generated_tokens, dim=1)\n",
    "        current_len = current_input.size(1)\n",
    "        \n",
    "        # 创建目标序列掩码\n",
    "        tgt_mask = torch.triu(torch.ones(current_len, current_len), diagonal=1) == 0\n",
    "        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # 创建源序列掩码（简化处理）\n",
    "        src_mask = torch.ones(batch_size, 1, 1, src_len) == 1\n",
    "        \n",
    "        # 解码器前向传播\n",
    "        decoder_output, self_attn_weights, encoder_attn_weights = decoder(\n",
    "            current_input, encoder_output, src_mask, tgt_mask\n",
    "        )\n",
    "        \n",
    "        # 获取最后一个时间步的输出\n",
    "        last_output = decoder_output[:, -1, :]  # [batch_size, d_model]\n",
    "        \n",
    "        # 投影到词汇表\n",
    "        logits = output_projection(last_output)  # [batch_size, vocab_size]\n",
    "        \n",
    "        # 选择最高概率的token（简化处理，实际应该使用更复杂的策略）\n",
    "        next_token_id = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # 模拟token嵌入（实际应该使用嵌入层）\n",
    "        next_token = torch.randn(1, 1, d_model)\n",
    "        \n",
    "        # 添加到生成序列\n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # 保存注意力权重\n",
    "        all_attention_weights.append({\n",
    "            'self_attention': self_attn_weights[0],  # 只保存第一层\n",
    "            'encoder_attention': encoder_attn_weights[0]\n",
    "        })\n",
    "        \n",
    "        print(f\"步骤 {step + 1}: 生成 token ID {next_token_id.item()}\")\n",
    "        \n",
    "        # 检查是否生成了结束符号（简化处理）\n",
    "        if next_token_id.item() == 0:  # 假设0是结束符号\n",
    "            print(f\"在步骤 {step + 1} 生成了结束符号\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n生成了长度为 {len(generated_tokens)} 的序列\")\n",
    "    \n",
    "    # 可视化最后一步的注意力权重\n",
    "    if all_attention_weights:\n",
    "        last_attention = all_attention_weights[-1]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # 自注意力\n",
    "        plt.subplot(1, 2, 1)\n",
    "        # 只显示最后一个注意力头\n",
    "        self_attn = last_attention['self_attention'][0, -1].detach().numpy()  # 最后一个头\n",
    "        sns.heatmap(self_attn.reshape(1, -1), \n",
    "                   cmap='Blues', \n",
    "                   cbar=True, \n",
    "                   xticklabels=range(1, len(generated_tokens) + 1),\n",
    "                   yticklabels=['最后位置'])\n",
    "        plt.title('最后一步的自注意力')\n",
    "        plt.xlabel('历史位置')\n",
    "        \n",
    "        # 编码器注意力\n",
    "        plt.subplot(1, 2, 2)\n",
    "        encoder_attn = last_attention['encoder_attention'][0, -1].detach().numpy()  # 最后一个头\n",
    "        sns.heatmap(encoder_attn.reshape(1, -1), \n",
    "                   cmap='Reds', \n",
    "                   cbar=True, \n",
    "                   xticklabels=range(1, src_len + 1),\n",
    "                   yticklabels=['最后位置'])\n",
    "        plt.title('最后一步的编码器注意力')\n",
    "        plt.xlabel('源序列位置')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"✓ 序列生成示例完成\")\n",
    "    \n",
    "    return generated_tokens, all_attention_weights\n",
    "\n",
    "# 运行示例\n",
    "generated_tokens, all_attention_weights = generate_sequence_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 总结\n",
    "\n",
    "在这个notebook中，我们完整实现了Transformer的解码器结构：\n",
    "\n",
    "### 1. 解码器层（Decoder Layer）\n",
    "- **子层1**: 带掩码的多头自注意力\n",
    "- **子层2**: 编码器-解码器注意力\n",
    "- **子层3**: 位置前馈网络\n",
    "- **残差连接**: 每个子层后都有残差连接和层归一化\n",
    "\n",
    "### 2. 掩码机制\n",
    "- **目标序列掩码**: 防止解码器看到未来信息\n",
    "- **源序列掩码**: 处理变长输入序列\n",
    "- **下三角矩阵**: 确保自回归生成\n",
    "\n",
    "### 3. 完整解码器（Decoder）\n",
    "- **层数**: 6个相同的解码器层\n",
    "- **输入**: 目标序列（已包含位置编码的嵌入）\n",
    "- **输出**: 解码后的表示\n",
    "\n",
    "### 4. 关键特性\n",
    "1. **自回归生成**: 只能基于已生成的部分进行预测\n",
    "2. **编码器-解码器注意力**: 让解码器关注输入序列\n",
    "3. **多头机制**: 同时关注不同类型的信息\n",
    "4. **残差连接**: 解决梯度消失问题\n",
    "\n",
    "### 5. 与编码器的区别\n",
    "- **额外掩码**: 解码器需要掩码自注意力\n",
    "- **额外注意力**: 编码器-解码器注意力层\n",
    "- **生成模式**: 解码器用于生成，编码器用于理解\n",
    "\n",
    "### 6. 应用场景\n",
    "- **机器翻译**: 源语言 → 目标语言\n",
    "- **文本生成**: 根据上下文生成文本\n",
    "- **摘要生成**: 长文本 → 短摘要\n",
    "- **对话系统**: 生成回复\n",
    "\n",
    "### 下一步：\n",
    "\n",
    "接下来我们将实现完整的Transformer模型，将编码器和解码器组合起来，并添加输入嵌入、位置编码和输出层。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
