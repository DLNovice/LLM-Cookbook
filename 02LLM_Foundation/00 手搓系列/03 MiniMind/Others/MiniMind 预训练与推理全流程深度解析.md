# ğŸ“ MiniMind é¢„è®­ç»ƒä¸æ¨ç†å…¨æµç¨‹æ·±åº¦è§£æ

æœ¬æ–‡æ¡£è¯¦ç»†è®²è§£ MiniMind é¡¹ç›®ä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹æ¨ç†çš„å®Œæ•´è¿‡ç¨‹ï¼ŒåŠ›æ±‚é€šä¿—æ˜“æ‡‚ã€‚

---

## ğŸ“š ç¬¬ä¸€ç« ï¼šæ•°æ®çš„æ—…ç¨‹ï¼ˆä»æ–‡ä»¶åˆ°Tensorï¼‰

### 1.1 æ•°æ®æ ¼å¼é•¿ä»€ä¹ˆæ ·ï¼Ÿ

é¦–å…ˆçœ‹çœ‹ JSONL æ–‡ä»¶ä¸­çš„æ•°æ®ï¼ˆ[dataset/pretrain_hq.jsonl](dataset/pretrain_hq.jsonl)ï¼‰ï¼š

```json
{"text": "<|im_start|>é‰´åˆ«ä¸€ç»„ä¸­æ–‡æ–‡ç« çš„é£æ ¼...<|im_end|> <|im_start|>å¥½çš„ï¼Œç°åœ¨å¸®æˆ‘æŸ¥ä¸€ä¸‹..."}
```

**å…³é”®ç‚¹**ï¼š
- æ¯è¡Œæ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ª `text` å­—æ®µ
- `<|im_start|>` å’Œ `<|im_end|>` æ˜¯ç‰¹æ®Šæ ‡è®°ï¼ˆç±»ä¼¼èŠå¤©çš„å¼€å§‹å’Œç»“æŸç¬¦å·ï¼‰
- æ•°æ®æ˜¯å¤šè½®å¯¹è¯æ‹¼æ¥æˆçš„è¿ç»­æ–‡æœ¬

### 1.2 æ•°æ®åŠ è½½å™¨çš„å·¥ä½œåŸç†

åœ¨ [dataset/lm_dataset.py:10-49](dataset/lm_dataset.py#L10-L49) ä¸­ï¼Œ`PretrainDataset` ç±»è´Ÿè´£å¤„ç†æ•°æ®ï¼š

```python
class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)  # åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
```

**æ­¥éª¤1ï¼šåŠ è½½æ•°æ®** ([dataset/lm_dataset.py:17-24](dataset/lm_dataset.py#L17-L24))

```python
def load_data(self, path):
    samples = []
    with open(path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            data = json.loads(line.strip())  # è§£ææ¯ä¸€è¡ŒJSON
            samples.append(data)
    return samples
```

**æ­¥éª¤2ï¼šè·å–å•æ¡æ•°æ®** ([dataset/lm_dataset.py:29-49](dataset/lm_dataset.py#L29-L49))
```python
def __getitem__(self, index):
    sample = self.samples[index]

    # ç”¨ tokenizer æŠŠæ–‡æœ¬è½¬æˆæ•°å­—ï¼ˆtoken IDsï¼‰
    encoding = self.tokenizer(
        str(sample["text"]),
        max_length=self.max_length,     # è¶…è¿‡512å°±æˆªæ–­
        padding="max_length",            # ä¸å¤Ÿå°±å¡«å……
        truncation=True,
        return_tensors="pt",
    )
```

### 1.3 ç¥å¥‡çš„"é”™ä½"æŠ€å·§

è¿™æ˜¯è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒæŠ€å·§ï¼ˆ[dataset/lm_dataset.py:41-49](dataset/lm_dataset.py#L41-L49)ï¼‰ï¼š

```python
input_ids = encoding.input_ids.squeeze()  # å‡è®¾æ˜¯: [101, 102, 103, 104, 105, 0, 0]

# Xï¼šç”¨å‰é¢çš„è¯é¢„æµ‹åé¢çš„è¯
X = input_ids[:-1]  # [101, 102, 103, 104, 105, 0]ï¼ˆå»æ‰æœ€åä¸€ä¸ªï¼‰

# Yï¼šæ­£ç¡®ç­”æ¡ˆï¼ˆè¦é¢„æµ‹çš„ä¸‹ä¸€ä¸ªè¯ï¼‰
Y = input_ids[1:]   # [102, 103, 104, 105, 0, 0]ï¼ˆå»æ‰ç¬¬ä¸€ä¸ªï¼‰

# loss_maskï¼šå“ªäº›ä½ç½®éœ€è¦è®¡ç®—æŸå¤±ï¼ˆå¿½ç•¥å¡«å……ä½ç½®ï¼‰
loss_mask = input_ids[1:] != tokenizer.pad_token_id  # [True, True, True, True, False, False]
```

**å½¢è±¡ç†è§£**ï¼š
```
åŸå§‹æ–‡æœ¬ï¼š     "ä»Šå¤© å¤©æ°” å¾ˆå¥½"
Token IDs:     [101, 102, 103, 104]

è¾“å…¥ X:        [101, 102, 103]       â†’ ç»™æ¨¡å‹çœ‹ "ä»Šå¤© å¤©æ°”"
æ ‡ç­¾ Y:        [102, 103, 104]       â†’ å¸Œæœ›æ¨¡å‹é¢„æµ‹ "å¤©æ°” å¾ˆå¥½"

æ¨¡å‹å­¦ä¹ ï¼šç”¨ "ä»Šå¤©" é¢„æµ‹ "å¤©æ°”"ï¼Œç”¨ "ä»Šå¤©å¤©æ°”" é¢„æµ‹ "å¾ˆå¥½"
```

---

## ğŸ—ï¸ ç¬¬äºŒç« ï¼šæ¨¡å‹çš„å»ºç­‘è“å›¾ï¼ˆArchitectureï¼‰

### 2.1 æ¨¡å‹çš„æ•´ä½“ç»“æ„

åœ¨ [model/model_minimind.py:581-619](model/model_minimind.py#L581-L619) ä¸­ï¼Œ`MokioMindForCausalLM` æ˜¯æœ€å¤–å±‚çš„æ¨¡å‹ï¼š

```
è¾“å…¥æ–‡æœ¬
    â†“
[1] Token Embeddingï¼ˆæŠŠæ•°å­—æ˜ å°„æˆå‘é‡ï¼‰
    â†“
[2] 8å±‚ MiniMindBlockï¼ˆæ ¸å¿ƒè®¡ç®—ï¼‰
    â†“
[3] RMSNormï¼ˆæœ€ç»ˆå½’ä¸€åŒ–ï¼‰
    â†“
[4] LM Headï¼ˆè¾“å‡ºå±‚ï¼Œé¢„æµ‹è¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªè¯çš„æ¦‚ç‡ï¼‰
    â†“
è¾“å‡º Logitsï¼ˆ6400ä¸ªè¯çš„åˆ†æ•°ï¼‰
```

### 2.2 æ¯ä¸€å±‚åœ¨å¹²ä»€ä¹ˆï¼Ÿ

#### ğŸ”¹ Embeddingå±‚ï¼ˆæŠŠè¯å˜æˆå‘é‡ï¼‰

åœ¨ [model/model_minimind.py:509](model/model_minimind.py#L509) ä¸­ï¼š
```python
self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
# vocab_size=6400ï¼ˆè¯æ±‡è¡¨å¤§å°ï¼‰
# hidden_size=512ï¼ˆå‘é‡ç»´åº¦ï¼‰
```

**å½¢è±¡ç†è§£**ï¼š
```
Token ID 102  â†’  [0.5, -0.3, 0.8, ..., 0.1]ï¼ˆ512ç»´å‘é‡ï¼‰
```

#### ğŸ”¹ MiniMindBlockï¼ˆæ ¸å¿ƒè®¡ç®—å•å…ƒï¼‰

æ¯ä¸ª Block åšä¸¤ä»¶äº‹ï¼ˆ[model/model_minimind.py:470-498](model/model_minimind.py#L470-L498)ï¼‰ï¼š

```python
def forward(self, hidden_states, ...):
    residual = hidden_states  # ä¿å­˜åŸå§‹è¾“å…¥ï¼ˆæ®‹å·®è¿æ¥ï¼‰

    # [æ­¥éª¤1] è‡ªæ³¨æ„åŠ›ï¼šè®©æ¯ä¸ªè¯"çœ‹"å…¶ä»–è¯
    hidden_states = self.self_attn(
        self.input_layernorm(hidden_states),  # å…ˆå½’ä¸€åŒ–
        ...
    )
    hidden_states = hidden_states + residual  # æ®‹å·®è¿æ¥

    # [æ­¥éª¤2] å‰é¦ˆç½‘ç»œï¼šå¯¹æ¯ä¸ªè¯ç‹¬ç«‹åšéçº¿æ€§å˜æ¢
    hidden_states = hidden_states + self.mlp(
        self.post_attention_layernorm(hidden_states)
    )

    return hidden_states
```

**æ®‹å·®è¿æ¥çš„æ„ä¹‰**ï¼šå°±åƒæŠ„ä½œä¸šæ—¶ä¿ç•™åŸé¢˜ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±ã€‚

#### ğŸ”¹ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰

è¿™æ˜¯æ¨¡å‹æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼ˆ[model/model_minimind.py:318-418](model/model_minimind.py#L318-L418)ï¼‰ï¼š

```python
def forward(self, x, ...):
    # [1] çº¿æ€§å˜æ¢ï¼šç”Ÿæˆ Qã€Kã€V
    xq = self.q_proj(x)  # Queryï¼ˆæŸ¥è¯¢ï¼‰
    xk = self.k_proj(x)  # Keyï¼ˆé”®ï¼‰
    xv = self.v_proj(x)  # Valueï¼ˆå€¼ï¼‰

    # [2] åŠ ä¸Šä½ç½®ç¼–ç ï¼ˆRoPEï¼‰
    xq, xk = apply_rotary_pos_emb(xq, xk, cos, sin)

    # [3] è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = (xq @ xk.transpose(-2, -1)) / sqrt(head_dim)

    # [4] Causal Maskï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
    causal_mask = torch.triu(...)  # ä¸Šä¸‰è§’çŸ©é˜µ
    scores = scores + causal_mask

    # [5] Softmax + åŠ æƒæ±‚å’Œ
    attention_weights = F.softmax(scores, dim=-1)
    output = attention_weights @ xv

    return output
```

**å½¢è±¡ç†è§£**ï¼š
```
å¥å­ï¼š"ä»Šå¤© å¤©æ°” å¾ˆå¥½"

è®¡ç®— "å¤©æ°”" å¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›ï¼š
- "å¤©æ°”" çœ‹ "ä»Šå¤©"ï¼š0.8ï¼ˆé«˜åº¦ç›¸å…³ï¼‰
- "å¤©æ°”" çœ‹ "å¤©æ°”"ï¼š0.6
- "å¤©æ°”" çœ‹ "å¾ˆå¥½"ï¼š0.0ï¼ˆå› ä¸ºCausal Maskï¼Œæœªæ¥ä¸å¯è§ï¼‰

æœ€ç»ˆè¾“å‡º = 0.8 * "ä»Šå¤©"çš„å‘é‡ + 0.6 * "å¤©æ°”"çš„å‘é‡
```

#### ğŸ”¹ å‰é¦ˆç½‘ç»œï¼ˆFeedForwardï¼‰

ä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°ï¼ˆ[model/model_minimind.py:444-451](model/model_minimind.py#L444-L451)ï¼‰ï¼š

```python
def forward(self, x):
    # é—¨æ§æœºåˆ¶ï¼šä¸€éƒ¨åˆ†åšæ¿€æ´»ï¼Œä¸€éƒ¨åˆ†åšé—¨æ§
    gated = self.act_fn(self.gate_proj(x)) * self.up_proj(x)
    return self.dropout(self.down_proj(gated))
```

**ç»´åº¦å˜åŒ–**ï¼š
```
x: [batch, seq_len, 512]
   â†“ gate_proj/up_proj
[batch, seq_len, 1365]  # æ‰©å¤§åˆ° intermediate_size
   â†“ down_proj
[batch, seq_len, 512]   # æ¢å¤åŸå§‹ç»´åº¦
```

---

## ğŸ”§ ç¬¬ä¸‰ç« ï¼šè®­ç»ƒçš„å…¨è¿‡ç¨‹ï¼ˆTraining Loopï¼‰

### 3.1 è®­ç»ƒå‰çš„å‡†å¤‡

åœ¨ [trainer/train_pretrain.py:212-310](trainer/train_pretrain.py#L212-L310) ä¸­ï¼Œåˆå§‹åŒ–è®­ç»ƒç¯å¢ƒï¼š

```python
# [1] åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
local_rank = init_distributed_mode()

# [2] è®¾ç½®éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
setup_seed(42 + dist.get_rank())

# [3] åˆ›å»ºæ¨¡å‹é…ç½®
lm_config = MiniMindConfig(
    hidden_size=args.hidden_size,      # 512
    num_hidden_layers=args.num_hidden_layers,  # 8å±‚
)

# [4] æ··åˆç²¾åº¦è®­ç»ƒ
dtype = torch.bfloat16  # åŠç²¾åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
autocast_ctx = torch.cuda.amp.autocast(dtype=dtype)

# [5] åˆå§‹åŒ–æ¨¡å‹ã€æ•°æ®ã€ä¼˜åŒ–å™¨
model, tokenizer = init_model(lm_config, ...)
train_ds = PretrainDataset(args.data_path, tokenizer, max_length=512)
optimizer = optim.AdamW(model.parameters(), lr=5e-4)
scaler = torch.cuda.amp.GradScaler()  # æ¢¯åº¦ç¼©æ”¾å™¨
```

### 3.2 è®­ç»ƒå¾ªç¯çš„æ¯ä¸€æ­¥

åœ¨ [trainer/train_pretrain.py:36-98](trainer/train_pretrain.py#L36-L98) çš„ `train_epoch` å‡½æ•°ä¸­ï¼š

```python
for step, (X, Y, loss_mask) in enumerate(loader):
    # [æ­¥éª¤1] æ•°æ®æ¬åˆ°GPU
    X = X.to(args.device)          # [batch_size, 512]
    Y = Y.to(args.device)
    loss_mask = loss_mask.to(args.device)

    # [æ­¥éª¤2] åŠ¨æ€å­¦ä¹ ç‡ï¼ˆä½™å¼¦é€€ç«ï¼‰
    lr = get_lr(epoch * iters + step, ...)
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr

    # [æ­¥éª¤3] å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
    with autocast_ctx:
        res = model(X)  # å¾—åˆ° logits: [batch, seq_len, vocab_size]

        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = loss_fct(
            res.logits.view(-1, 6400),  # å±•å¹³æˆ [batch*seq, vocab_size]
            Y.view(-1),                 # [batch*seq]
        ).view(Y.size())

        # åªè®¡ç®—éå¡«å……ä½ç½®çš„æŸå¤±
        loss = (loss * loss_mask).sum() / loss_mask.sum()

        # æ¢¯åº¦ç´¯ç§¯
        loss = loss / args.accumulation_steps

    # [æ­¥éª¤4] åå‘ä¼ æ’­
    scaler.scale(loss).backward()

    # [æ­¥éª¤5] æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
    if (step + 1) % args.accumulation_steps == 0:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
```

### 3.3 å…³é”®æŠ€æœ¯è¯¦è§£

#### ğŸ”¹ æ··åˆç²¾åº¦è®­ç»ƒ

```python
# ä¸ºä»€ä¹ˆç”¨bfloat16ï¼Ÿ
# - float32ï¼š4å­—èŠ‚ï¼Œç²¾åº¦é«˜ä½†å å†…å­˜
# - float16ï¼š2å­—èŠ‚ï¼Œæ•°å€¼èŒƒå›´å°ï¼Œå®¹æ˜“æº¢å‡º
# - bfloat16ï¼š2å­—èŠ‚ï¼Œæ•°å€¼èŒƒå›´å’Œfloat32ä¸€æ ·ï¼Œä¸å®¹æ˜“æº¢å‡º

with torch.cuda.amp.autocast(dtype=torch.bfloat16):
    logits = model(X)  # è‡ªåŠ¨ç”¨bfloat16è®¡ç®—
```

#### ğŸ”¹ æ¢¯åº¦ç´¯ç§¯

```python
# ä¸ºä»€ä¹ˆéœ€è¦æ¢¯åº¦ç´¯ç§¯ï¼Ÿ
# å‡è®¾æ˜¾å­˜åªèƒ½æ”¾batch_size=4ï¼Œä½†æˆ‘ä»¬æƒ³è¦32çš„æ•ˆæœï¼š

for i in range(8):  # ç´¯ç§¯8æ¬¡
    loss = compute_loss(batch) / 8  # é™¤ä»¥8ï¼Œä¿æŒæ¢¯åº¦è§„æ¨¡ä¸€è‡´
    loss.backward()  # æ¢¯åº¦ä¼šç´¯åŠ åœ¨å‚æ•°çš„.gradä¸Š

optimizer.step()  # æœ€åä¸€æ¬¡æ€§æ›´æ–°
optimizer.zero_grad()
```

#### ğŸ”¹ æ¢¯åº¦è£å‰ª

```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# åŸç†ï¼šå¦‚æœæ¢¯åº¦çš„L2èŒƒæ•° > 1.0ï¼Œå°±ç¼©æ”¾åˆ°1.0
# ä¾‹å¦‚ï¼šæ¢¯åº¦=[3, 4]ï¼ŒèŒƒæ•°=5ï¼Œç¼©æ”¾å=[0.6, 0.8]
```

---

## ğŸ¯ ç¬¬å››ç« ï¼šæ¨ç†çš„é­”æ³•ï¼ˆInferenceï¼‰

### 4.1 æ¨ç†çš„å®Œæ•´æµç¨‹

åœ¨ [eval.py:127-165](eval.py#L127-L165) ä¸­ï¼š

```python
# [æ­¥éª¤1] å‡†å¤‡è¾“å…¥
conversation = [{"role": "user", "content": "ä½ å¥½"}]
inputs = tokenizer.apply_chat_template(
    conversation=conversation,
    tokenize=False,
    add_generation_prompt=True
)
# ç»“æœç±»ä¼¼ï¼š<|im_start|>user\nä½ å¥½<|im_end|>\n<|im_start|>assistant\n

# [æ­¥éª¤2] Tokenize
inputs = tokenizer(inputs, return_tensors="pt").to(device)
# input_ids: [1, 5]ï¼ˆå‡è®¾5ä¸ªtokenï¼‰

# [æ­¥éª¤3] è‡ªå›å½’ç”Ÿæˆ
generated_ids = model.generate(
    inputs=inputs["input_ids"],
    max_new_tokens=8192,      # æœ€å¤šç”Ÿæˆ8192ä¸ªtoken
    do_sample=True,           # é‡‡æ ·æ¨¡å¼ï¼ˆè€Œéè´ªå¿ƒï¼‰
    temperature=0.85,         # æ§åˆ¶éšæœºæ€§
    top_p=0.85,              # æ ¸é‡‡æ ·
    streamer=streamer,       # æµå¼è¾“å‡º
)

# [æ­¥éª¤4] è§£ç 
response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
```

### 4.2 Generate çš„å†…éƒ¨åŸç†

è™½ç„¶ä»£ç ä¸­ä½¿ç”¨ HuggingFace çš„ `model.generate()`ï¼Œä½†å…¶æ ¸å¿ƒé€»è¾‘æ˜¯ï¼š

```python
# ç®€åŒ–ç‰ˆçš„è‡ªå›å½’ç”Ÿæˆ
def simple_generate(model, input_ids, max_new_tokens):
    past_key_values = None  # KV Cache

    for _ in range(max_new_tokens):
        # [1] å‰å‘ä¼ æ’­ï¼ˆåªè®¡ç®—æœ€åä¸€ä¸ªtokenï¼‰
        outputs = model(
            input_ids[:, -1:] if past_key_values else input_ids,
            past_key_values=past_key_values,
            use_cache=True,
        )

        logits = outputs.logits[:, -1, :]  # [batch, vocab_size]

        # [2] é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        next_token = sample(logits, temperature, top_p)

        # [3] æ‹¼æ¥åˆ°åºåˆ—ä¸­
        input_ids = torch.cat([input_ids, next_token], dim=1)

        # [4] ç¼“å­˜KVï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        past_key_values = outputs.past_key_values

        # [5] æ£€æŸ¥æ˜¯å¦ç”Ÿæˆç»“æŸç¬¦
        if next_token == eos_token_id:
            break

    return input_ids
```

### 4.3 å…³é”®æŠ€æœ¯ï¼šKV Cache

åœ¨ [model/model_minimind.py:346-356](model/model_minimind.py#L346-L356) ä¸­ï¼š

```python
# ä¸ºä»€ä¹ˆéœ€è¦KV Cacheï¼Ÿ
# å‡è®¾å·²ç»ç”Ÿæˆäº† "ä»Šå¤©å¤©æ°”"ï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬5ä¸ªè¯ï¼š

# æ²¡æœ‰Cacheï¼š
# - éœ€è¦é‡æ–°è®¡ç®— "ä»Šå¤©" "å¤©æ°”" çš„Keyå’ŒValueï¼ˆæµªè´¹ï¼ï¼‰

# æœ‰Cacheï¼š
if past_key_value is not None:
    xk = torch.cat([past_key_value[0], xk], dim=1)  # æ‹¼æ¥å†å²Key
    xv = torch.cat([past_key_value[1], xv], dim=1)  # æ‹¼æ¥å†å²Value

past_kv = (xk, xv) if use_cache else None  # ç¼“å­˜èµ·æ¥
```

**æ•ˆæœå¯¹æ¯”**ï¼š
```
åºåˆ—é•¿åº¦=512ï¼Œç”Ÿæˆ100ä¸ªtoken

æ— Cacheï¼š512æ¬¡å‰å‘ + 513æ¬¡ + 514æ¬¡ + ... â‰ˆ 51,200æ¬¡è®¡ç®—
æœ‰Cacheï¼š512æ¬¡ + 1æ¬¡ + 1æ¬¡ + ... â‰ˆ 612æ¬¡è®¡ç®—ï¼ˆå¿«83å€ï¼ï¼‰
```

### 4.4 é‡‡æ ·ç­–ç•¥

```python
# [æ–¹æ³•1] è´ªå¿ƒé‡‡æ ·ï¼ˆGreedyï¼‰
next_token = logits.argmax(dim=-1)  # æ€»é€‰æ¦‚ç‡æœ€é«˜çš„

# [æ–¹æ³•2] Temperatureé‡‡æ ·
logits = logits / temperature  # temperatureè¶Šå°è¶Šç¡®å®šï¼Œè¶Šå¤§è¶Šéšæœº
probs = F.softmax(logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)

# [æ–¹æ³•3] Top-Pé‡‡æ ·ï¼ˆNucleus Samplingï¼‰
sorted_probs, sorted_indices = torch.sort(probs, descending=True)
cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
mask = cumulative_probs > top_p  # åªä¿ç•™ç´¯ç§¯æ¦‚ç‡ <= top_p çš„token
```

**å½¢è±¡ç†è§£**ï¼š
```
å‡è®¾è¯æ±‡è¡¨ï¼š["çš„", "äº†", "æ˜¯", "åœ¨", ...]
Logits: [8.5, 7.2, 6.8, 2.1, ...]

Temperature=0.5ï¼ˆæ›´ç¡®å®šï¼‰ï¼š
æ¦‚ç‡: [0.85, 0.10, 0.04, 0.00, ...]  â†’ å‡ ä¹æ€»é€‰"çš„"

Temperature=2.0ï¼ˆæ›´éšæœºï¼‰ï¼š
æ¦‚ç‡: [0.45, 0.25, 0.18, 0.08, ...]  â†’ æœ‰æ›´å¤šå¯èƒ½æ€§
```

---

## ğŸ¬ ç¬¬äº”ç« ï¼šå®Œæ•´æµç¨‹æ€»ç»“

### è®­ç»ƒæµç¨‹ï¼ˆä»JSONLåˆ°æ¨¡å‹æƒé‡ï¼‰

```
[1] æ•°æ®å‡†å¤‡
    jsonlæ–‡ä»¶
      â†“ PretrainDataset.load_data()
    samplesåˆ—è¡¨ï¼ˆå†…å­˜ä¸­çš„æ‰€æœ‰æ•°æ®ï¼‰
      â†“ PretrainDataset.__getitem__()
    (X, Y, loss_mask)

[2] æ‰¹å¤„ç†
    DataLoader
      â†“ collate + batch
    [batch_size, seq_len] çš„Tensor

[3] æ¨¡å‹å‰å‘
    Embedding â†’ 8Ã—Block â†’ RMSNorm â†’ LM Head
      â†“
    Logits [batch, seq_len, 6400]

[4] æŸå¤±è®¡ç®—
    CrossEntropyLoss(Logits, Y) * loss_mask
      â†“
    æ ‡é‡æŸå¤±å€¼

[5] åå‘ä¼ æ’­
    loss.backward() â†’ è®¡ç®—æ¢¯åº¦
      â†“ æ¢¯åº¦è£å‰ª
    optimizer.step() â†’ æ›´æ–°å‚æ•°

[6] ä¿å­˜
    model.state_dict() â†’ .pthæ–‡ä»¶
```

### æ¨ç†æµç¨‹ï¼ˆä»æ–‡æœ¬åˆ°ç”Ÿæˆï¼‰

```
[1] è¾“å…¥å¤„ç†
    "ä½ å¥½"
      â†“ tokenizer
    [101, 102, ...]

[2] ç¬¬ä¸€æ¬¡å‰å‘
    Embedding + 8å±‚Transformer
      â†“
    Logits [1, 5, 6400]
      â†“ é‡‡æ ·
    ä¸‹ä¸€ä¸ªtoken: 103

[3] è‡ªå›å½’ç”Ÿæˆï¼ˆå¾ªç¯ï¼‰
    while æœªåˆ°max_new_tokens:
        ç”¨ [101, 102, 103] é¢„æµ‹ä¸‹ä¸€ä¸ª
          â†“ é‡‡æ ·
        token: 104
          â†“ æ‹¼æ¥
        [101, 102, 103, 104]

    ï¼ˆåˆ©ç”¨KV CacheåŠ é€Ÿï¼‰

[4] è§£ç 
    [101, 102, 103, ..., 200]
      â†“ tokenizer.decode()
    "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼"
```

---

## ğŸ”‘ æ ¸å¿ƒçŸ¥è¯†ç‚¹é€ŸæŸ¥è¡¨

| æŠ€æœ¯ | ä½ç½® | ä½œç”¨ |
|------|------|------|
| **é”™ä½æ ‡ç­¾** | [dataset/lm_dataset.py:45-47](dataset/lm_dataset.py#L45-L47) | è®©æ¨¡å‹å­¦ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ |
| **RoPEä½ç½®ç¼–ç ** | [model/model_minimind.py:209-223](model/model_minimind.py#L209-L223) | å‘Šè¯‰æ¨¡å‹æ¯ä¸ªè¯çš„ä½ç½®ä¿¡æ¯ |
| **GQAæ³¨æ„åŠ›** | [model/model_minimind.py:226-253](model/model_minimind.py#L226-L253) | å‡å°‘KVå¤´æ•°ï¼ŒèŠ‚çœæ˜¾å­˜ |
| **Causal Mask** | [model/model_minimind.py:393-396](model/model_minimind.py#L393-L396) | é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ |
| **RMSNorm** | [model/model_minimind.py:89-106](model/model_minimind.py#L89-L106) | å½’ä¸€åŒ–å±‚ï¼Œç¨³å®šè®­ç»ƒ |
| **SwiGLU** | [model/model_minimind.py:444-451](model/model_minimind.py#L444-L451) | é—¨æ§æ¿€æ´»å‡½æ•° |
| **æ··åˆç²¾åº¦** | [trainer/train_pretrain.py:259-266](trainer/train_pretrain.py#L259-L266) | ç”¨bfloat16èŠ‚çœæ˜¾å­˜ |
| **æ¢¯åº¦ç´¯ç§¯** | [trainer/train_pretrain.py:64-80](trainer/train_pretrain.py#L64-L80) | æ¨¡æ‹Ÿå¤§batch_size |
| **æ¢¯åº¦è£å‰ª** | [trainer/train_pretrain.py:72](trainer/train_pretrain.py#L72) | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |
| **KV Cache** | [model/model_minimind.py:349-355](model/model_minimind.py#L349-L355) | æ¨ç†åŠ é€Ÿ83å€+ |
| **Top-Pé‡‡æ ·** | [eval.py:158](eval.py#L158) | æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ |

---

## ğŸ’¡ å¸¸è§é—®é¢˜è§£ç­”

### Q1: ä¸ºä»€ä¹ˆéœ€è¦loss_maskï¼Ÿ

```python
# å‡è®¾batchä¸­æœ‰å¡«å……ï¼š
input_ids = [101, 102, 0, 0]  # 0æ˜¯padding
loss_mask = [1, 1, 0, 0]      # åªè®¡ç®—å‰2ä¸ªä½ç½®çš„æŸå¤±

loss = (loss * loss_mask).sum() / loss_mask.sum()
# è¿™æ ·é¿å…äº†paddingä½ç½®å½±å“è®­ç»ƒ
```

### Q2: ä¸ºä»€ä¹ˆè¦ç”¨æ®‹å·®è¿æ¥ï¼Ÿ

```
æ²¡æœ‰æ®‹å·®ï¼šx â†’ Layer1 â†’ Layer2 â†’ ... â†’ Layer8
é—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±ï¼Œåé¢çš„å±‚å­¦ä¸åˆ°ä¸œè¥¿

æœ‰æ®‹å·®ï¼šx â†’ (+) â†’ (+) â†’ (+) â†’ ...
           â†‘     â†‘     â†‘
         Layer1 Layer2 Layer3
ä¼˜ç‚¹ï¼šæ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ å›æœ€å‰é¢
```

### Q3: Flash Attention å¿«åœ¨å“ªé‡Œï¼Ÿ

```
æ ‡å‡†Attentionï¼š
1. è®¡ç®—æ•´ä¸ª scores çŸ©é˜µ [seq, seq]
2. å…¨éƒ¨åŠ è½½åˆ°æ˜¾å­˜
3. æ—¶é—´ O(nÂ²)ï¼Œæ˜¾å­˜ O(nÂ²)

Flash Attentionï¼š
1. åˆ†å—è®¡ç®—ï¼Œä¸ä¿å­˜æ•´ä¸ªçŸ©é˜µ
2. æ—¶é—´ O(nÂ²)ï¼Œæ˜¾å­˜ O(n)
3. åœ¨ç¡¬ä»¶ä¸Šä¼˜åŒ–äº†å†…å­˜è®¿é—®æ¨¡å¼
```

### Q4: ä»€ä¹ˆæ˜¯åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼Ÿ

```
ä¼ ç»ŸMHAï¼ˆMulti-Head Attentionï¼‰ï¼š
- Query heads: 8ä¸ª
- Key heads: 8ä¸ª
- Value heads: 8ä¸ª
- KV Cacheå¤§å°: 8 * seq_len * head_dim

GQAï¼ˆGrouped Query Attentionï¼‰ï¼š
- Query heads: 8ä¸ª
- Key heads: 2ä¸ªï¼ˆå…±äº«ï¼‰
- Value heads: 2ä¸ªï¼ˆå…±äº«ï¼‰
- KV Cacheå¤§å°: 2 * seq_len * head_dimï¼ˆèŠ‚çœ75%ï¼ï¼‰

åŸç†ï¼šæ¯4ä¸ªQueryå¤´å…±äº«1ç»„KV
```

### Q5: ä¸ºä»€ä¹ˆç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡ï¼Ÿ

```python
def get_lr(current_step, total_steps, lr):
    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))
```

```
å­¦ä¹ ç‡å˜åŒ–æ›²çº¿ï¼š

lr=5e-4
  â†“
0.0005 â”œâ”€â”€â”€â”€â”€â•®
       â”‚      â•²
       â”‚       â•²
       â”‚        â•²___
0.00005â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0%    50%   100% (è®­ç»ƒè¿›åº¦)

å¥½å¤„ï¼š
1. å¼€å§‹æ—¶å¤§æ­¥å¿«èµ°ï¼ˆå¿«é€Ÿæ”¶æ•›ï¼‰
2. ç»“æŸæ—¶å°æ­¥ç²¾è°ƒï¼ˆæ‰¾åˆ°æœ€ä¼˜è§£ï¼‰
3. å¹³æ»‘è¿‡æ¸¡ï¼Œé¿å…éœ‡è¡
```

### Q6: ä»€ä¹ˆæ—¶å€™ä¿å­˜æ£€æŸ¥ç‚¹ï¼Ÿ

ä»£ç ä¸­çš„ä¿å­˜ç­–ç•¥ï¼ˆ[trainer/train_pretrain.py:99-132](trainer/train_pretrain.py#L99-L132)ï¼‰ï¼š

```python
if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
    # ä¿å­˜ä¸¤ç§æ–‡ä»¶ï¼š

    # 1. æ¨¡å‹æƒé‡ï¼ˆç”¨äºæ¨ç†ï¼‰
    torch.save(state_dict, "out/pretrain_512.pth")

    # 2. å®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
    lm_checkpoint(
        model=model,
        optimizer=optimizer,  # ä¼˜åŒ–å™¨çŠ¶æ€
        scaler=scaler,        # æ¢¯åº¦ç¼©æ”¾å™¨çŠ¶æ€
        epoch=epoch,
        step=step,
        wandb=wandb,          # å®éªŒè·Ÿè¸ªID
    )
```

**ä¸¤ç§æ–‡ä»¶çš„åŒºåˆ«**ï¼š
```
pretrain_512.pth (è½»é‡çº§)
â”œâ”€ åªåŒ…å«æ¨¡å‹å‚æ•°
â”œâ”€ å¤§å°: ~50MB
â””â”€ ç”¨é€”: æ¨ç†ã€åˆ†äº«æ¨¡å‹

pretrain_512_resume.pth (å®Œæ•´)
â”œâ”€ æ¨¡å‹å‚æ•°
â”œâ”€ ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŠ¨é‡ã€æ–¹å·®ä¼°è®¡ï¼‰
â”œâ”€ è®­ç»ƒè¿›åº¦ï¼ˆepoch, stepï¼‰
â”œâ”€ å®éªŒè·Ÿè¸ªID
â”œâ”€ å¤§å°: ~150MB
â””â”€ ç”¨é€”: æ–­ç‚¹ç»­è®­
```

### Q7: åˆ†å¸ƒå¼è®­ç»ƒå¦‚ä½•å·¥ä½œï¼Ÿ

```python
# [1] åˆå§‹åŒ–
dist.init_process_group(backend="nccl")  # GPUé—´é€šä¿¡
local_rank = int(os.environ["LOCAL_RANK"])  # å½“å‰è¿›ç¨‹çš„GPUç¼–å·

# [2] æ•°æ®åˆ†ç‰‡
train_sampler = DistributedSampler(train_ds)
# GPU 0: å¤„ç†æ ·æœ¬ [0, 4, 8, 12, ...]
# GPU 1: å¤„ç†æ ·æœ¬ [1, 5, 9, 13, ...]
# GPU 2: å¤„ç†æ ·æœ¬ [2, 6, 10, 14, ...]
# GPU 3: å¤„ç†æ ·æœ¬ [3, 7, 11, 15, ...]

# [3] æ¨¡å‹åŒ…è£…
model = DistributedDataParallel(model, device_ids=[local_rank])
# è‡ªåŠ¨åœ¨åå‘ä¼ æ’­æ—¶åŒæ­¥æ¢¯åº¦

# [4] æ¢¯åº¦åŒæ­¥è¿‡ç¨‹
loss.backward()  # å„GPUç‹¬ç«‹è®¡ç®—æ¢¯åº¦
# DDPè‡ªåŠ¨æ‰§è¡Œ AllReduce æ“ä½œï¼š
#   GPU0æ¢¯åº¦ + GPU1æ¢¯åº¦ + GPU2æ¢¯åº¦ + GPU3æ¢¯åº¦ â†’ æ±‚å¹³å‡ â†’ å¹¿æ’­ç»™æ‰€æœ‰GPU
optimizer.step()  # å„GPUç”¨ç›¸åŒçš„æ¢¯åº¦æ›´æ–°å‚æ•°
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æŠ€å·§æ€»ç»“

### è®­ç»ƒåŠ é€Ÿ

| æŠ€æœ¯ | åŠ é€Ÿå€æ•° | æ˜¾å­˜èŠ‚çœ | å®ç°éš¾åº¦ |
|------|---------|---------|---------|
| æ··åˆç²¾åº¦(bfloat16) | 1.5-2x | 50% | â­ |
| æ¢¯åº¦ç´¯ç§¯ | æ—  | ç­‰æ•ˆå¤§batch | â­ |
| Flash Attention | 1.2-1.5x | 30-50% | â­â­ |
| åˆ†å¸ƒå¼è®­ç»ƒ(4å¡) | 3.5-4x | æ—  | â­â­â­ |
| GQAæ³¨æ„åŠ› | 1.1x | 25%(KV Cache) | â­â­ |

### æ¨ç†åŠ é€Ÿ

| æŠ€æœ¯ | åŠ é€Ÿå€æ•° | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| KV Cache | 10-100x | è‡ªå›å½’ç”Ÿæˆ |
| æ‰¹å¤„ç†æ¨ç† | çº¿æ€§å¢é•¿ | å¤šè¯·æ±‚å¹¶å‘ |
| é‡åŒ–(INT8) | 2-4x | éƒ¨ç½² |
| Flash Attention | 1.2-1.5x | é•¿åºåˆ— |

---

## ğŸš€ è¿›é˜¶å­¦ä¹ è·¯å¾„

### åˆçº§ï¼šç†è§£åŸºç¡€æ¦‚å¿µ
- âœ… Tokenizerçš„å·¥ä½œåŸç†
- âœ… äº¤å‰ç†µæŸå¤±çš„è®¡ç®—
- âœ… è‡ªæ³¨æ„åŠ›æœºåˆ¶
- âœ… æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–

### ä¸­çº§ï¼šæŒæ¡è®­ç»ƒæŠ€å·§
- âœ… æ··åˆç²¾åº¦è®­ç»ƒ
- âœ… æ¢¯åº¦ç´¯ç§¯å’Œè£å‰ª
- âœ… å­¦ä¹ ç‡è°ƒåº¦
- âœ… åˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€

### é«˜çº§ï¼šä¼˜åŒ–ä¸éƒ¨ç½²
- ğŸ”² Flash Attention å®ç°åŸç†
- ğŸ”² æ¨¡å‹é‡åŒ–ï¼ˆINT8/INT4ï¼‰
- ğŸ”² æ¨¡å‹å‰ªæ
- ğŸ”² TensorRT éƒ¨ç½²
- ğŸ”² vLLM æ¨ç†å¼•æ“

---

## ğŸ“– å‚è€ƒèµ„æº

### è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - TransformeråŸå§‹è®ºæ–‡
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) - RoPEä½ç½®ç¼–ç 
- [GQA: Training Generalized Multi-Query Transformer](https://arxiv.org/abs/2305.13245) - åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›
- [Flash Attention](https://arxiv.org/abs/2205.14135) - é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶

### ä»£ç å®ç°
- [MiniMindé¡¹ç›®](https://github.com/jingyaogong/minimind) - æœ¬é¡¹ç›®åŸå§‹ä»“åº“
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathyçš„æ•™å­¦å®ç°
- [LLaMA](https://github.com/facebookresearch/llama) - Metaçš„å¼€æºå¤§æ¨¡å‹

### å­¦ä¹ èµ„æº
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - å›¾è§£Transformer
- [Stanford CS224N](http://web.stanford.edu/class/cs224n/) - NLPè¯¾ç¨‹
- [Hugging Face Course](https://huggingface.co/course) - Transformersæ•™ç¨‹

---

## ğŸ“ ç»“è¯­

æ­å–œä½ å®Œæˆäº† MiniMind é¢„è®­ç»ƒä¸æ¨ç†å…¨æµç¨‹çš„å­¦ä¹ ï¼ç°åœ¨ä½ åº”è¯¥èƒ½å¤Ÿï¼š

âœ… ç†è§£è¯­è¨€æ¨¡å‹çš„æ•°æ®å‡†å¤‡æµç¨‹
âœ… æŒæ¡Transformeræ¶æ„çš„æ¯ä¸€å±‚
âœ… äº†è§£è®­ç»ƒå¾ªç¯ä¸­çš„å…³é”®æŠ€æœ¯
âœ… æ˜ç™½æ¨ç†ç”Ÿæˆçš„è‡ªå›å½’è¿‡ç¨‹
âœ… åº”ç”¨å„ç§ä¼˜åŒ–æŠ€å·§æå‡æ€§èƒ½

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š
1. åŠ¨æ‰‹è¿è¡Œä»£ç ï¼Œè§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹
2. ä¿®æ”¹è¶…å‚æ•°ï¼Œæ„Ÿå—å¯¹ç»“æœçš„å½±å“
3. å°è¯•æ·»åŠ æ–°åŠŸèƒ½ï¼ˆå¦‚MoEã€LoRAç­‰ï¼‰
4. é˜…è¯»ç›¸å…³è®ºæ–‡ï¼Œæ·±å…¥ç†è§£åŸç†

è®°ä½ï¼š**æœ€å¥½çš„å­¦ä¹ æ–¹æ³•å°±æ˜¯åŠ¨æ‰‹å®è·µ**ï¼

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´ï¼š2025å¹´*
*ä½œè€…ï¼šClaude (Anthropic)*
*é¡¹ç›®ï¼šLearnMinimind*
