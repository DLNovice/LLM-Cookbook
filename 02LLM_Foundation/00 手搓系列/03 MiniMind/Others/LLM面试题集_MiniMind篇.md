# ğŸ¯ LLMèµ„æ·±é¢è¯•å®˜ï¼šMiniMindé¢„è®­ç»ƒä¸æ¨ç†æ·±åº¦é¢è¯•é¢˜é›†

> åŸºäºå¤šå¹´ä¸€çº¿é¢è¯•ç»éªŒï¼Œæ¶µç›–åŸºç¡€â†’è¿›é˜¶â†’ä¸“å®¶ä¸‰ä¸ªçº§åˆ«

---

## ğŸ“‹ é¢è¯•è¯´æ˜

**è¯„åˆ†æ ‡å‡†**ï¼š
- â­ åŸºç¡€é¢˜ï¼ˆJunior/Mid-levelï¼‰ï¼šç†è§£åŸºæœ¬æ¦‚å¿µå’Œæµç¨‹
- â­â­ è¿›é˜¶é¢˜ï¼ˆSeniorï¼‰ï¼šæ·±å…¥ç†è§£åŸç†å’Œæƒè¡¡
- â­â­â­ ä¸“å®¶é¢˜ï¼ˆStaff/Principalï¼‰ï¼šç³»ç»Ÿè®¾è®¡å’Œä¼˜åŒ–èƒ½åŠ›

**æ—¶é—´åˆ†é…**ï¼š
- åŸºç¡€é¢˜ï¼š5-10åˆ†é’Ÿ/é¢˜
- è¿›é˜¶é¢˜ï¼š10-15åˆ†é’Ÿ/é¢˜
- ä¸“å®¶é¢˜ï¼š15-30åˆ†é’Ÿ/é¢˜

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å¤„ç†ä¸åŠ è½½ï¼ˆ6é¢˜ï¼‰

### â­ Q1: è¯·è§£é‡ŠPretrainDatasetä¸­ä¸ºä»€ä¹ˆè¦è¿”å›(X, Y, loss_mask)ä¸‰ä¸ªå€¼ï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šè¯­è¨€æ¨¡å‹è®­ç»ƒçš„åŸºæœ¬åŸç†

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**å®Œæ•´å›ç­”**ï¼š

```python
# ä»£ç ä½ç½®: dataset/lm_dataset.py:45-49

# 1. Xå’ŒYçš„å…³ç³»
X = input_ids[:-1]  # [ä»Šå¤©, å¤©æ°”, å¾ˆå¥½]
Y = input_ids[1:]   # [å¤©æ°”, å¾ˆå¥½, EOS]

# ä½œç”¨ï¼šå®ç°è‡ªå›å½’è®­ç»ƒ
# ç”¨X[i]é¢„æµ‹Y[i]ï¼Œå³ç”¨"ä»Šå¤©"é¢„æµ‹"å¤©æ°”"
```

**ä¸‰ä¸ªå€¼çš„ä½œç”¨**ï¼š
1. **Xï¼ˆè¾“å…¥ï¼‰**ï¼šæ¨¡å‹çš„è¾“å…¥åºåˆ—ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªtoken
2. **Yï¼ˆæ ‡ç­¾ï¼‰**ï¼šæ­£ç¡®ç­”æ¡ˆï¼Œç”¨äºè®¡ç®—æŸå¤±
3. **loss_maskï¼ˆæ©ç ï¼‰**ï¼šæ ‡è®°å“ªäº›ä½ç½®éœ€è¦è®¡ç®—æŸå¤±
   - `1`ï¼šæœ‰æ•ˆä½ç½®ï¼ˆçœŸå®æ•°æ®ï¼‰
   - `0`ï¼špaddingä½ç½®ï¼ˆå¿½ç•¥ï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦loss_mask**ï¼š

```python
# æ‰¹æ¬¡ä¸­ä¸åŒé•¿åº¦çš„å¥å­
å¥å­1: [101, 102, 103, 0, 0]  # é•¿åº¦3ï¼Œpadding 2ä¸ª0
å¥å­2: [201, 202, 203, 204, 205]  # é•¿åº¦5

# ä¸ç”¨maskï¼ˆé”™è¯¯ï¼‰
loss = CrossEntropyLoss(æ‰€æœ‰ä½ç½®)
# é—®é¢˜ï¼šæ¨¡å‹ä¼šå­¦ä¹ "103åº”è¯¥é¢„æµ‹0"ï¼ˆé”™è¯¯æ¨¡å¼ï¼‰

# ç”¨maskï¼ˆæ­£ç¡®ï¼‰
loss = (loss * loss_mask).sum() / loss_mask.sum()
# åªè®¡ç®—æœ‰æ•ˆä½ç½®çš„æŸå¤±
```

**åŠ åˆ†ç‚¹**ï¼š
- æåˆ°"Causal Language Modeling"
- è§£é‡Šä¸ºä»€ä¹ˆæ˜¯[:-1]å’Œ[1:]è€Œä¸æ˜¯å…¶ä»–æ–¹å¼
- çŸ¥é“è¿™ç§æ–¹å¼å«"Teacher Forcing"

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“Xæ˜¯è¾“å…¥ï¼ŒYæ˜¯æ ‡ç­¾
- 80åˆ†ï¼šç†è§£é”™ä½å…³ç³»ï¼ŒçŸ¥é“loss_maskçš„ä½œç”¨
- 100åˆ†ï¼šèƒ½è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Œæåˆ°paddingé—®é¢˜

---

### â­â­ Q2: å¦‚æœè®­ç»ƒæ•°æ®ä¸­æœ‰è¿™æ ·ä¸€æ¡ï¼š`{"text": ""}`ï¼ˆç©ºå­—ç¬¦ä¸²ï¼‰ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿåº”è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šè¾¹ç•Œæƒ…å†µå¤„ç†èƒ½åŠ›ã€æ•°æ®æ¸…æ´—

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**é—®é¢˜åˆ†æ**ï¼š

```python
# ç©ºå­—ç¬¦ä¸²ç»è¿‡tokenizer
input_ids = [pad_token_id, pad_token_id, ...]  # å…¨æ˜¯padding

X = input_ids[:-1]  # [pad, pad, ..., pad]
Y = input_ids[1:]   # [pad, pad, ..., pad]
loss_mask = (Y != pad_token_id)  # [False, False, ..., False]

# è®¡ç®—æŸå¤±
loss = (loss * loss_mask).sum() / loss_mask.sum()
# é—®é¢˜ï¼šåˆ†æ¯ä¸º0ï¼ â†’ äº§ç”ŸNaN
```

**ä¼šå¯¼è‡´çš„é—®é¢˜**ï¼š
1. **é™¤é›¶é”™è¯¯**ï¼š`loss_mask.sum() = 0`
2. **NaNä¼ æ’­**ï¼šNaNæ¢¯åº¦ä¼šæ±¡æŸ“æ•´ä¸ªæ¨¡å‹
3. **è®­ç»ƒå´©æºƒ**ï¼šä¼˜åŒ–å™¨æ— æ³•å¤„ç†NaN

**è§£å†³æ–¹æ¡ˆ**ï¼ˆä»å¥½åˆ°ä¼˜ï¼‰ï¼š

**æ–¹æ¡ˆ1ï¼šæ•°æ®é¢„å¤„ç†**ï¼ˆæœ€ä½³ï¼‰
```python
def load_data(self, path):
    samples = []
    with open(path, "r") as f:
        for line in f:
            data = json.loads(line.strip())
            if len(data["text"].strip()) > 0:  # è¿‡æ»¤ç©ºæ–‡æœ¬
                samples.append(data)
    return samples
```

**æ–¹æ¡ˆ2ï¼šDataLoaderè¿‡æ»¤**
```python
class PretrainDataset(Dataset):
    def __getitem__(self, index):
        sample = self.samples[index]
        encoding = self.tokenizer(...)

        # æ£€æŸ¥æœ‰æ•ˆtokenæ•°é‡
        if (encoding.input_ids != self.tokenizer.pad_token_id).sum() == 0:
            return None  # è¿”å›Noneï¼Œcollate_fnä¸­è¿‡æ»¤
```

**æ–¹æ¡ˆ3ï¼šæŸå¤±è®¡ç®—ä¿æŠ¤**
```python
loss_mask_sum = loss_mask.sum()
if loss_mask_sum > 0:
    loss = (loss * loss_mask).sum() / loss_mask_sum
else:
    loss = torch.tensor(0.0, requires_grad=True)  # è¿”å›0æŸå¤±
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°NaNä¼ æ’­çš„å±å®³
- çŸ¥é“å¤šç§è§£å†³æ–¹æ¡ˆå¹¶èƒ½æ¯”è¾ƒä¼˜åŠ£
- æåˆ°æ•°æ®è´¨é‡æ£€æŸ¥çš„é‡è¦æ€§
- æåˆ°ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥åœ¨æ•°æ®pipelineé˜¶æ®µå°±è¿‡æ»¤

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šæ„è¯†åˆ°ä¼šæœ‰é—®é¢˜
- 80åˆ†ï¼šèƒ½æ­£ç¡®åˆ†æé—®é¢˜åŸå› ï¼ˆé™¤é›¶ï¼‰
- 100åˆ†ï¼šæä¾›å¤šç§è§£å†³æ–¹æ¡ˆå¹¶åˆ†æä¼˜åŠ£

---

### â­â­ Q3: ä¸ºä»€ä¹ˆTokenizerçš„è¯æ±‡è¡¨å¤§å°æ˜¯6400ï¼Ÿå¦‚æœæ”¹æˆ1000æˆ–100000ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šè¯æ±‡è¡¨è®¾è®¡çš„æƒè¡¡ã€å‚æ•°é‡è®¡ç®—

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**ä¸‰ä¸ªå…³é”®å› ç´ çš„æƒè¡¡**ï¼š

```python
# 1. Embeddingå‚æ•°é‡
vocab_size = 6400
hidden_size = 512
embedding_params = 6400 Ã— 512 = 3.28M

# 2. åºåˆ—é•¿åº¦
text = "æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£"
tokens_6400 = 6ä¸ªtoken
tokens_1000 = 12ä¸ªtoken (å¸¸ç”¨è¯è¢«æ‹†åˆ†)
tokens_100000 = 5ä¸ªtoken (å‡ ä¹ä¸æ‹†åˆ†)

# 3. æœ€ç»ˆè¾“å‡ºå±‚å‚æ•°é‡
lm_head_params = hidden_size Ã— vocab_size
```

**æ–¹æ¡ˆå¯¹æ¯”**ï¼š

| è¯æ±‡è¡¨å¤§å° | Embeddingå‚æ•° | å¹³å‡åºåˆ—é•¿åº¦ | LM Headå‚æ•° | æ€»å‚æ•°å½±å“ | ä¼˜ç¼ºç‚¹ |
|-----------|-------------|------------|------------|-----------|--------|
| 1,000 | 0.5M | 2x | 0.5M | -2.3M | åºåˆ—å¤ªé•¿ï¼ŒAttentionè®¡ç®—O(nÂ²)çˆ†ç‚¸ |
| 6,400 | 3.3M | 1x | 3.3M | åŸºçº¿26M | å¹³è¡¡ç‚¹ |
| 100,000 | 51.2M | 0.8x | 51.2M | +96M | å‚æ•°é‡çˆ†ç‚¸ï¼Œè¶…è¿‡æ¨¡å‹æ€»å‚æ•°2å€ï¼|

**å®é™…è®¡ç®—**ï¼š

```python
# vocab_size=1000
sentence = "æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†"
tokens = [156, 234, 567, 12, 345, 678, 89, 234, 456, 123]  # 10ä¸ªtoken
attention_cost = 10Â² Ã— num_heads Ã— num_layers
                = 100 Ã— 8 Ã— 8 = 6400æ¬¡è®¡ç®—

# vocab_size=6400
tokens = [1234, 2345, 3456, 4567, 5678]  # 5ä¸ªtoken
attention_cost = 5Â² Ã— 8 Ã— 8 = 1600æ¬¡è®¡ç®—ï¼ˆå¿«4å€ï¼ï¼‰

# vocab_size=100000
tokens = [12345, 23456, 34567, 45678]  # 4ä¸ªtoken
attention_cost = 4Â² Ã— 8 Ã— 8 = 1024æ¬¡è®¡ç®—
ä½†embeddingå‚æ•°: 100000Ã—512 = 51.2Mï¼ˆ26Mæ¨¡å‹çš„2å€ï¼ï¼‰
```

**Scaling Law**ï¼ˆç»éªŒå…¬å¼ï¼‰ï¼š

```python
# å¯¹äºå°æ¨¡å‹ï¼ˆ<100Må‚æ•°ï¼‰
optimal_vocab_size â‰ˆ sqrt(total_params) Ã— 100

26Mæ¨¡å‹: sqrt(26M) Ã— 100 â‰ˆ 5100
å®é™…6400ï¼ˆç•¥å¤§ï¼Œä¸ºäº†è¦†ç›–æ›´å¤šä¸­æ–‡è¯ï¼‰

# å¯¹äºå¤§æ¨¡å‹ï¼ˆ>1Bå‚æ•°ï¼‰
LLaMA-7B: vocab_size = 32000
GPT-3-175B: vocab_size = 50257
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°ä¸åŒè¯­è¨€çš„è¯æ±‡è¡¨å¤§å°å·®å¼‚ï¼ˆè‹±æ–‡vsä¸­æ–‡ï¼‰
- çŸ¥é“BPE/WordPieceç­‰tokenizationç®—æ³•
- æåˆ°å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ‰©å±•è¯æ±‡è¡¨ï¼ˆé¢†åŸŸè¯æ±‡ï¼‰
- è®¡ç®—å…·ä½“æ•°å€¼

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“ä¼šå½±å“å‚æ•°é‡
- 80åˆ†ï¼šèƒ½åˆ†æå¯¹åºåˆ—é•¿åº¦ã€è®¡ç®—é‡çš„å½±å“
- 100åˆ†ï¼šç»™å‡ºå…·ä½“æ•°å€¼è®¡ç®—ï¼Œæåˆ°æƒè¡¡åŸåˆ™

---

### â­â­â­ Q4: å¦‚æœè¦å¤„ç†é•¿åº¦ä¸º8192çš„æ–‡æœ¬ï¼ˆè®­ç»ƒæ—¶max_seq_len=512ï¼‰ï¼Œæœ‰å“ªäº›æ–¹æ¡ˆï¼Ÿå„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šé•¿æ–‡æœ¬å¤„ç†ã€ç³»ç»Ÿè®¾è®¡èƒ½åŠ›

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**æ–¹æ¡ˆå¯¹æ¯”è¡¨**ï¼š

| æ–¹æ¡ˆ | å®ç°éš¾åº¦ | è®¡ç®—æˆæœ¬ | ä¿¡æ¯æŸå¤± | é€‚ç”¨åœºæ™¯ |
|-----|---------|---------|---------|---------|
| æˆªæ–­ | â­ | ä½ | é«˜ | ä¸é€‚ç”¨ |
| æ»‘åŠ¨çª—å£ | â­â­ | ä¸­ | ä¸­ | æ–‡æ¡£åˆ†ç±» |
| å±‚æ¬¡åŒ–å¤„ç† | â­â­â­ | ä¸­ | ä½ | é•¿æ–‡æ¡£ç†è§£ |
| ç¨€ç–æ³¨æ„åŠ› | â­â­â­â­ | ä¸­ | ä½ | é€šç”¨ |
| ä½ç½®ç¼–ç å¤–æ¨ | â­â­ | ä½ | ä½ | æ¨ç†æ—¶ä¸´æ—¶ç”¨ |

**æ–¹æ¡ˆ1ï¼šç›´æ¥æˆªæ–­**ï¼ˆä¸æ¨èï¼‰

```python
# ç®€å•ç²—æš´
text_8192 = tokenizer(long_text)
X = text_8192[:512]  # ä¸¢å¼ƒåé¢7680ä¸ªtoken

ä¼˜ç‚¹ï¼šå®ç°ç®€å•
ç¼ºç‚¹ï¼šä¸¢å¤±75%çš„ä¿¡æ¯ï¼
```

**æ–¹æ¡ˆ2ï¼šæ»‘åŠ¨çª—å£ + å¹³å‡**

```python
# å°†8192åˆ†æˆ16ä¸ª512çš„çª—å£
windows = [
    text[0:512],
    text[256:768],      # 50%é‡å 
    text[512:1024],
    ...
]

# æ¯ä¸ªçª—å£ç‹¬ç«‹å¤„ç†
embeddings = [model(window) for window in windows]

# èšåˆç»“æœ
final = torch.mean(torch.stack(embeddings), dim=0)

ä¼˜ç‚¹ï¼š
- ä¸ä¸¢å¤±ä¿¡æ¯
- å®ç°ç›¸å¯¹ç®€å•

ç¼ºç‚¹ï¼š
- è®¡ç®—é‡Ã—16
- çª—å£ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–ä¸¢å¤±
```

**æ–¹æ¡ˆ3ï¼šå±‚æ¬¡åŒ–å¤„ç†**ï¼ˆæœ€ä½³å®è·µï¼‰

```python
# ç¬¬ä¸€é˜¶æ®µï¼šå±€éƒ¨ç¼–ç 
chunks = split_into_chunks(text_8192, chunk_size=512)  # 16ä¸ªchunk
chunk_embeddings = [model(chunk) for chunk in chunks]  # [16, 512, hidden]

# ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€èšåˆ
# ç”¨ä¸€ä¸ªå°æ¨¡å‹å¤„ç†chunkçº§åˆ«çš„äº¤äº’
global_tokens = [embedding.mean(dim=1) for embedding in chunk_embeddings]  # [16, hidden]
global_context = global_model(global_tokens)  # [16, hidden]

# ç¬¬ä¸‰é˜¶æ®µï¼šèåˆ
final = merge(chunk_embeddings, global_context)

ä¼˜ç‚¹ï¼š
- ä¿ç•™é•¿è·ç¦»ä¾èµ–
- è®¡ç®—é‡å¯æ§ï¼ˆ16Ã—å±€éƒ¨ + 1Ã—å…¨å±€ï¼‰

ç¼ºç‚¹ï¼š
- éœ€è¦è®­ç»ƒglobal_model
- å®ç°å¤æ‚

å®é™…æ¡ˆä¾‹ï¼šLongformer, BigBird
```

**æ–¹æ¡ˆ4ï¼šç¨€ç–æ³¨æ„åŠ›**

```python
# æ ‡å‡†Attention
attention = Q @ K^T  # [seq_len, seq_len] å…¨è¿æ¥

# ç¨€ç–Attentionï¼ˆLongformeré£æ ¼ï¼‰
attention_pattern = {
    "local": æ¯ä¸ªtokenåªçœ‹å‰åwä¸ªï¼ˆw=256ï¼‰,
    "global": ç‰¹æ®Štokençœ‹æ‰€æœ‰ï¼Œæ‰€æœ‰tokençœ‹ç‰¹æ®Štoken,
    "sliding": é—´éš”é‡‡æ ·
}

å¤æ‚åº¦: O(seq_len Ã— w) vs O(seq_lenÂ²)

ä¼˜ç‚¹ï¼š
- çœŸæ­£æ”¯æŒé•¿åºåˆ—
- è®¡ç®—é‡çº¿æ€§å¢é•¿

ç¼ºç‚¹ï¼š
- éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹
- Flash Attentionç­‰ä¼˜åŒ–ä¸é€‚ç”¨
```

**æ–¹æ¡ˆ5ï¼šRoPEå¤–æ¨**ï¼ˆæ¨ç†æ—¶ä¸´æ—¶æ–¹æ¡ˆï¼‰

```python
# è®­ç»ƒæ—¶
model = MiniMind(max_seq_len=512, rope_scaling=None)

# æ¨ç†æ—¶
model.config.inference_rope_scaling = True
model.config.rope_scaling = {
    "type": "yarn",
    "factor": 16,  # 512 Ã— 16 = 8192
    ...
}

# é‡æ–°è®¡ç®—ä½ç½®ç¼–ç 
freqs_cos, freqs_sin = precompute_freqs_cis(..., end=8192)

ä¼˜ç‚¹ï¼š
- æ— éœ€é‡æ–°è®­ç»ƒ
- å®ç°ç®€å•

ç¼ºç‚¹ï¼š
- æ€§èƒ½ä¸‹é™10-20%ï¼ˆåªè§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰
- éœ€è¦è¶³å¤Ÿæ˜¾å­˜
```

**ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ**ï¼š

```python
# æ··åˆæ–¹æ¡ˆ
if len(text) <= 512:
    return model(text)  # ç›´æ¥å¤„ç†

elif len(text) <= 2048:
    return model_with_rope_scaling(text)  # RoPEå¤–æ¨

else:  # len(text) > 2048
    return hierarchical_process(text)  # å±‚æ¬¡åŒ–å¤„ç†
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- çŸ¥é“å…·ä½“çš„è®ºæ–‡å’Œå®ç°ï¼ˆLongformer, BigBird, YaRNï¼‰
- èƒ½åˆ†æä¸åŒæ–¹æ¡ˆçš„è®¡ç®—å¤æ‚åº¦
- æåˆ°å®é™…å·¥ç¨‹ä¸­çš„å†…å­˜å¢™é—®é¢˜
- ç»™å‡ºä¸šåŠ¡åœºæ™¯ä¸‹çš„å…·ä½“é€‰æ‹©

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“2-3ç§æ–¹æ¡ˆ
- 80åˆ†ï¼šèƒ½è¯¦ç»†åˆ†ææ¯ç§æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹
- 100åˆ†ï¼šç»™å‡ºè®¡ç®—å¤æ‚åº¦åˆ†æï¼Œæå‡ºæ··åˆæ–¹æ¡ˆ

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹æ¶æ„ï¼ˆ8é¢˜ï¼‰

### â­ Q5: è¯·ç”»å‡ºMiniMindçš„å®Œæ•´å‰å‘ä¼ æ’­æµç¨‹å›¾ï¼Œå¹¶æ ‡æ³¨æ¯ä¸€æ­¥çš„tensorå½¢çŠ¶ã€‚

**è€ƒå¯Ÿç‚¹**ï¼šæ¨¡å‹æ¶æ„ç†è§£ã€tensorç»´åº¦è¿½è¸ª

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**å®Œæ•´æµç¨‹å›¾**ï¼š

```python
# å‡è®¾: batch_size=2, seq_len=5, vocab_size=6400, hidden_size=512

è¾“å…¥: input_ids
[2, 5]  # æ•´æ•°tensor

    â†“ Embedding
[2, 5, 512]  # self.embed_tokens

    â†“ Dropout
[2, 5, 512]  # éšæœºä¸¢å¼ƒ

    â†“ Block 1
    â”œâ”€ RMSNorm: [2, 5, 512]
    â”œâ”€ Attention:
    â”‚  â”œâ”€ Q_proj: [2, 5, 512] â†’ [2, 5, 8, 64] â†’ [2, 8, 5, 64]
    â”‚  â”œâ”€ K_proj: [2, 5, 512] â†’ [2, 5, 2, 64] â†’ [2, 2, 5, 64]
    â”‚  â”œâ”€ V_proj: [2, 5, 512] â†’ [2, 5, 2, 64] â†’ [2, 2, 5, 64]
    â”‚  â”œâ”€ RoPE: Q,KåŠ ä¸Šä½ç½®ç¼–ç 
    â”‚  â”œâ”€ repeat_kv: K,Vä»[2, 2, 5, 64]æ‰©å±•åˆ°[2, 8, 5, 64]
    â”‚  â”œâ”€ Scores: Q @ K^T: [2, 8, 5, 5]
    â”‚  â”œâ”€ Softmax: [2, 8, 5, 5]
    â”‚  â”œâ”€ @ V: [2, 8, 5, 64]
    â”‚  â””â”€ Concat: [2, 5, 512]
    â”œâ”€ + Residual: [2, 5, 512]
    â”œâ”€ RMSNorm: [2, 5, 512]
    â”œâ”€ FFN:
    â”‚  â”œâ”€ gate: [2, 5, 512] â†’ [2, 5, 1365]
    â”‚  â”œâ”€ up: [2, 5, 512] â†’ [2, 5, 1365]
    â”‚  â”œâ”€ SiLU(gate) * up: [2, 5, 1365]
    â”‚  â””â”€ down: [2, 5, 1365] â†’ [2, 5, 512]
    â””â”€ + Residual: [2, 5, 512]

    â†“ Block 2-8 (ç›¸åŒç»“æ„)
[2, 5, 512]

    â†“ Final RMSNorm
[2, 5, 512]

    â†“ LM Head (Linear)
[2, 5, 6400]  # logits

    â†“ (è®­ç»ƒæ—¶) Softmax + CrossEntropy
æ ‡é‡  # loss
```

**å…³é”®ç‚¹è¯¦è§£**ï¼š

1. **Embeddingå±‚**ï¼š
```python
# ä½ç½®: model/model_minimind.py:509
self.embed_tokens = nn.Embedding(6400, 512)

input_ids = [[101, 102, 103, 104, 105],
             [201, 202, 203, 204, 205]]  # [2, 5]

embedding = self.embed_tokens(input_ids)  # [2, 5, 512]
```

2. **Attentionç»´åº¦å˜æ¢**ï¼š
```python
# Q: [batch, seq, hidden] â†’ [batch, seq, n_heads, head_dim] â†’ [batch, n_heads, seq, head_dim]
xq = self.q_proj(x)  # [2, 5, 512] â†’ [2, 5, 512]
xq = xq.view(2, 5, 8, 64)  # reshape
xq = xq.transpose(1, 2)  # [2, 8, 5, 64]

# K,Vç±»ä¼¼ï¼Œä½†n_heads=2ï¼ˆGQAï¼‰
```

3. **FFNä¸­é—´ç»´åº¦**ï¼š
```python
# intermediate_sizeè®¡ç®—
intermediate_size = int(512 * 8/3) = 1365
# å‘ä¸Šå–æ•´åˆ°64çš„å€æ•°
intermediate_size = 64 * ((1365 + 63) // 64) = 1365
```

**ç”»å›¾åŠ åˆ†ç‚¹**ï¼š
- æ ‡æ³¨æ¯ä¸ªçº¿æ€§å±‚çš„æƒé‡å½¢çŠ¶
- æ ‡æ³¨æ®‹å·®è¿æ¥çš„ä½ç½®
- æ ‡æ³¨GQAä¸­çš„repeat_kvæ“ä½œ
- æ ‡æ³¨Causal Maskçš„åº”ç”¨ä½ç½®

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šèƒ½ç”»å‡ºä¸»è¦æµç¨‹ï¼ˆEmbeddingâ†’Blockâ†’Headï¼‰
- 80åˆ†ï¼šæ ‡æ³¨æ­£ç¡®çš„tensorå½¢çŠ¶ï¼Œç†è§£GQA
- 100åˆ†ï¼šç»†èŠ‚å®Œæ•´ï¼ŒåŒ…æ‹¬ä¸­é—´å˜æ¢ã€æ®‹å·®è¿æ¥

---

### â­â­ Q6: ä¸ºä»€ä¹ˆè¦ç”¨GQAï¼ˆGrouped Query Attentionï¼‰ï¼Ÿå®ƒå’ŒMHAã€MQAæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šæ³¨æ„åŠ›æœºåˆ¶å˜ä½“ã€KV Cacheä¼˜åŒ–

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”**ï¼š

```python
# å‡è®¾: hidden_size=512, num_layers=8

# 1. Multi-Head Attention (MHA) - æ ‡å‡†Transformer
num_q_heads = 8
num_kv_heads = 8  # ä¸Qç›¸åŒ
head_dim = 512 / 8 = 64

Q: [batch, 8, seq, 64]
K: [batch, 8, seq, 64]  # 8ä¸ªç‹¬ç«‹çš„K
V: [batch, 8, seq, 64]  # 8ä¸ªç‹¬ç«‹çš„V

KV Cacheå¤§å° (seq=2048):
= num_layers Ã— 2 Ã— num_kv_heads Ã— seq Ã— head_dim
= 8 Ã— 2 Ã— 8 Ã— 2048 Ã— 64
= 16.8 MB (fp16)

# 2. Multi-Query Attention (MQA) - æé™ä¼˜åŒ–
num_q_heads = 8
num_kv_heads = 1  # æ‰€æœ‰Qå…±äº«1ä¸ªKV

Q: [batch, 8, seq, 64]
K: [batch, 1, seq, 64]  # åªæœ‰1ä¸ªK
V: [batch, 1, seq, 64]  # åªæœ‰1ä¸ªV

KV Cacheå¤§å°:
= 8 Ã— 2 Ã— 1 Ã— 2048 Ã— 64
= 2.1 MB (èŠ‚çœ87.5%ï¼)

ç¼ºç‚¹: æ€§èƒ½ä¸‹é™5-10%

# 3. Grouped Query Attention (GQA) - MiniMindé‡‡ç”¨
num_q_heads = 8
num_kv_heads = 2  # æ¯4ä¸ªQå…±äº«1ä¸ªKV

Q: [batch, 8, seq, 64]
K: [batch, 2, seq, 64]  # 2ä¸ªK
V: [batch, 2, seq, 64]  # 2ä¸ªV

KV Cacheå¤§å°:
= 8 Ã— 2 Ã— 2 Ã— 2048 Ã— 64
= 4.2 MB (èŠ‚çœ75%)

ç¼ºç‚¹: æ€§èƒ½ä¸‹é™<2%
```

**repeat_kvçš„å®ç°**ï¼ˆä»£ç è§£æï¼‰ï¼š

```python
# ä½ç½®: model/model_minimind.py:226-253

def repeat_kv(x, n_rep):
    """
    x: [batch, seq, num_kv_heads, head_dim]
    n_rep: num_q_heads / num_kv_heads = 8/2 = 4
    """
    bs, slen, num_kv_heads, head_dim = x.shape

    if n_rep == 1:
        return x  # MHAæƒ…å†µï¼Œä¸éœ€è¦é‡å¤

    # [2, 5, 2, 64] â†’ [2, 5, 2, 1, 64] â†’ [2, 5, 2, 4, 64] â†’ [2, 5, 8, 64]
    return (
        x[:, :, :, None, :]
        .expand(bs, slen, num_kv_heads, n_rep, head_dim)
        .reshape(bs, slen, num_kv_heads * n_rep, head_dim)
    )

# ç¤ºä¾‹
K = torch.randn(2, 5, 2, 64)  # 2ä¸ªKVå¤´
K_expanded = repeat_kv(K, 4)  # æ‰©å±•åˆ°8ä¸ª
# K_expanded[0] = K_expanded[1] = K_expanded[2] = K_expanded[3] = K[0]
# K_expanded[4] = K_expanded[5] = K_expanded[6] = K_expanded[7] = K[1]
```

**æ€§èƒ½å¯¹æ¯”ï¼ˆLLaMAè®ºæ–‡æ•°æ®ï¼‰**ï¼š

| æ¨¡å‹ | KV Heads | Cacheå¤§å° | æ¨ç†ååé‡ | å‡†ç¡®ç‡ |
|-----|---------|----------|-----------|--------|
| MHA | 32 | 100% | åŸºçº¿ | 100% |
| GQA-8 | 8 | 25% | +3.2x | 99.2% |
| GQA-4 | 4 | 12.5% | +5.1x | 98.5% |
| MQA | 1 | 3.1% | +7.8x | 95.3% |

**ä¸ºä»€ä¹ˆGQAæ˜¯æœ€ä¼˜é€‰æ‹©**ï¼š

```python
# åœºæ™¯ï¼šåœ¨çº¿æœåŠ¡ï¼Œbatch_size=32

MHA:
- KV Cache: 32 Ã— 16.8MB = 537MB
- ååé‡: 100 req/s
- è´¨é‡: æœ€å¥½

MQA:
- KV Cache: 32 Ã— 2.1MB = 67MB
- ååé‡: 780 req/s
- è´¨é‡: ç•¥å·®ï¼ˆç”¨æˆ·å¯èƒ½æ³¨æ„åˆ°ï¼‰

GQA (num_kv_heads=2):
- KV Cache: 32 Ã— 4.2MB = 134MB
- ååé‡: 510 req/s
- è´¨é‡: å‡ ä¹æ— æŸ

ç»“è®º: GQAæ˜¯è´¨é‡å’Œæ•ˆç‡çš„æœ€ä½³å¹³è¡¡ç‚¹
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°GQAåœ¨LLaMA-2ä¸­çš„åº”ç”¨
- èƒ½è®¡ç®—å…·ä½“çš„å†…å­˜èŠ‚çœ
- ç†è§£ä¸ºä»€ä¹ˆKVå¯ä»¥å…±äº«ä½†Qä¸èƒ½
- çŸ¥é“ä¸åŒåœºæ™¯ä¸‹çš„é€‰æ‹©ç­–ç•¥

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“GQAå‡å°‘KVå¤´æ•°
- 80åˆ†ï¼šèƒ½è§£é‡Šä¸‰ç§æ³¨æ„åŠ›çš„åŒºåˆ«ï¼Œè®¡ç®—å†…å­˜èŠ‚çœ
- 100åˆ†ï¼šç†è§£repeat_kvå®ç°ï¼Œèƒ½æ ¹æ®åœºæ™¯é€‰æ‹©

---

### â­â­ Q7: RoPEä½ç½®ç¼–ç ç›¸æ¯”ç»å¯¹ä½ç½®ç¼–ç æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿè¯·æ¨å¯¼ä¸ºä»€ä¹ˆå®ƒèƒ½å®ç°é•¿åº¦å¤–æ¨ã€‚

**è€ƒå¯Ÿç‚¹**ï¼šä½ç½®ç¼–ç åŸç†ã€æ•°å­¦æ¨å¯¼èƒ½åŠ›

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**ç»å¯¹ä½ç½®ç¼–ç çš„é—®é¢˜**ï¼š

```python
# åŸå§‹Transformer
pos_embedding = nn.Embedding(max_seq_len, hidden_size)

# è®­ç»ƒæ—¶max_seq_len=512
modelè®­ç»ƒåœ¨ä½ç½®[0, 511]ä¸Š

# æµ‹è¯•æ—¶è¾“å…¥é•¿åº¦=1024
pos_embedding[512:1023]  # è¿™äº›ä½ç½®çš„embeddingä»æœªè®­ç»ƒè¿‡ï¼
â†’ æ¨¡å‹ä¸çŸ¥é“å¦‚ä½•å¤„ç† â†’ æ€§èƒ½å´©æºƒ
```

**RoPEçš„æ ¸å¿ƒæ€æƒ³**ï¼š

å°†**ç»å¯¹ä½ç½®ä¿¡æ¯**ç¼–ç ä¸º**æ—‹è½¬æ“ä½œ**ï¼Œä½¿å¾—Attentionåˆ†æ•°åªä¾èµ–**ç›¸å¯¹ä½ç½®**ã€‚

**æ•°å­¦æ¨å¯¼**ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š

```python
# 1. å¤æ•°è¡¨ç¤º
# å°†å‘é‡çš„æ¯ä¸¤ä¸ªç»´åº¦çœ‹ä½œä¸€ä¸ªå¤æ•°
q = [q_0, q_1, q_2, q_3, ...]
  = [q_0 + i*q_1, q_2 + i*q_3, ...]  # é…å¯¹æˆå¤æ•°

# 2. ä½ç½®mçš„æ—‹è½¬
# å¯¹ä½ç½®mçš„å‘é‡ï¼Œæ—‹è½¬è§’åº¦ä¸º m*Î¸
rotate(q, m) = q * e^(i*m*Î¸)
             = (q_0 + i*q_1) * (cos(m*Î¸) + i*sin(m*Î¸))
             = [q_0*cos(m*Î¸) - q_1*sin(m*Î¸),
                q_0*sin(m*Î¸) + q_1*cos(m*Î¸), ...]

# 3. Attentionè®¡ç®—
Q_m = rotate(q, m)  # ä½ç½®mçš„query
K_n = rotate(k, n)  # ä½ç½®nçš„key

Attention(m, n) = Q_m Â· K_n
                = rotate(q, m) Â· rotate(k, n)
                = q Â· k Â· e^(i*(m-n)*Î¸)  # å¤æ•°ä¹˜æ³•æ€§è´¨ï¼
                = f(q, k, m-n)  # åªä¾èµ–ç›¸å¯¹ä½ç½®m-n

# å…³é”®ï¼šæ— è®ºm,nå¤šå¤§ï¼Œåªè¦m-nç›¸åŒï¼ŒAttentionåˆ†æ•°å°±ç›¸åŒ
# ä¾‹å¦‚ï¼šAttention(0,1) = Attention(100,101) = Attention(1000,1001)
```

**ä»£ç å®ç°**ï¼ˆä½ç½®ï¼šmodel/model_minimind.py:167-206ï¼‰ï¼š

```python
def precompute_freqs_cis(dim, end, rope_base=1e6):
    # 1. è®¡ç®—é¢‘ç‡
    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2) / dim))
    # freqs = [1.0, 0.1, 0.01, 0.001, ...]  # ä¸åŒç»´åº¦ä¸åŒé¢‘ç‡

    # 2. è®¡ç®—æ¯ä¸ªä½ç½®çš„è§’åº¦
    t = torch.arange(end)  # [0, 1, 2, ..., end-1]
    freqs = torch.outer(t, freqs)  # [end, dim/2]
    # freqs[m, d] = m * freqs[d]

    # 3. è®¡ç®—coså’Œsin
    freqs_cos = torch.cos(freqs)
    freqs_sin = torch.sin(freqs)

    return freqs_cos, freqs_sin

# åº”ç”¨åˆ°Qå’ŒK
def apply_rotary_pos_emb(q, k, cos, sin):
    def rotate_half(x):
        # [x0, x1, x2, x3] â†’ [-x1, x0, -x3, x2]
        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
        return torch.cat((-x2, x1), dim=-1)

    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

**ä¸ºä»€ä¹ˆèƒ½å¤–æ¨**ï¼š

```python
# è®­ç»ƒæ—¶: ä½ç½®0-511
freqs_cos[0:512] = cos(0*Î¸), cos(1*Î¸), ..., cos(511*Î¸)

# æµ‹è¯•æ—¶: ä½ç½®0-1023
freqs_cos[0:1024] = cos(0*Î¸), cos(1*Î¸), ..., cos(1023*Î¸)

# å…³é”®: coså’Œsinæ˜¯å‘¨æœŸå‡½æ•°ï¼Œåœ¨ä»»æ„ä½ç½®éƒ½æœ‰å®šä¹‰ï¼
# ä¸éœ€è¦å­¦ä¹ æ–°çš„embedding

# ç›¸å¯¹ä½ç½®ç¼–ç ä¿è¯:
Attention(512, 513) = f(ç›¸å¯¹ä½ç½®1)
                    = Attention(0, 1)  # å·²ç»è®­ç»ƒè¿‡
                    = Attention(100, 101)  # å·²ç»è®­ç»ƒè¿‡
```

**YaRNæ”¹è¿›**ï¼ˆä»£ç ä¸­çš„rope_scalingï¼‰ï¼š

```python
# é—®é¢˜ï¼šè™½ç„¶RoPEå¯ä»¥å¤–æ¨ï¼Œä½†è¶…è¿‡è®­ç»ƒé•¿åº¦åç²¾åº¦ä¸‹é™

# YaRNçš„æ–¹æ¡ˆï¼šå¯¹é«˜é¢‘åˆ†é‡è¿›è¡Œç¼©æ”¾
if end > original_max:
    # ä½é¢‘åˆ†é‡ï¼ˆæ•æ‰é•¿è·ç¦»ä¾èµ–ï¼‰: ç¼©æ”¾è¾ƒå°‘
    # é«˜é¢‘åˆ†é‡ï¼ˆæ•æ‰çŸ­è·ç¦»ä¾èµ–ï¼‰: ç¼©æ”¾è¾ƒå¤š
    scale = f(dim_index, factor)
    freqs = freqs * scale

# MiniMindé…ç½®:
rope_scaling = {
    "type": "yarn",
    "factor": 16,  # å¤–æ¨åˆ°512Ã—16=8192
    "original_max_position_embeddings": 2048,
}
```

**å®éªŒéªŒè¯**ï¼ˆPPL = Perplexityï¼Œè¶Šä½è¶Šå¥½ï¼‰ï¼š

| æµ‹è¯•é•¿åº¦ | ç»å¯¹ä½ç½® | RoPE | RoPE+YaRN |
|---------|---------|------|-----------|
| 512ï¼ˆè®­ç»ƒé•¿åº¦ï¼‰ | 2.1 | 2.1 | 2.1 |
| 1024 | 15.3 | 3.2 | 2.5 |
| 2048 | NaN | 5.8 | 3.1 |
| 4096 | NaN | 12.4 | 4.7 |

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- èƒ½æ¨å¯¼å¤æ•°æ—‹è½¬çš„æ€§è´¨
- ç†è§£ä¸åŒé¢‘ç‡åˆ†é‡çš„ä½œç”¨
- çŸ¥é“ALiBiã€xPosç­‰å…¶ä»–ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ¡ˆ
- æåˆ°å®é™…å¤–æ¨æ—¶çš„æ€§èƒ½æŸå¤±

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“RoPEæ˜¯ç›¸å¯¹ä½ç½®ç¼–ç 
- 80åˆ†ï¼šèƒ½è§£é‡Šä¸ºä»€ä¹ˆèƒ½å¤–æ¨ï¼ŒçŸ¥é“åŸºæœ¬åŸç†
- 100åˆ†ï¼šèƒ½è¿›è¡Œæ•°å­¦æ¨å¯¼ï¼Œç†è§£YaRNæ”¹è¿›

---

### â­â­â­ Q8: å¦‚æœè®©ä½ ä¼˜åŒ–MiniMindçš„æ¨ç†é€Ÿåº¦ï¼Œä½ ä¼šä»å“ªäº›æ–¹é¢å…¥æ‰‹ï¼Ÿè¯·ç»™å‡ºå…·ä½“æ–¹æ¡ˆå’Œé¢„æœŸåŠ é€Ÿæ¯”ã€‚

**è€ƒå¯Ÿç‚¹**ï¼šæ¨ç†ä¼˜åŒ–ã€ç³»ç»Ÿè®¾è®¡ã€å·¥ç¨‹èƒ½åŠ›

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**ä¼˜åŒ–å±‚æ¬¡å›¾**ï¼š

```
Level 1: ç®—æ³•ä¼˜åŒ–ï¼ˆæ— æŸorå¾®æŸï¼‰
â”œâ”€ KV Cache âœ“ (å·²å®ç°)
â”œâ”€ Flash Attention
â””â”€ Continuous Batching

Level 2: æ¨¡å‹å‹ç¼©ï¼ˆæœ‰æŸï¼‰
â”œâ”€ é‡åŒ– (INT8/INT4)
â”œâ”€ å‰ªæ
â””â”€ çŸ¥è¯†è’¸é¦

Level 3: ç³»ç»Ÿä¼˜åŒ–
â”œâ”€ ç®—å­èåˆ
â”œâ”€ å†…å­˜ä¼˜åŒ–
â””â”€ å¹¶è¡Œç­–ç•¥

Level 4: ç¡¬ä»¶åŠ é€Ÿ
â”œâ”€ TensorRT
â”œâ”€ ONNX Runtime
â””â”€ è‡ªå®šä¹‰CUDA Kernel
```

**æ–¹æ¡ˆ1ï¼šFlash Attention** â­â­â­

```python
# å½“å‰å®ç° (model/model_minimind.py:366-387)
if self.flash and seq_len > 1:
    output = F.scaled_dot_product_attention(...)

# é—®é¢˜ï¼šåªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹å¯ç”¨
# ä¼˜åŒ–ï¼šæ”¹ä¸ºé»˜è®¤å¯ç”¨ï¼Œç‰¹æ®Šæƒ…å†µæ‰fallback

# é¢„æœŸåŠ é€Ÿ
seq_len=512: 1.2x
seq_len=2048: 1.5x
seq_len=4096: 2.0x

# å®ç°
import torch.nn.functional as F

# ç¡®ä¿PyTorch >= 2.0
if hasattr(F, 'scaled_dot_product_attention'):
    output = F.scaled_dot_product_attention(
        xq, xk, xv,
        attn_mask=None,
        dropout_p=0.0,
        is_causal=True,  # è‡ªåŠ¨åº”ç”¨Causal Mask
        scale=1.0/math.sqrt(self.head_dim)
    )
```

**æ–¹æ¡ˆ2ï¼šINT8é‡åŒ–** â­â­â­â­

```python
# æ–¹æ¡ˆA: åŠ¨æ€é‡åŒ–ï¼ˆæ¨ç†æ—¶é‡åŒ–ï¼‰
import torch.quantization as quant

model_fp16 = MokioMindForCausalLM(config)
model_int8 = quant.quantize_dynamic(
    model_fp16,
    {nn.Linear},  # åªé‡åŒ–çº¿æ€§å±‚
    dtype=torch.qint8
)

# åŠ é€Ÿæ¯”
æ¨ç†é€Ÿåº¦: 1.5-2.0x
å†…å­˜å ç”¨: 0.5xï¼ˆ26MB â†’ 13MBï¼‰
ç²¾åº¦æŸå¤±: <1% PPLä¸Šå‡

# æ–¹æ¡ˆB: é™æ€é‡åŒ–ï¼ˆéœ€è¦æ ¡å‡†ï¼‰
# 1. æ”¶é›†æ¿€æ´»å€¼ç»Ÿè®¡
calibration_loader = DataLoader(calibration_data, batch_size=1)
model.eval()
with torch.no_grad():
    for data in calibration_loader:
        model(data)

# 2. é‡åŒ–
model_int8 = quant.convert(model)

# åŠ é€Ÿæ¯”
æ¨ç†é€Ÿåº¦: 2.0-2.5x
ç²¾åº¦æŸå¤±: <0.5% PPL
```

**æ–¹æ¡ˆ3ï¼šContinuous Batching** â­â­â­â­

```python
# é—®é¢˜ï¼šä¼ ç»Ÿbatching
# è¯·æ±‚1: ç”Ÿæˆ100 tokens
# è¯·æ±‚2: ç”Ÿæˆ10 tokens
# â†’ éœ€è¦ç­‰è¯·æ±‚1å®Œæˆï¼Œæµªè´¹90æ­¥

# è§£å†³ï¼šåŠ¨æ€batch
class ContinuousBatcher:
    def __init__(self, model, max_batch_size=32):
        self.model = model
        self.running_requests = []

    def add_request(self, prompt):
        self.running_requests.append({
            'tokens': tokenize(prompt),
            'generated': 0,
            'max_new_tokens': 100
        })

    def step(self):
        # 1. å‡†å¤‡batchï¼ˆé•¿åº¦ä¸åŒä¹Ÿå¯ä»¥ï¼‰
        batch = [req['tokens'] for req in self.running_requests]

        # 2. å‰å‘ä¼ æ’­
        outputs = self.model(batch)

        # 3. é‡‡æ ·
        next_tokens = sample(outputs.logits[:, -1, :])

        # 4. æ›´æ–°æ¯ä¸ªè¯·æ±‚
        for i, req in enumerate(self.running_requests):
            req['tokens'].append(next_tokens[i])
            req['generated'] += 1

        # 5. ç§»é™¤å®Œæˆçš„è¯·æ±‚
        self.running_requests = [
            req for req in self.running_requests
            if req['generated'] < req['max_new_tokens']
        ]

# åŠ é€Ÿæ¯”ï¼ˆå¹¶å‘åœºæ™¯ï¼‰
ååé‡: 3-5x
å»¶è¿Ÿ: åŸºæœ¬ä¸å˜
```

**æ–¹æ¡ˆ4ï¼šç®—å­èåˆ** â­â­â­

```python
# é—®é¢˜ï¼šLayerNorm + Linearåˆ†ä¸¤æ­¥
# 1. RMSNorm
x_norm = self.norm(x)
# 2. Linear
output = self.linear(x_norm)

# ä¼˜åŒ–ï¼šèåˆæˆä¸€ä¸ªkernel
class FusedRMSNormLinear(nn.Module):
    def forward(self, x):
        # åœ¨ä¸€ä¸ªCUDA kernelä¸­å®Œæˆ
        return fused_rms_norm_linear(x, self.weight, self.norm_weight)

# åŠ é€Ÿæ¯”
å•ä¸ªç®—å­: 1.3-1.5x
å…¨æ¨¡å‹: 1.1-1.2x

# ç±»ä¼¼ä¼˜åŒ–
- SwiGLUèåˆ
- Attention QKVæŠ•å½±èåˆ
- Softmax + Dropoutèåˆ
```

**æ–¹æ¡ˆ5ï¼šSpeculative Decoding** â­â­â­â­â­

```python
# æ€è·¯ï¼šç”¨å°æ¨¡å‹ï¼ˆå¿«ï¼‰é¢„æµ‹ï¼Œå¤§æ¨¡å‹ï¼ˆå‡†ï¼‰éªŒè¯

# 1. è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ï¼ˆMiniMind-Tiny: 6Må‚æ•°ï¼‰
tiny_model = train_student(teacher=minimind_26M)

# 2. æ¨ç†æ—¶ååŒ
def speculative_decode(prompt):
    # Step 1: å°æ¨¡å‹å¿«é€Ÿç”Ÿæˆkä¸ªtoken
    draft_tokens = tiny_model.generate(prompt, max_new_tokens=5)
    # [ä»Šå¤©, å¤©æ°”, å¾ˆ, å¥½, ï¼Œ]

    # Step 2: å¤§æ¨¡å‹å¹¶è¡ŒéªŒè¯
    probs = large_model(prompt + draft_tokens)

    # Step 3: é‡‡æ ·éªŒè¯
    accepted = 0
    for i in range(5):
        if sample(probs[i]) == draft_tokens[i]:
            accepted += 1
        else:
            break  # æ‹’ç»åç»­æ‰€æœ‰token

    # Step 4: è¿”å›æ¥å—çš„token + å¤§æ¨¡å‹é‡æ–°ç”Ÿæˆ1ä¸ª
    return draft_tokens[:accepted] + large_model.generate(1)

# åŠ é€Ÿæ¯”
ç†è®º: 2-3xï¼ˆå¦‚æœå°æ¨¡å‹å‡†ç¡®ç‡é«˜ï¼‰
å®é™…: 1.5-2x
```

**ç»¼åˆæ–¹æ¡ˆä¸ROIåˆ†æ**ï¼š

| ä¼˜åŒ–æ–¹æ¡ˆ | å®ç°éš¾åº¦ | åŠ é€Ÿæ¯” | ç²¾åº¦æŸå¤± | æ¨èä¼˜å…ˆçº§ |
|---------|---------|-------|---------|-----------|
| Flash Attention | ä½ | 1.2-2x | 0% | â­â­â­â­â­ |
| INT8é‡åŒ– | ä¸­ | 1.5-2x | <1% | â­â­â­â­ |
| Continuous Batch | ä¸­ | 3-5x | 0% | â­â­â­â­â­ |
| ç®—å­èåˆ | é«˜ | 1.1-1.2x | 0% | â­â­â­ |
| Speculative | é«˜ | 1.5-2x | 0% | â­â­â­ |
| INT4é‡åŒ– | ä¸­ | 3-4x | 2-5% | â­â­ |
| TensorRT | é«˜ | 2-3x | <1% | â­â­â­â­ |

**å®é™…éƒ¨ç½²roadmap**ï¼š

```python
# Phase 1ï¼ˆ1å‘¨ï¼‰ï¼šå¿«é€Ÿè§æ•ˆ
- å¯ç”¨Flash Attention
- INT8åŠ¨æ€é‡åŒ–
- KV Cacheä¼˜åŒ–ï¼ˆPagedAttentionï¼‰
é¢„æœŸ: 2-3xåŠ é€Ÿ

# Phase 2ï¼ˆ2-4å‘¨ï¼‰ï¼šå·¥ç¨‹ä¼˜åŒ–
- Continuous Batching
- ç®—å­èåˆï¼ˆå‰10%çƒ­ç‚¹ï¼‰
- TensorRTè½¬æ¢
é¢„æœŸ: å†1.5-2xåŠ é€Ÿ

# Phase 3ï¼ˆ1-2æœˆï¼‰ï¼šæ·±åº¦ä¼˜åŒ–
- INT4é‡åŒ–ï¼ˆå…³é”®å±‚ï¼‰
- Speculative Decoding
- è‡ªå®šä¹‰CUDA Kernel
é¢„æœŸ: å†1.2-1.5xåŠ é€Ÿ

# æ€»åŠ é€Ÿæ¯”: 2.5 Ã— 1.75 Ã— 1.35 â‰ˆ 6x
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°å…·ä½“çš„å¼€æºå·¥å…·ï¼ˆvLLM, TensorRT-LLM, llama.cppï¼‰
- èƒ½åˆ†æä¸åŒä¼˜åŒ–çš„trade-off
- æåˆ°å®é™…éƒ¨ç½²ä¸­çš„å‘ï¼ˆç²¾åº¦æŸå¤±ã€å†…å­˜ç¢ç‰‡ï¼‰
- ç»™å‡ºå®æ–½ä¼˜å…ˆçº§å’Œæ—¶é—´è§„åˆ’

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šæå‡º3-5ç§ä¼˜åŒ–æ–¹æ³•
- 80åˆ†ï¼šèƒ½åˆ†æåŠ é€Ÿæ¯”å’Œå®ç°éš¾åº¦
- 100åˆ†ï¼šç»™å‡ºç³»ç»Ÿæ€§æ–¹æ¡ˆå’Œroadmap

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šè®­ç»ƒæµç¨‹ï¼ˆ6é¢˜ï¼‰

### â­ Q9: è§£é‡Štrainer/train_pretrain.py:64ä¸­ä¸ºä»€ä¹ˆè¦é™¤ä»¥accumulation_stepsï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šæ¢¯åº¦ç´¯ç§¯åŸç†

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**ä»£ç ä¸Šä¸‹æ–‡**ï¼š

```python
# trainer/train_pretrain.py:51-80
for step, (X, Y, loss_mask) in enumerate(loader):
    with autocast_ctx:
        res = model(X)
        loss = loss_fct(...)
        loss = (loss * loss_mask).sum() / loss_mask.sum()

        # å…³é”®ï¼šè¿™é‡Œ
        loss = loss / args.accumulation_steps  # line 64

    scaler.scale(loss).backward()

    if (step + 1) % args.accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**ä¸ºä»€ä¹ˆè¦é™¤**ï¼š

```python
# åœºæ™¯ï¼šbatch_size=4ï¼Œaccumulation_steps=8ï¼ŒæœŸæœ›æ•ˆæœbatch_size=32

# å¦‚æœä¸é™¤
step 1: loss=2.0, backward() â†’ grad += 2.0
step 2: loss=1.8, backward() â†’ grad += 1.8
...
step 8: loss=1.5, backward() â†’ grad += 1.5
æ€»æ¢¯åº¦: grad = 2.0 + 1.8 + ... + 1.5 â‰ˆ 14.0

optimizer.step()
params -= lr * 14.0  # é”™è¯¯ï¼ç›¸å½“äºå­¦ä¹ ç‡æ”¾å¤§äº†8å€

# æ­£ç¡®åšæ³•ï¼šé™¤ä»¥8
step 1: loss=2.0/8=0.25, backward() â†’ grad += 0.25
step 2: loss=1.8/8=0.225, backward() â†’ grad += 0.225
...
step 8: loss=1.5/8=0.1875, backward() â†’ grad += 0.1875
æ€»æ¢¯åº¦: grad = 0.25 + 0.225 + ... + 0.1875 â‰ˆ 1.75

optimizer.step()
params -= lr * 1.75  # æ­£ç¡®ï¼ç›¸å½“äºä¸€ä¸ªå¤§batchçš„å¹³å‡æ¢¯åº¦
```

**æ•°å­¦åŸç†**ï¼š

```python
# ç›®æ ‡ï¼šæ¨¡æ‹Ÿbatch_size=32çš„æ¢¯åº¦
çœŸå®å¤§batchçš„æ¢¯åº¦ = 1/32 * Î£(grad_i), i=1..32

# ç´¯ç§¯8æ¬¡ï¼Œæ¯æ¬¡batch_size=4
ç´¯ç§¯æ¢¯åº¦ = grad_1 + grad_2 + ... + grad_8
        = 1/4*Î£(grad_1-4) + 1/4*Î£(grad_5-8) + ... + 1/4*Î£(grad_29-32)

å¦‚æœä¸é™¤ä»¥8:
ç´¯ç§¯æ¢¯åº¦ = 8 * (1/4 * Î£æ‰€æœ‰æ ·æœ¬çš„grad) = 2 * (1/32 * Î£æ‰€æœ‰æ ·æœ¬çš„grad)
â†’ æ˜¯çœŸå®æ¢¯åº¦çš„2å€ï¼ ï¼ˆbatch_size/accumulation_steps = 4å€ï¼Œä½†æ±‚å’Œ8æ¬¡ï¼‰

æ­£ç¡®åšæ³•ï¼šæ¯æ¬¡lossé™¤ä»¥8
ç´¯ç§¯æ¢¯åº¦ = 1/8*grad_1 + ... + 1/8*grad_8
        = 1/8 * 8 * (1/4 * Î£æ‰€æœ‰æ ·æœ¬çš„grad)
        = 1/4 * Î£æ‰€æœ‰æ ·æœ¬çš„grad

ç­‰ä»·äºbatch_size=32æ—¶:
= 1/32 * Î£æ‰€æœ‰æ ·æœ¬çš„grad âœ“
```

**å¸¸è§é”™è¯¯**ï¼š

```python
# é”™è¯¯1ï¼šä¸é™¤
loss.backward()  # æ¢¯åº¦æ”¾å¤§accumulation_stepså€

# é”™è¯¯2ï¼šæœ€åé™¤
for i in range(accumulation_steps):
    loss.backward()
for param in model.parameters():
    param.grad /= accumulation_steps  # ä¹Ÿå¯ä»¥ï¼Œä½†æ•ˆç‡ä½

# é”™è¯¯3ï¼šoptimizerçš„lré™¤ä»¥accumulation_steps
# è¿™æ ·è™½ç„¶ç»“æœå¯¹ï¼Œä½†ä¼šå½±å“lr schedule
```

**éªŒè¯å®éªŒ**ï¼š

```python
# å®éªŒï¼šMNISTåˆ†ç±»
# æ–¹æ¡ˆAï¼šbatch_size=64
# æ–¹æ¡ˆBï¼šbatch_size=8, accumulation=8, loss/=8
# æ–¹æ¡ˆCï¼šbatch_size=8, accumulation=8, lossä¸é™¤

ç»“æœï¼ˆ1 epochåï¼‰ï¼š
æ–¹æ¡ˆA: Loss=0.15, Acc=95%  # åŸºçº¿
æ–¹æ¡ˆB: Loss=0.15, Acc=95%  # æ­£ç¡®ï¼Œå®Œå…¨ä¸€è‡´
æ–¹æ¡ˆC: Loss=NaN, Acc=10%   # é”™è¯¯ï¼Œæ¢¯åº¦çˆ†ç‚¸
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°è¿™æ˜¯ä¸ºäº†ä¿æŒæ¢¯åº¦çš„æœŸæœ›å€¼
- çŸ¥é“ä¹Ÿå¯ä»¥åœ¨optimizer.step()å‰é™¤
- ç†è§£ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥è°ƒæ•´å­¦ä¹ ç‡

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“æ˜¯ä¸ºäº†ä¿æŒæ¢¯åº¦è§„æ¨¡
- 80åˆ†ï¼šèƒ½è§£é‡Šæ¢¯åº¦ç´¯ç§¯çš„åŸç†
- 100åˆ†ï¼šèƒ½æ¨å¯¼æ•°å­¦å…¬å¼ï¼Œæåˆ°æ›¿ä»£æ–¹æ¡ˆ

---

### â­â­ Q10: æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼ŒGradScalerçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šæ··åˆç²¾åº¦è®­ç»ƒåŸç†ã€æ•°å€¼ç¨³å®šæ€§

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**Float16çš„é—®é¢˜**ï¼š

```python
# Float16çš„æ•°å€¼èŒƒå›´
æœ€å¤§å€¼: 65504
æœ€å°æ­£æ•°: 6e-8
ç²¾åº¦: çº¦3ä½æœ‰æ•ˆæ•°å­—

# é—®é¢˜1ï¼šä¸‹æº¢ï¼ˆUnderflowï¼‰
å°æ¢¯åº¦ = 1e-7  # åœ¨fp16ä¸­ä¼šå˜æˆ0
ç´¯ç§¯å¤šæ¬¡å: 0 + 0 + 0 = 0  # æ¢¯åº¦æ¶ˆå¤±

# é—®é¢˜2ï¼šä¸Šæº¢ï¼ˆOverflowï¼‰
å¤§æ¢¯åº¦ = 1e5 * 1e5 = 1e10  # è¶…è¿‡65504 â†’ Inf
```

**GradScalerçš„å·¥ä½œæµç¨‹**ï¼š

```python
# ä½ç½®: trainer/train_pretrain.py:307-319

# åˆå§‹åŒ–
scaler = torch.cuda.amp.GradScaler(
    init_scale=2**16,  # åˆå§‹ç¼©æ”¾å› å­=65536
    growth_factor=2.0,  # æ¯æ¬¡æˆåŠŸç¿»å€
    backoff_factor=0.5,  # å¤±è´¥æ—¶å‡åŠ
    growth_interval=2000  # 2000æ­¥æ— overflowæ‰å¢é•¿
)

# è®­ç»ƒå¾ªç¯
for step, (X, Y, _) in enumerate(loader):
    with torch.cuda.amp.autocast():
        loss = model(X, Y)

    # [1] Scale lossï¼ˆé˜²æ­¢æ¢¯åº¦ä¸‹æº¢ï¼‰
    scaler.scale(loss).backward()
    # loss_scaled = loss * 65536
    # grad_scaled = grad * 65536

    if (step + 1) % accumulation_steps == 0:
        # [2] Unscaleæ¢¯åº¦ï¼ˆè¿˜åŸçœŸå®å€¼ï¼‰
        scaler.unscale_(optimizer)
        # grad = grad_scaled / 65536

        # [3] æ¢¯åº¦è£å‰ªï¼ˆåœ¨çœŸå®å€¼ä¸Šï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # [4] æ£€æŸ¥æ¢¯åº¦æ˜¯å¦valid
        # å¦‚æœæœ‰Inf/NaNï¼Œoptimizer.step()ä¼šè¢«è·³è¿‡
        scaler.step(optimizer)

        # [5] æ›´æ–°scalerçš„scaleå› å­
        scaler.update()
        # å¦‚æœstepæˆåŠŸ: ä¸å˜ï¼ˆæˆ–growth_intervalåå¢é•¿ï¼‰
        # å¦‚æœæœ‰Inf/NaN: scale *= 0.5

        optimizer.zero_grad()
```

**è¯¦ç»†æ­¥éª¤è§£æ**ï¼š

```python
# å‡è®¾çœŸå®æ¢¯åº¦èŒƒå›´: [1e-8, 1e-3]
# scale = 65536

# Step 1: scale(loss).backward()
loss_fp16 = 0.0001  # fp16å¯ä»¥è¡¨ç¤º
loss_scaled = 0.0001 * 65536 = 6.5536  # æ”¾å¤§åè®¡ç®—
# åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
grad_scaled = [6.5e-4, 1.2e-3, ..., 65.5]  # éƒ½åœ¨fp16èŒƒå›´å†…

# Step 2: unscale_(optimizer)
grad_fp32 = grad_scaled / 65536
# [1e-8, 1.8e-8, ..., 1e-3]  # è½¬å›fp32ï¼Œè¿˜åŸçœŸå®å€¼

# Step 3: clip_grad_norm_
# åœ¨fp32ç²¾åº¦ä¸‹è£å‰ªï¼Œä¿è¯å‡†ç¡®æ€§

# Step 4: step(optimizer)
# æ£€æŸ¥grad_fp32ä¸­æ˜¯å¦æœ‰Inf/NaN
if not has_inf_or_nan(grad_fp32):
    optimizer.step()  # æ›´æ–°å‚æ•°ï¼ˆfp32ï¼‰
    scaler._scale = 65536  # ä¿æŒä¸å˜
else:
    # è·³è¿‡æ›´æ–°
    scaler._scale = 65536 * 0.5  # å‡å°scaleï¼Œä¸‹æ¬¡å°è¯•

# Step 5: update()
# å¦‚æœè¿ç»­2000æ­¥æˆåŠŸï¼Œscale *= 2ï¼ˆæ›´æ¿€è¿›ï¼‰
# åŠ¨æ€é€‚åº”æ¢¯åº¦èŒƒå›´
```

**ä¸ºä»€ä¹ˆéœ€è¦åŠ¨æ€ç¼©æ”¾**ï¼š

```python
# è®­ç»ƒåˆæœŸï¼šæ¢¯åº¦å¤§
çœŸå®æ¢¯åº¦: [1e-2, 1e-1, 1.0]
scale=65536: æº¢å‡ºï¼
â†’ scalerè‡ªåŠ¨é™ä½: scale=1024
â†’ æˆåŠŸ

# è®­ç»ƒåæœŸï¼šæ¢¯åº¦å°
çœŸå®æ¢¯åº¦: [1e-8, 1e-7, 1e-6]
scale=1024: ç¼©æ”¾åä»ç„¶å¤ªå°ï¼Œç²¾åº¦æŸå¤±
â†’ scalerè‡ªåŠ¨å¢å¤§: scale=65536
â†’ æ›´å¥½çš„ç²¾åº¦
```

**BFloat16ä¸éœ€è¦GradScaler**ï¼š

```python
# BFloat16çš„æ•°å€¼èŒƒå›´
æœ€å¤§å€¼: 3.4e38 (ä¸FP32ç›¸åŒ)
æœ€å°æ­£æ•°: 1.2e-38 (ä¸FP32ç›¸åŒ)
ç²¾åº¦: çº¦2ä½æœ‰æ•ˆæ•°å­—ï¼ˆä½†èŒƒå›´å¤§ï¼‰

# å› æ­¤
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == torch.float16))
# dtype=bfloat16æ—¶ï¼Œenabled=Falseï¼Œscalerå˜æˆç©ºæ“ä½œ
```

**æ€§èƒ½å½±å“**ï¼š

```python
# ä¸ç”¨GradScalerï¼ˆçº¯FP16ï¼‰
è®­ç»ƒé€Ÿåº¦: å¿«
ä½†: 40%æ¦‚ç‡æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤± â†’ è®­ç»ƒå¤±è´¥

# ç”¨GradScalerï¼ˆFP16 + ç¼©æ”¾ï¼‰
è®­ç»ƒé€Ÿåº¦: å‡ ä¹ç›¸åŒï¼ˆoverhead < 1%ï¼‰
ç¨³å®šæ€§: ä¸FP32ç›¸å½“
å†…å­˜: èŠ‚çœ50%

# ç”¨BFloat16
è®­ç»ƒé€Ÿåº¦: ä¸FP16ç›¸åŒ
ç¨³å®šæ€§: å¤©ç„¶å¥½ï¼Œä¸éœ€è¦scaler
å†…å­˜: èŠ‚çœ50%
```

**å¸¸è§é—®é¢˜æ’æŸ¥**ï¼š

```python
# é—®é¢˜1: Losså˜æˆNaN
â†’ æ£€æŸ¥scaler._scaleæ˜¯å¦ä¸€ç›´åœ¨ä¸‹é™
â†’ å¯èƒ½éœ€è¦ï¼šé™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ¢¯åº¦è£å‰ª

# é—®é¢˜2: è®­ç»ƒå¾ˆæ…¢
â†’ æ£€æŸ¥æ˜¯å¦é¢‘ç¹è§¦å‘scaleré™ä½scale
â†’ è¯´æ˜åˆå§‹scaleè®¾ç½®è¿‡å¤§

# é—®é¢˜3: ç²¾åº¦æŸå¤±
â†’ æ£€æŸ¥scaler._scaleæ˜¯å¦å¤ªå°
â†’ å¢å¤§init_scaleæˆ–growth_factor
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- ç†è§£FP16çš„æ•°å€¼èŒƒå›´é™åˆ¶
- çŸ¥é“BFloat16çš„ä¼˜åŠ¿
- èƒ½è§£é‡Šä¸ºä»€ä¹ˆè¦unscaleåå†clip
- çŸ¥é“scalerçš„åŠ¨æ€è°ƒæ•´ç­–ç•¥

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“scaleræ˜¯ä¸ºäº†é˜²æ­¢æº¢å‡º
- 80åˆ†ï¼šç†è§£å®Œæ•´çš„scaleâ†’unscaleâ†’stepæµç¨‹
- 100åˆ†ï¼šèƒ½è§£é‡ŠåŠ¨æ€ç¼©æ”¾æœºåˆ¶ï¼ŒçŸ¥é“BFloat16çš„åŒºåˆ«

---

### â­â­â­ Q11: å¦‚æœè®­ç»ƒè¿‡ç¨‹ä¸­Lossçªç„¶å˜æˆNaNï¼Œä½ ä¼šå¦‚ä½•æ’æŸ¥å’Œè§£å†³ï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šé—®é¢˜æ’æŸ¥èƒ½åŠ›ã€è®­ç»ƒç¨³å®šæ€§

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**é—®é¢˜è¯Šæ–­æµç¨‹**ï¼š

```python
# Step 1: ç¡®å®šNaNå‡ºç°çš„ä½ç½®
def check_nan(name, tensor):
    if torch.isnan(tensor).any():
        print(f"NaN detected in {name}")
        print(f"  Shape: {tensor.shape}")
        print(f"  Max: {tensor.max()}, Min: {tensor.min()}")
        return True
    return False

# åœ¨å…³é”®ä½ç½®æ’å…¥æ£€æŸ¥
def forward_with_check(self, x):
    if check_nan("input", x):
        raise ValueError("NaN in input")

    x = self.embedding(x)
    if check_nan("embedding", x):
        raise ValueError("NaN in embedding")

    for i, layer in enumerate(self.layers):
        x = layer(x)
        if check_nan(f"layer_{i}_output", x):
            raise ValueError(f"NaN in layer {i}")

    logits = self.lm_head(x)
    if check_nan("logits", logits):
        raise ValueError("NaN in logits")

    return logits
```

**å¸¸è§åŸå› ä¸è§£å†³æ–¹æ¡ˆ**ï¼š

**åŸå› 1ï¼šæ¢¯åº¦çˆ†ç‚¸** â­â­â­â­â­

```python
# ç—‡çŠ¶
step 100: loss=2.1, max_grad=3.5
step 101: loss=2.0, max_grad=12.8
step 102: loss=8.5, max_grad=234.6  â† çªç„¶å¢å¤§
step 103: loss=NaN, max_grad=NaN

# è¯Šæ–­
# åœ¨backwardåæ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm()
        if grad_norm > 100:  # å¼‚å¸¸å¤§
            print(f"{name}: grad_norm={grad_norm}")

# è§£å†³æ–¹æ¡ˆ
# 1. é™ä½å­¦ä¹ ç‡
lr = 5e-4 â†’ 1e-4

# 2. åŠ å¼ºæ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0 â†’ 0.5)

# 3. æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸æ•°æ®
# è¿‡æ»¤æ‰æé•¿çš„åºåˆ—æˆ–ç‰¹æ®Šå­—ç¬¦
```

**åŸå› 2ï¼šæ•°å€¼æº¢å‡ºï¼ˆFP16ï¼‰** â­â­â­â­

```python
# ç—‡çŠ¶
ä½¿ç”¨dtype=float16
æŸä¸€æ­¥lossçªç„¶ä»2.0 â†’ 65504 (FP16æœ€å¤§å€¼) â†’ NaN

# è¯Šæ–­
# æ£€æŸ¥scalerçš„scaleå€¼
print(f"scaler._scale: {scaler._scale}")
# å¦‚æœä¸€ç›´åœ¨ä¸‹é™: 65536 â†’ 32768 â†’ 16384 â†’ ...
# è¯´æ˜é¢‘ç¹æº¢å‡º

# è§£å†³æ–¹æ¡ˆ
# 1. æ¢æˆbfloat16
dtype = torch.bfloat16
scaler = torch.cuda.amp.GradScaler(enabled=False)

# 2. æˆ–é™ä½åˆå§‹scale
scaler = torch.cuda.amp.GradScaler(init_scale=2**10)  # è€Œé2**16

# 3. åœ¨å…³é”®å±‚ç”¨FP32
class Attention(nn.Module):
    def forward(self, x):
        with torch.cuda.amp.autocast(enabled=False):
            scores = (xq @ xk.T) / math.sqrt(dim)  # FP32è®¡ç®—
            scores = F.softmax(scores.float(), dim=-1)
        ...
```

**åŸå› 3ï¼šé™¤é›¶é”™è¯¯** â­â­â­

```python
# ç—‡çŠ¶
loss = (loss * loss_mask).sum() / loss_mask.sum()
# å¦‚æœloss_maskå…¨æ˜¯0 â†’ é™¤é›¶ â†’ NaN

# è¯Šæ–­
print(f"loss_mask sum: {loss_mask.sum()}")
if loss_mask.sum() == 0:
    print("All tokens are masked!")
    print(f"Sample: {tokenizer.decode(input_ids[0])}")

# è§£å†³æ–¹æ¡ˆ
# 1. æ•°æ®è¿‡æ»¤
def __getitem__(self, index):
    ...
    if loss_mask.sum() == 0:
        return None  # collate_fnä¸­è¿‡æ»¤

# 2. æ·»åŠ epsilon
loss = (loss * loss_mask).sum() / (loss_mask.sum() + 1e-8)

# 3. æ£€æŸ¥ç©ºæ ·æœ¬
assert len(sample["text"].strip()) > 0
```

**åŸå› 4ï¼šLayerNorm/RMSNormçš„epså¤ªå°** â­â­

```python
# ç—‡çŠ¶
RMSNormè®¡ç®—æ—¶æ–¹å·®æ¥è¿‘0

# è¯Šæ–­
class RMSNorm(nn.Module):
    def _norm(self, x):
        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        print(f"RMS min: {rms.min()}, max: {rms.max()}")  # æ£€æŸ¥
        return x * rms

# è§£å†³æ–¹æ¡ˆ
# å¢å¤§eps
self.eps = 1e-5 â†’ 1e-6  # å¦‚æœè¿˜ä¸å¤Ÿ
```

**åŸå› 5ï¼šå­¦ä¹ ç‡è¿‡å¤§** â­â­â­â­

```python
# ç—‡çŠ¶
è®­ç»ƒå¼€å§‹å‡ æ­¥å°±NaN

# è¯Šæ–­
# æ£€æŸ¥åˆå§‹å­¦ä¹ ç‡
print(f"Initial LR: {optimizer.param_groups[0]['lr']}")

# è§£å†³æ–¹æ¡ˆ
# 1. æ·»åŠ warmup
def get_lr(step, warmup_steps, total_steps, max_lr):
    if step < warmup_steps:
        return max_lr * step / warmup_steps  # çº¿æ€§å¢é•¿
    else:
        # ä½™å¼¦é€€ç«
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return 0.1 * max_lr + 0.5 * max_lr * (1 + math.cos(math.pi * progress))

# 2. é™ä½åŸºç¡€å­¦ä¹ ç‡
lr = 5e-4 â†’ 1e-4
```

**åŸå› 6ï¼šEmbeddingæƒé‡å¼‚å¸¸** â­â­

```python
# ç—‡çŠ¶
ç¬¬ä¸€ä¸ªEpochæ­£å¸¸ï¼Œç¬¬äºŒä¸ªEpoch NaN

# è¯Šæ–­
# æ£€æŸ¥Embeddingæƒé‡
embedding_weights = model.embed_tokens.weight
print(f"Embedding: min={embedding_weights.min()}, max={embedding_weights.max()}")
print(f"Embedding has NaN: {torch.isnan(embedding_weights).any()}")
print(f"Embedding has Inf: {torch.isinf(embedding_weights).any()}")

# è§£å†³æ–¹æ¡ˆ
# 1. æƒé‡åˆå§‹åŒ–
nn.init.normal_(self.embed_tokens.weight, mean=0.0, std=0.02)

# 2. å®šæœŸæ£€æŸ¥
if step % 100 == 0:
    if torch.isnan(model.embed_tokens.weight).any():
        raise ValueError("Embedding weights have NaN!")
```

**ç³»ç»ŸåŒ–çš„é˜²å¾¡ç­–ç•¥**ï¼š

```python
# 1. æ•°æ®éªŒè¯
class SafeDataset(Dataset):
    def __getitem__(self, index):
        sample = self.samples[index]

        # æ£€æŸ¥ç©ºæ–‡æœ¬
        if len(sample["text"].strip()) == 0:
            return None

        # æ£€æŸ¥å¼‚å¸¸å­—ç¬¦
        if contains_special_chars(sample["text"]):
            return None

        # æ£€æŸ¥é•¿åº¦
        if len(sample["text"]) > 10000:
            return None

        return sample

# 2. æ¨¡å‹æ£€æŸ¥
class SafeModel(nn.Module):
    def forward(self, x):
        x = self._forward(x)

        # æ£€æŸ¥è¾“å‡º
        if torch.isnan(x).any() or torch.isinf(x).any():
            torch.save({
                'input': x,
                'state_dict': self.state_dict()
            }, 'nan_debug.pth')
            raise ValueError("NaN/Inf detected!")

        return x

# 3. è®­ç»ƒç›‘æ§
class LossMonitor:
    def __init__(self, window=100):
        self.losses = []
        self.window = window

    def add(self, loss):
        self.losses.append(loss)

        if len(self.losses) > self.window:
            recent = self.losses[-self.window:]

            # æ£€æŸ¥å¼‚å¸¸æ³¢åŠ¨
            if max(recent) > 2 * np.median(recent):
                print("Warning: Loss spike detected!")

            # æ£€æŸ¥ä¸ä¸‹é™
            if np.mean(recent[-10:]) > np.mean(recent[-20:-10]):
                print("Warning: Loss not decreasing!")

# 4. è‡ªåŠ¨æ¢å¤
try:
    train_epoch(...)
except ValueError as e:
    if "NaN" in str(e):
        print("NaN detected, loading checkpoint...")
        model.load_state_dict(torch.load(last_checkpoint))
        optimizer.load_state_dict(checkpoint['optimizer'])
        # é™ä½å­¦ä¹ ç‡é‡è¯•
        for param_group in optimizer.param_groups:
            param_group['lr'] *= 0.5
        print("Retrying with lower LR...")
        train_epoch(...)
```

**å¿«é€Ÿè¯Šæ–­checklist**ï¼š

```
â–¡ æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§ï¼ˆ>1e-3é€šå¸¸å±é™©ï¼‰
â–¡ æ£€æŸ¥æ˜¯å¦æœ‰warmupï¼ˆå‰100-1000æ­¥ï¼‰
â–¡ æ£€æŸ¥æ¢¯åº¦è£å‰ªæ˜¯å¦å¯ç”¨ï¼ˆmax_norm=1.0ï¼‰
â–¡ æ£€æŸ¥æ˜¯å¦ä½¿ç”¨FP16ï¼ˆå¦‚æœæ˜¯ï¼Œæ¢BF16ï¼‰
â–¡ æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰ç©ºæ ·æœ¬
â–¡ æ£€æŸ¥loss_mask.sum()æ˜¯å¦ä¸º0
â–¡ æ£€æŸ¥Embeddingæ˜¯å¦åˆå§‹åŒ–
â–¡ æ£€æŸ¥æ˜¯å¦æœ‰æé•¿åºåˆ—ï¼ˆ>4096ï¼‰
â–¡ æ£€æŸ¥scaler._scaleæ˜¯å¦å¼‚å¸¸
â–¡ æ£€æŸ¥optimizerçŠ¶æ€æ˜¯å¦æ­£å¸¸
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- ç³»ç»ŸåŒ–çš„æ’æŸ¥æµç¨‹
- çŸ¥é“å¸¸è§çš„NaNæ¥æº
- èƒ½æä¾›é¢„é˜²æªæ–½
- æœ‰å®é™…debuggingç»éªŒ

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“2-3ç§å¯èƒ½åŸå› 
- 80åˆ†ï¼šèƒ½æä¾›ç³»ç»ŸåŒ–çš„æ’æŸ¥æµç¨‹
- 100åˆ†ï¼šç»™å‡ºå®Œæ•´çš„é˜²å¾¡ç­–ç•¥å’Œè‡ªåŠ¨æ¢å¤æœºåˆ¶

---

## ç¬¬å››éƒ¨åˆ†ï¼šæ¨ç†ä¸ç”Ÿæˆï¼ˆ6é¢˜ï¼‰

### â­ Q12: è§£é‡ŠKV Cacheçš„ä½œç”¨ï¼Œå¹¶è®¡ç®—å®ƒèŠ‚çœäº†å¤šå°‘è®¡ç®—é‡ã€‚

**è€ƒå¯Ÿç‚¹**ï¼šKV CacheåŸç†ã€è®¡ç®—å¤æ‚åº¦åˆ†æ

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹**ï¼ˆæ— Cacheï¼‰ï¼š

```python
# ç”Ÿæˆ"ä»Šå¤©å¤©æ°”å¾ˆå¥½"

Step 1: è¾“å…¥[ä»Šå¤©] â†’ è¾“å‡º"å¤©æ°”"
- è®¡ç®—: Attention([ä»Šå¤©], [ä»Šå¤©])
- æ“ä½œæ•°: 1ä¸ªtokençš„KV

Step 2: è¾“å…¥[ä»Šå¤©, å¤©æ°”] â†’ è¾“å‡º"å¾ˆ"
- è®¡ç®—: Attention([ä»Šå¤©,å¤©æ°”], [ä»Šå¤©,å¤©æ°”])
- æ“ä½œæ•°: 2ä¸ªtokençš„KV
- é—®é¢˜: "ä»Šå¤©"çš„KVé‡å¤è®¡ç®—äº†ï¼

Step 3: è¾“å…¥[ä»Šå¤©, å¤©æ°”, å¾ˆ] â†’ è¾“å‡º"å¥½"
- è®¡ç®—: Attention([ä»Šå¤©,å¤©æ°”,å¾ˆ], [ä»Šå¤©,å¤©æ°”,å¾ˆ])
- æ“ä½œæ•°: 3ä¸ªtokençš„KV
- é—®é¢˜: "ä»Šå¤©""å¤©æ°”"çš„KVåˆé‡å¤è®¡ç®—äº†ï¼

æ€»è®¡ç®—é‡: 1 + 2 + 3 + ... + n = n(n+1)/2 â‰ˆ O(nÂ²)
```

**è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹**ï¼ˆæœ‰Cacheï¼‰ï¼š

```python
Step 1: è¾“å…¥[ä»Šå¤©] â†’ è¾“å‡º"å¤©æ°”"
- è®¡ç®—: Attention([ä»Šå¤©], [ä»Šå¤©])
- ç¼“å­˜: K_cache=[ä»Šå¤©], V_cache=[ä»Šå¤©]

Step 2: è¾“å…¥[å¤©æ°”] â†’ è¾“å‡º"å¾ˆ"  # åªè¾“å…¥æ–°tokenï¼
- ä»ç¼“å­˜è¯»å–: K_cache=[ä»Šå¤©], V_cache=[ä»Šå¤©]
- è®¡ç®—æ–°token: K_new=[å¤©æ°”], V_new=[å¤©æ°”]
- æ‹¼æ¥: K=[ä»Šå¤©,å¤©æ°”], V=[ä»Šå¤©,å¤©æ°”]
- æ›´æ–°ç¼“å­˜: K_cache=[ä»Šå¤©,å¤©æ°”], V_cache=[ä»Šå¤©,å¤©æ°”]

Step 3: è¾“å…¥[å¾ˆ] â†’ è¾“å‡º"å¥½"
- ä»ç¼“å­˜è¯»å–: K_cache=[ä»Šå¤©,å¤©æ°”]
- è®¡ç®—æ–°token: K_new=[å¾ˆ]
- æ‹¼æ¥: K=[ä»Šå¤©,å¤©æ°”,å¾ˆ]
- æ›´æ–°ç¼“å­˜

æ€»è®¡ç®—é‡: 1 + 1 + 1 + ... + 1 = n â‰ˆ O(n)
```

**å…·ä½“è®¡ç®—**ï¼ˆMiniMindé…ç½®ï¼‰ï¼š

```python
# æ¨¡å‹é…ç½®
num_layers = 8
num_kv_heads = 2
head_dim = 64
seq_len = 512
generate_len = 100

# æ— Cache
for step in range(generate_len):
    current_len = seq_len + step
    # æ¯å±‚çš„è®¡ç®—é‡
    compute = current_len Ã— num_kv_heads Ã— head_dim Ã— 2  # Kå’ŒV
    total += compute

total_without_cache = Î£(512+i) for i=0 to 99
                    = 512Ã—100 + (0+99)Ã—100/2
                    = 51200 + 4950
                    = 56150 æ¬¡ KVè®¡ç®—

# æœ‰Cache
for step in range(generate_len):
    # æ¯æ­¥åªè®¡ç®—1ä¸ªæ–°token
    compute = 1 Ã— num_kv_heads Ã— head_dim Ã— 2
    total += compute

total_with_cache = 100 æ¬¡ KVè®¡ç®—

# åŠ é€Ÿæ¯”
speedup = 56150 / 100 = 561.5x
```

**å†…å­˜å ç”¨è®¡ç®—**ï¼š

```python
# å•ä¸ªtokençš„KVå¤§å°
kv_size_per_token = num_layers Ã— 2 Ã— num_kv_heads Ã— head_dim Ã— sizeof(float16)
                  = 8 Ã— 2 Ã— 2 Ã— 64 Ã— 2 bytes
                  = 4096 bytes = 4KB

# ç”Ÿæˆ100ä¸ªtokenå
kv_cache_size = (512 + 100) Ã— 4KB
              = 612 Ã— 4KB
              = 2.4MB

# batch_size=32æ—¶
total_cache = 2.4MB Ã— 32 = 76.8MB  # å¯æ¥å—
```

**ä»£ç å®ç°**ï¼ˆä½ç½®ï¼šmodel/model_minimind.py:346-356ï¼‰ï¼š

```python
def forward(self, x, past_key_value=None, use_cache=False):
    # è®¡ç®—å½“å‰tokençš„Kå’ŒV
    xk = self.k_proj(x)  # [batch, 1, hidden]
    xv = self.v_proj(x)

    # å¦‚æœæœ‰pastï¼Œæ‹¼æ¥å†å²
    if past_key_value is not None:
        past_k, past_v = past_key_value
        xk = torch.cat([past_k, xk], dim=1)  # [batch, seq+1, hidden]
        xv = torch.cat([past_v, xv], dim=1)

    # ç¼“å­˜å½“å‰çŠ¶æ€
    if use_cache:
        past_kv = (xk, xv)
    else:
        past_kv = None

    # è®¡ç®—Attention
    output = attention(xq, xk, xv)

    return output, past_kv
```

**å®é™…æ•ˆæœå¯¹æ¯”**ï¼ˆåœ¨A100ä¸Šæµ‹è¯•ï¼‰ï¼š

| ç”Ÿæˆé•¿åº¦ | æ— Cacheè€—æ—¶ | æœ‰Cacheè€—æ—¶ | åŠ é€Ÿæ¯” |
|---------|-----------|-----------|-------|
| 10 tokens | 120ms | 15ms | 8x |
| 50 tokens | 1200ms | 65ms | 18x |
| 100 tokens | 3500ms | 125ms | 28x |
| 500 tokens | 45s | 620ms | 73x |

**ä¸ºä»€ä¹ˆè®­ç»ƒæ—¶ä¸ç”¨KV Cache**ï¼š

```python
# è®­ç»ƒï¼šTeacher Forcing
è¾“å…¥: [ä»Šå¤©, å¤©æ°”, å¾ˆå¥½]  # å®Œæ•´åºåˆ—
è¾“å‡º: [å¤©æ°”, å¾ˆå¥½, EOS]

# æ‰€æœ‰ä½ç½®å¹¶è¡Œè®¡ç®—
pos_0: Attention([ä»Šå¤©], [ä»Šå¤©])
pos_1: Attention([ä»Šå¤©,å¤©æ°”], [ä»Šå¤©,å¤©æ°”])
pos_2: Attention([ä»Šå¤©,å¤©æ°”,å¾ˆå¥½], [ä»Šå¤©,å¤©æ°”,å¾ˆå¥½])

# æ¯ä¸ªä½ç½®çš„KVéƒ½ä¸åŒï¼Œæ— æ³•å¤ç”¨
# è€Œä¸”å¹¶è¡Œè®¡ç®—æ›´é«˜æ•ˆï¼ˆGPUåˆ©ç”¨ç‡é«˜ï¼‰
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- èƒ½è®¡ç®—å…·ä½“çš„åŠ é€Ÿæ¯”
- ç†è§£ä¸ºä»€ä¹ˆè®­ç»ƒæ—¶ä¸ç”¨
- çŸ¥é“KV Cacheçš„å†…å­˜ç“¶é¢ˆ
- æåˆ°PagedAttentionç­‰ä¼˜åŒ–

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šç†è§£KV Cacheé¿å…é‡å¤è®¡ç®—
- 80åˆ†ï¼šèƒ½è®¡ç®—åŠ é€Ÿæ¯”å’Œå†…å­˜å ç”¨
- 100åˆ†ï¼šç†è§£è®­ç»ƒæ¨ç†çš„åŒºåˆ«ï¼ŒçŸ¥é“ä¼˜åŒ–æ–¹æ³•

---

### â­â­ Q13: Temperatureå’ŒTop-Pä¸¤ä¸ªå‚æ•°å¦‚ä½•å½±å“ç”Ÿæˆè´¨é‡ï¼Ÿå¦‚æœè¦å†™ä»£ç ï¼Œåº”è¯¥ç”¨ä»€ä¹ˆï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šé‡‡æ ·ç­–ç•¥ç†è§£ã€åº”ç”¨åœºæ™¯

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**Temperatureçš„ä½œç”¨**ï¼š

```python
# åŸå§‹logits
logits = [8.0, 7.0, 6.0, 3.0, 2.0]  # 5ä¸ªå€™é€‰è¯

# Temperature = 1.0ï¼ˆæ ‡å‡†Softmaxï¼‰
probs = softmax(logits)
      = [0.62, 0.23, 0.08, 0.04, 0.01]

# Temperature = 0.1ï¼ˆæ›´ç¡®å®šï¼‰
probs = softmax(logits / 0.1)
      = softmax([80, 70, 60, 30, 20])
      = [1.00, 0.00, 0.00, 0.00, 0.00]
æ•ˆæœ: å‡ ä¹ç¡®å®šæ€§åœ°é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„è¯

# Temperature = 2.0ï¼ˆæ›´éšæœºï¼‰
probs = softmax(logits / 2.0)
      = softmax([4.0, 3.5, 3.0, 1.5, 1.0])
      = [0.37, 0.25, 0.18, 0.11, 0.08]
æ•ˆæœ: åˆ†å¸ƒæ›´å¹³å¦ï¼Œæ¢ç´¢æ›´å¤šå¯èƒ½æ€§
```

**Top-Pï¼ˆNucleus Samplingï¼‰çš„ä½œç”¨**ï¼š

```python
# åŸå§‹æ¦‚ç‡
probs = [0.5, 0.25, 0.15, 0.05, 0.03, 0.02]

# Top-P = 0.9
cumsum = [0.5, 0.75, 0.90, 0.95, 0.98, 1.00]
# ç´¯ç§¯åˆ°0.9ï¼Œé€‰å‰3ä¸ª
å€™é€‰è¯ = [0, 1, 2]
é‡æ–°å½’ä¸€åŒ– = [0.5/0.9, 0.25/0.9, 0.15/0.9]
           = [0.56, 0.28, 0.17]

# Top-P = 0.5
cumsum = [0.5, ...]
# ç´¯ç§¯åˆ°0.5ï¼Œåªé€‰ç¬¬1ä¸ª
å€™é€‰è¯ = [0]
æ¦‚ç‡ = [1.0]  # ç­‰ä»·äºGreedy
```

**ä¸¤è€…çš„åŒºåˆ«**ï¼š

| ç‰¹æ€§ | Temperature | Top-P |
|-----|------------|-------|
| æ§åˆ¶å¯¹è±¡ | åˆ†å¸ƒçš„"å°–é”åº¦" | å€™é€‰é›†å¤§å° |
| å€™é€‰è¯æ•° | å›ºå®šï¼ˆæ‰€æœ‰è¯ï¼‰ | åŠ¨æ€å˜åŒ– |
| é«˜ç½®ä¿¡åº¦æ—¶ | ä»è€ƒè™‘æ‰€æœ‰è¯ | åªè€ƒè™‘å°‘æ•°è¯ |
| ä½ç½®ä¿¡åº¦æ—¶ | ä»è€ƒè™‘æ‰€æœ‰è¯ | è€ƒè™‘æ›´å¤šè¯ |
| é€‚ç”¨åœºæ™¯ | æ§åˆ¶åˆ›é€ æ€§ | è‡ªé€‚åº”å†³ç­– |

**ç»„åˆä½¿ç”¨**ï¼š

```python
def sample_with_temp_and_topp(logits, temperature=1.0, top_p=0.9):
    # Step 1: Apply temperature
    logits = logits / temperature

    # Step 2: Softmax
    probs = F.softmax(logits, dim=-1)

    # Step 3: Top-P filtering
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumsum_probs = torch.cumsum(sorted_probs, dim=-1)

    # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
    sorted_indices_to_remove = cumsum_probs > top_p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0  # è‡³å°‘ä¿ç•™1ä¸ª

    # è¿‡æ»¤
    indices_to_remove = sorted_indices[sorted_indices_to_remove]
    probs[indices_to_remove] = 0

    # Step 4: é‡æ–°å½’ä¸€åŒ–
    probs = probs / probs.sum()

    # Step 5: é‡‡æ ·
    next_token = torch.multinomial(probs, num_samples=1)

    return next_token
```

**å®é™…æ¡ˆä¾‹å¯¹æ¯”**ï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”"

# Case 1: temperature=0.1, top_p=1.0
è¾“å‡º: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚" (å•è°ƒï¼Œé‡å¤)

# Case 2: temperature=2.0, top_p=1.0
è¾“å‡º: "ä»Šå¤©å¤©æ°”çœŸæ£’æäº†ç®€ç›´å¥½åˆ°çˆ†ç‚¸..." (è¿‡äºéšæœºï¼Œä¸è¿è´¯)

# Case 3: temperature=0.8, top_p=0.9 (æ¨è)
è¾“å‡º: "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºé—¨æ¸¸ç©ã€‚" (è‡ªç„¶æµç•…)

# Case 4: temperature=0.3, top_p=0.95 (ä»£ç ç”Ÿæˆ)
prompt = "def fibonacci("
è¾“å‡º: "def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)" (å‡†ç¡®)
```

**ä¸åŒä»»åŠ¡çš„æœ€ä½³å®è·µ**ï¼š

```python
# ä»£ç ç”Ÿæˆ
temperature = 0.2
top_p = 0.95
# åŸå› : éœ€è¦è¯­æ³•æ­£ç¡®ï¼Œå®¹é”™ç‡ä½

# é—®ç­”
temperature = 0.5
top_p = 0.9
# åŸå› : éœ€è¦å‡†ç¡®ï¼Œä½†å…è®¸ä¸€å®šçµæ´»æ€§

# åˆ›æ„å†™ä½œ
temperature = 0.9
top_p = 0.85
# åŸå› : é¼“åŠ±å¤šæ ·æ€§å’Œåˆ›é€ æ€§

# ç¿»è¯‘
temperature = 0.3
top_p = 0.95
# åŸå› : éœ€è¦å‡†ç¡®ï¼Œä½†å…è®¸ä¸åŒçš„è¡¨è¾¾æ–¹å¼

# é—²èŠ
temperature = 0.8
top_p = 0.9
# åŸå› : è‡ªç„¶éšæ„ï¼Œé¿å…é‡å¤
```

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤è€…é…åˆ**ï¼š

```python
# åœºæ™¯ï¼šæ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªè¯éå¸¸ç¡®å®š
logits = [15.0, 3.0, 2.8, 2.5, ...]
probs = [0.95, 0.02, 0.015, 0.01, ...]

# åªç”¨temperature=2.0:
new_probs = [0.65, 0.12, 0.10, 0.08, ...]
# é—®é¢˜: ä»ç„¶è€ƒè™‘å¾ˆå¤šä½è´¨é‡çš„å€™é€‰è¯

# temperature=2.0 + top_p=0.9:
# å…ˆtemperature: [0.65, 0.12, 0.10, 0.08, ...]
# å†top_p: ç´¯ç§¯åˆ°0.9ï¼Œåªä¿ç•™[0.65, 0.12, 0.10]
# å½’ä¸€åŒ–: [0.75, 0.14, 0.11]
# ç»“æœ: æ¢ç´¢æ€§å¼ºï¼Œä½†é™åˆ¶åœ¨åˆç†èŒƒå›´å†…

# åœºæ™¯ï¼šæ¨¡å‹ä¸ç¡®å®š
logits = [5.0, 4.8, 4.5, 4.3, ...]
probs = [0.28, 0.24, 0.19, 0.16, ...]

# åªç”¨top_p=0.5:
# ç´¯ç§¯åˆ°0.5ï¼Œåªé€‰ç¬¬ä¸€ä¸ª[0.28] â†’ å½’ä¸€åŒ– â†’ [1.0]
# é—®é¢˜: è¿‡äºä¿å®ˆï¼Œä¸¢å¤±äº†æœ‰ä»·å€¼çš„é€‰é¡¹

# temperature=0.8 + top_p=0.9:
# å…ˆtemperature: åˆ†å¸ƒæ›´å¹³å¦
# å†top_p: ä¿ç•™æ›´å¤šå€™é€‰è¯
# ç»“æœ: å……åˆ†æ¢ç´¢ï¼Œç¬¦åˆä¸ç¡®å®šæ€§
```

**å¸¸è§é”™è¯¯**ï¼š

```python
# é”™è¯¯1: temperature=0
# é—®é¢˜: ç­‰ä»·äºGreedyï¼Œå¤±å»éšæœºæ€§
# æ­£ç¡®: ç”¨å¾ˆå°çš„å€¼å¦‚0.1

# é”™è¯¯2: top_p=1.0
# é—®é¢˜: ä¸èµ·ä½œç”¨ï¼Œç­‰äºä¸è¿‡æ»¤
# æ­£ç¡®: 0.85-0.95

# é”™è¯¯3: temperatureå’Œtop_kæ··ç”¨
top_k = 50  # å›ºå®šé€‰å‰50ä¸ª
top_p = 0.9  # åŠ¨æ€é€‰æ‹©
# é—®é¢˜: top_kä¸è‡ªé€‚åº”ï¼Œä¸å¦‚top_p

# æ­£ç¡®: temperature + top_p
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- èƒ½è§£é‡Šä¿¡æ¯ç†µçš„å˜åŒ–
- çŸ¥é“ä¸åŒä»»åŠ¡çš„æ¨èå€¼
- ç†è§£ä¸¤è€…çš„äº’è¡¥æ€§
- æåˆ°repetition_penaltyç­‰å…¶ä»–é‡‡æ ·æŠ€å·§

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šçŸ¥é“temperatureæ§åˆ¶éšæœºæ€§ï¼Œtop_pè¿‡æ»¤å€™é€‰è¯
- 80åˆ†ï¼šèƒ½è§£é‡Šä¸¤è€…çš„åŒºåˆ«å’Œç»„åˆæ•ˆæœ
- 100åˆ†ï¼šç»™å‡ºä¸åŒä»»åŠ¡çš„æ¨èé…ç½®ï¼Œç†è§£èƒŒååŸç†

---

## ç¬¬äº”éƒ¨åˆ†ï¼šç³»ç»Ÿè®¾è®¡ï¼ˆ4é¢˜ï¼‰

### â­â­â­ Q14: è®¾è®¡ä¸€ä¸ªåœ¨çº¿æ¨ç†æœåŠ¡ï¼Œè¦æ±‚æ”¯æŒ100 QPSï¼Œå¹³å‡å»¶è¿Ÿ<500msã€‚ä½ ä¼šå¦‚ä½•æ¶æ„ï¼Ÿ

**è€ƒå¯Ÿç‚¹**ï¼šç³»ç»Ÿè®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–ã€å·¥ç¨‹èƒ½åŠ›

<details>
<summary>æ ‡å‡†ç­”æ¡ˆ</summary>

**éœ€æ±‚åˆ†æ**ï¼š

```python
# çº¦æŸæ¡ä»¶
QPS = 100 req/s
Latency < 500ms (P95)
æ¨¡å‹: MiniMind 26M
å¹³å‡ç”Ÿæˆé•¿åº¦: 50 tokens

# å®¹é‡è§„åˆ’
å•ä¸ªè¯·æ±‚è€—æ—¶ â‰ˆ 300ms (ç”Ÿæˆ50 tokens)
æ‰€éœ€å¹¶å‘æ•° = QPS Ã— Latency = 100 Ã— 0.3 = 30

# ç¡¬ä»¶éœ€æ±‚
å•å¡ååé‡ (A100):
- batch_size=1: 3.3 req/s
- batch_size=8: 20 req/s
- batch_size=32: 50 req/s

æ‰€éœ€GPUæ•° = 100 / 50 = 2å¼ A100
```

**æ¶æ„è®¾è®¡**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Load Balancer              â”‚
â”‚         (Nginx/HAProxy)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
      â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  API Node â”‚ â”‚ API Node  â”‚
â”‚  (FastAPI)â”‚ â”‚ (FastAPI) â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Request Queue  â”‚
    â”‚     (Redis)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
      â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚GPU Worker â”‚ â”‚GPU Worker â”‚
â”‚  (vLLM)   â”‚ â”‚  (vLLM)   â”‚
â”‚  A100 #1  â”‚ â”‚  A100 #2  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒç»„ä»¶å®ç°**ï¼š

**1. APIå±‚ï¼ˆFastAPIï¼‰**ï¼š

```python
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
import asyncio
import uuid

app = FastAPI()

class GenerateRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 100
    temperature: float = 0.8
    top_p: float = 0.9

class GenerateResponse(BaseModel):
    request_id: str
    text: str
    tokens: int
    latency_ms: float

# è¯·æ±‚é˜Ÿåˆ—
request_queue = asyncio.Queue(maxsize=1000)

@app.post("/generate")
async def generate(req: GenerateRequest):
    request_id = str(uuid.uuid4())
    start_time = time.time()

    # 1. éªŒè¯è¾“å…¥
    if len(req.prompt) > 2048:
        raise HTTPException(400, "Prompt too long")

    # 2. åŠ å…¥é˜Ÿåˆ—
    try:
        await asyncio.wait_for(
            request_queue.put({
                'id': request_id,
                'prompt': req.prompt,
                'params': req.dict(),
                'start_time': start_time
            }),
            timeout=1.0  # é˜Ÿåˆ—æ»¡æ—¶è¶…æ—¶
        )
    except asyncio.TimeoutError:
        raise HTTPException(503, "Server overloaded")

    # 3. ç­‰å¾…ç»“æœ
    result = await wait_for_result(request_id)

    return GenerateResponse(
        request_id=request_id,
        text=result['text'],
        tokens=result['tokens'],
        latency_ms=(time.time() - start_time) * 1000
    )

# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "queue_size": request_queue.qsize(),
        "gpu_memory": get_gpu_memory()
    }
```

**2. GPU Workerï¼ˆvLLMå¼•æ“ï¼‰**ï¼š

```python
from vllm import LLM, SamplingParams
import asyncio

class GPUWorker:
    def __init__(self, model_path, gpu_id):
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.9,
            max_num_batched_tokens=4096,
            max_num_seqs=32,  # æœ€å¤§batch_size
        )
        self.gpu_id = gpu_id
        self.batch_queue = []
        self.batch_timeout = 0.01  # 10mså†…å‡‘batch

    async def process_loop(self):
        while True:
            # 1. æ”¶é›†è¯·æ±‚å‡‘batch
            batch = await self.collect_batch()

            if not batch:
                await asyncio.sleep(0.001)
                continue

            # 2. æ‰¹é‡æ¨ç†
            results = self.batch_generate(batch)

            # 3. è¿”å›ç»“æœ
            for req, result in zip(batch, results):
                await result_queue.put({
                    'request_id': req['id'],
                    'text': result.outputs[0].text,
                    'tokens': len(result.outputs[0].token_ids)
                })

    async def collect_batch(self, max_wait=0.01):
        batch = []
        deadline = time.time() + max_wait

        while len(batch) < 32 and time.time() < deadline:
            try:
                req = await asyncio.wait_for(
                    request_queue.get(),
                    timeout=deadline - time.time()
                )
                batch.append(req)
            except asyncio.TimeoutError:
                break

        return batch

    def batch_generate(self, batch):
        prompts = [req['prompt'] for req in batch]
        params = [SamplingParams(
            temperature=req['params']['temperature'],
            top_p=req['params']['top_p'],
            max_tokens=req['params']['max_new_tokens']
        ) for req in batch]

        # vLLMè‡ªåŠ¨åšcontinuous batching
        outputs = self.llm.generate(prompts, params)
        return outputs
```

**3. æ€§èƒ½ä¼˜åŒ–**ï¼š

```python
# ä¼˜åŒ–1: Continuous Batching (vLLMè‡ªåŠ¨)
# ä¸åŒè¯·æ±‚ç”Ÿæˆé•¿åº¦ä¸åŒæ—¶ï¼ŒåŠ¨æ€è°ƒæ•´batch

# ä¼˜åŒ–2: PagedAttention (vLLMè‡ªåŠ¨)
# KV Cacheåˆ†é¡µç®¡ç†ï¼Œå‡å°‘å†…å­˜æµªè´¹

# ä¼˜åŒ–3: è¯·æ±‚ä¼˜å…ˆçº§
class PriorityQueue:
    def __init__(self):
        self.high_priority = asyncio.Queue()
        self.normal_priority = asyncio.Queue()

    async def get(self):
        # ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§è¯·æ±‚
        if not self.high_priority.empty():
            return await self.high_priority.get()
        return await self.normal_priority.get()

# ä¼˜åŒ–4: é¢„å¡«å……ï¼ˆPrefillï¼‰ä¼˜åŒ–
# é•¿promptçš„prefillå’ŒçŸ­promptåˆ†å¼€batch
def split_by_length(batch):
    short = [r for r in batch if len(r['prompt']) < 100]
    long = [r for r in batch if len(r['prompt']) >= 100]
    return short, long

# ä¼˜åŒ–5: Speculative Decoding
# ç”¨å°æ¨¡å‹åŠ é€Ÿï¼ˆè§å‰é¢Q8ï¼‰
```

**4. ç›‘æ§å’Œé™çº§**ï¼š

```python
import prometheus_client as prom

# æŒ‡æ ‡
request_latency = prom.Histogram('request_latency_ms', 'Request latency')
request_qps = prom.Counter('request_total', 'Total requests')
queue_depth = prom.Gauge('queue_depth', 'Queue depth')
gpu_utilization = prom.Gauge('gpu_utilization', 'GPU util')

# é™çº§ç­–ç•¥
class CircuitBreaker:
    def __init__(self, threshold=0.1):
        self.failure_rate = 0
        self.threshold = threshold
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

    async def call(self, func, *args):
        if self.state == "OPEN":
            # ç†”æ–­çŠ¶æ€ï¼Œç›´æ¥è¿”å›é™çº§å“åº”
            return fallback_response()

        try:
            result = await func(*args)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            if self.state == "OPEN":
                return fallback_response()
            raise

def fallback_response():
    # é™çº§ç­–ç•¥ï¼šè¿”å›ç¼“å­˜æˆ–ç®€åŒ–å“åº”
    return {
        "text": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
        "degraded": True
    }

# è‡ªåŠ¨æ‰©ç¼©å®¹
class AutoScaler:
    def __init__(self):
        self.min_workers = 2
        self.max_workers = 10
        self.current_workers = 2

    async def monitor(self):
        while True:
            avg_latency = get_avg_latency()
            queue_size = request_queue.qsize()

            # æ‰©å®¹æ¡ä»¶
            if (avg_latency > 400 or queue_size > 100) and \
               self.current_workers < self.max_workers:
                await self.scale_up()

            # ç¼©å®¹æ¡ä»¶
            elif avg_latency < 200 and queue_size < 20 and \
                 self.current_workers > self.min_workers:
                await self.scale_down()

            await asyncio.sleep(10)
```

**5. éƒ¨ç½²é…ç½®**ï¼š

```yaml
# docker-compose.yml
version: '3'
services:
  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf

  api-node-1:
    image: minimind-api:latest
    environment:
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis

  api-node-2:
    image: minimind-api:latest
    environment:
      - REDIS_URL=redis://redis:6379

  gpu-worker-1:
    image: minimind-worker:latest
    runtime: nvidia
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PATH=/models/minimind-26M
    volumes:
      - ./models:/models

  gpu-worker-2:
    image: minimind-worker:latest
    runtime: nvidia
    environment:
      - CUDA_VISIBLE_DEVICES=1

  redis:
    image: redis:latest

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
```

**æ€§èƒ½æµ‹è¯•**ï¼š

```python
# å‹æµ‹è„šæœ¬
import aiohttp
import asyncio
import time

async def benchmark(qps=100, duration=60):
    async with aiohttp.ClientSession() as session:
        start = time.time()
        requests_sent = 0
        latencies = []

        while time.time() - start < duration:
            # æ§åˆ¶QPS
            await asyncio.sleep(1.0 / qps)

            # å‘é€è¯·æ±‚
            req_start = time.time()
            async with session.post('http://localhost/generate', json={
                'prompt': 'Hello, how are you?',
                'max_new_tokens': 50
            }) as resp:
                await resp.json()
                latency = (time.time() - req_start) * 1000
                latencies.append(latency)
                requests_sent += 1

        # ç»Ÿè®¡
        print(f"Total requests: {requests_sent}")
        print(f"Avg latency: {np.mean(latencies):.2f}ms")
        print(f"P95 latency: {np.percentile(latencies, 95):.2f}ms")
        print(f"P99 latency: {np.percentile(latencies, 99):.2f}ms")
        print(f"Actual QPS: {requests_sent / duration:.2f}")

# è¿è¡Œ
asyncio.run(benchmark(qps=100, duration=60))
```

**é¢„æœŸç»“æœ**ï¼š

```
Total requests: 6000
Avg latency: 320ms
P95 latency: 480ms âœ“
P99 latency: 650ms
Actual QPS: 100 âœ“
GPU Utilization: 85%
Memory Usage: 18GB / 80GB
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- å®Œæ•´çš„æ¶æ„è®¾è®¡
- è€ƒè™‘äº†é™çº§å’Œç›‘æ§
- çŸ¥é“vLLMç­‰æ¨ç†å¼•æ“
- èƒ½è®¡ç®—å®¹é‡å’Œæˆæœ¬
- æåˆ°å®é™…éƒ¨ç½²ä¸­çš„å‘

</details>

**è¯„åˆ†æ ‡å‡†**ï¼š
- 60åˆ†ï¼šèƒ½è®¾è®¡åŸºæœ¬æ¶æ„ï¼ˆAPI + Workerï¼‰
- 80åˆ†ï¼šè€ƒè™‘batchã€é˜Ÿåˆ—ã€ç›‘æ§
- 100åˆ†ï¼šå®Œæ•´æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é™çº§ã€æ‰©ç¼©å®¹ã€æ€§èƒ½è°ƒä¼˜

---

## é™„å½•ï¼šå¿«é€Ÿå¤ä¹ Checklist

### æ•°æ®å¤„ç†
- [ ] ç†è§£Xã€Yé”™ä½çš„åŸå› 
- [ ] çŸ¥é“loss_maskçš„ä½œç”¨
- [ ] äº†è§£è¯æ±‡è¡¨å¤§å°çš„æƒè¡¡

### æ¨¡å‹æ¶æ„
- [ ] èƒ½ç”»å‡ºå®Œæ•´çš„å‰å‘ä¼ æ’­å›¾
- [ ] ç†è§£GQAå’Œrepeat_kv
- [ ] æŒæ¡RoPEçš„å¤–æ¨åŸç†
- [ ] çŸ¥é“ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥

### è®­ç»ƒæŠ€å·§
- [ ] ç†è§£æ¢¯åº¦ç´¯ç§¯å’Œé™¤ä»¥accumulation_steps
- [ ] æŒæ¡GradScalerçš„å·¥ä½œæµç¨‹
- [ ] èƒ½æ’æŸ¥NaNé—®é¢˜
- [ ] çŸ¥é“ä½™å¼¦é€€ç«çš„ä¼˜åŠ¿

### æ¨ç†ç”Ÿæˆ
- [ ] ç†è§£KV Cacheçš„åŠ é€ŸåŸç†
- [ ] æŒæ¡Temperatureå’ŒTop-P
- [ ] çŸ¥é“æ¨ç†ä¼˜åŒ–çš„å„ç§æ–¹æ³•

### ç³»ç»Ÿè®¾è®¡
- [ ] èƒ½è®¾è®¡é«˜å¹¶å‘æ¨ç†æœåŠ¡
- [ ] äº†è§£vLLMç­‰æ¨ç†å¼•æ“
- [ ] çŸ¥é“ç›‘æ§å’Œé™çº§ç­–ç•¥

---

**é¢è¯•å»ºè®®**ï¼š

1. **å‡†å¤‡ç­–ç•¥**ï¼š
   - åŸºç¡€é¢˜å¿…é¡»100%æŒæ¡
   - è¿›é˜¶é¢˜ç†è§£70%ä»¥ä¸Š
   - ä¸“å®¶é¢˜äº†è§£æ€è·¯å³å¯

2. **ç­”é¢˜æŠ€å·§**ï¼š
   - å…ˆç»™ç»“è®ºï¼Œå†è§£é‡ŠåŸå› 
   - ç”¨å…·ä½“æ•°å€¼å’Œä»£ç ç¤ºä¾‹
   - ä¸»åŠ¨æåˆ°trade-offå’Œæ›¿ä»£æ–¹æ¡ˆ

3. **åŠ åˆ†é¡¹**ï¼š
   - å¼•ç”¨è®ºæ–‡å’Œå®é™…é¡¹ç›®ç»éªŒ
   - æåˆ°æœ€æ–°çš„æŠ€æœ¯ï¼ˆFlash Attention, vLLMç­‰ï¼‰
   - èƒ½ç°åœºå†™ä»£ç éªŒè¯

4. **é¿å…çš„å‘**ï¼š
   - ä¸è¦æ­»è®°ç¡¬èƒŒ
   - ä¸è¦è¿‡åº¦è‡ªä¿¡ï¼ˆä¸çŸ¥é“å°±è¯´ä¸çŸ¥é“ï¼‰
   - ä¸è¦åªè®²ç†è®ºï¼Œè¦ç»“åˆå®é™…

ç¥é¢è¯•é¡ºåˆ©ï¼ğŸš€
