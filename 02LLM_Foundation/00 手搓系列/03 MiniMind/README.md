# MiniMind

å®˜æ–¹é¡¹ç›®ï¼šhttps://github.com/jingyaogong/minimind

å…¶ä»–å‚è€ƒï¼š

- [Onlyä¸‰å°æ—¶ï¼Pytorchä»é›¶æ‰‹æ•²å¤§æ¨¡å‹ï¼Œæ¶æ„åˆ°è®­ç»ƒå…¨æ•™ç¨‹](https://www.bilibili.com/video/BV1T2k6BaEeC)
- https://github.com/MLNLP-World/minimind-notes



## ä¸€ã€æ¶æ„å›¾

### 1ã€Dense Model

![image-20251205090732564](./assets/image-20251205090732564.png)



![image-20251205091017096](./assets/image-20251205091017096.png)

### 2ã€MoE Model

![image-20251205090825495](./assets/image-20251205090825495.png)



## äºŒã€åˆå§‹åŒ–é¡¹ç›®

1ã€å‡†å¤‡Pythonç¯å¢ƒï¼š`uv init` åˆå§‹åŒ–é¡¹ç›®ï¼›æ›´æ–°`pyproject.toml`ä¸­çš„`dependencies`ï¼›åŸºäº`uv sync`ä¸€é”®å®‰è£…

```python
dependencies = [
    "numpy>=2.3.4",
    "pandas>=2.3.3",
    "torch>=2.9.0",
    "transformers>=4.57.1",
]
```

2ã€åˆ›å»ºdatasetã€modelã€traineråˆå§‹æ–‡ä»¶å¤¹åŠç©ºè„šæœ¬

```python
$ tree
.
â”œâ”€â”€ dataset
â”‚   â””â”€â”€ lm_dataset.py
â”œâ”€â”€ main.py
â”œâ”€â”€ model
â”‚   â””â”€â”€ model_minimind.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ trainer
â”‚   â”œâ”€â”€ trainer_utils.py
â”‚   â””â”€â”€ train_pretrain.py
â””â”€â”€ uv.lock

4 directories, 8 files
```

3ã€æ’°å†™model_minimind.pyï¼šç›´æ¥å¤åˆ¶å³å¯

```python
from transformers import PretrainedConfig


class MokioMindConfig(PretrainedConfig):
    model_type = "mokiomind"

    def __init__(
        self,
        dropout: float = 0.0,
        bos_token_id: int = 1,
        eos_token_id: int = 2,
        hidden_act: str = "silu",
        hidden_size: int = 512,
        intermediate_size: int = None,
        max_position_embeddings: int = 32768,
        num_attention_heads: int = 8,
        num_hidden_layers: int = 8,
        num_key_value_heads: int = 2,
        vocab_size: int = 6400,
        rms_norm_eps: float = 1e-05,
        rope_theta: int = 1000000,
        inference_rope_scaling: bool = False,
        flash_attention: bool = True,
        
        ############ MoE ############
        use_moe:bool=False,
        num_experts_per_tok:int=2,
        n_routed_experts:int=4,
        n_shared_experts:int=1,
        scoring_func:str='softmax',
        aux_loss_alpha:float=0.1,
        seq_aux:bool=True,
        norm_topk_prob:bool=True,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        self.flash_attention = flash_attention
        self.use_moe=use_moe
        self.num_experts_per_tok=num_experts_per_tok
        self.n_routed_experts=n_routed_experts
        self.n_shared_experts=n_shared_experts
        self.seq_aux=seq_aux
        self.norm_topk_prob=norm_topk_prob
        self.aux_loss_alpha=aux_loss_alpha
        self.scoring_func=scoring_func

        self.rope_scaling = (
            {
                "beta_fast": 4,
                "beta_slow": 1,
                "factor": 4,
                "original_max_position_embeddings": 2048,
                "type": "yarn",
            }
            if self.inference_rope_scaling
            else None
        )
```



## ä¸‰ã€RMSNorm

### 1ã€ç†è®º

>RMSNorm æ˜¯ä¸€ä¸ªè½»é‡ã€é«˜æ•ˆã€ç¨³å®šçš„å½’ä¸€åŒ–æ–¹æ³•ï¼šå®ƒä¸ç®¡å¹³å‡å€¼ï¼Œåªé€šè¿‡â€œæ•´ä½“å¤§å°â€çš„å‡æ–¹æ ¹è°ƒæ•´æ•°å€¼å¹…åº¦ã€‚

![image-20251205093005270](./assets/image-20251205093005270.png)

RMSNormï¼ˆRoot Mean Square Layer Normalizationï¼‰æ˜¯ä¸€ç§æ¯” LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰æ›´ç®€å•ã€æ›´é«˜æ•ˆçš„å½’ä¸€åŒ–æ–¹æ³•ã€‚
 **å®ƒä¸å‡å‡å€¼ï¼Œåªé™¤ä»¥â€œå‡æ–¹æ ¹ï¼ˆRMSï¼‰â€ã€‚**

å…¬å¼å¦‚ä¸‹ï¼š
$$
\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot g
$$
å…¶ä¸­ï¼š
$$
\text{RMS}(x)=\sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2 + \epsilon}
$$
æŠ½è±¡è§£é‡Šï¼š

```python
1ã€å‡è®¾æœ‰ä¸€ç¾¤å­¦ç”Ÿçš„â€œæˆç»©â€ç»„æˆä¸€ä¸ªå‘é‡
x = [å°æ˜çš„æˆç»©ï¼Œå°çº¢çš„æˆç»©ï¼Œå°åˆšçš„æˆç»©ï¼Œ...]
æˆ‘ä»¬æƒ³è®©æ•´ä¸ªâ€œç­çº§è¡¨ç°â€æ›´ç¨³å®šï¼Œä¸ä¼šæœ‰ç‰¹åˆ«å¤§çš„æ•°å½±å“è®¡ç®—ï¼Œè¿™æ—¶å€™è¦åšå½’ä¸€åŒ–ï¼ˆNormalizationï¼‰ã€‚

2ã€LayerNorm çš„åšæ³•ï¼ˆä¼ ç»Ÿï¼‰
LayerNorm ä¼šï¼š
1ï¼‰æ±‚å¹³å‡æˆç»©
2ï¼‰æ¯ä¸ªäººçš„æˆç»©éƒ½å‡å»å¹³å‡å€¼
3ï¼‰å†é™¤ä»¥æ ‡å‡†å·®
è¿™ç›¸å½“äºï¼šæŠŠå¹³å‡æ°´å¹³æŠ¹æ‰ï¼Œè®©å¤§å®¶æ›´â€œå…¬å¹³â€

3ã€RMSNorm çš„åšæ³•ï¼ˆæ›´ç®€å•ï¼‰
RMSNorm ä¸å‡å¹³å‡åˆ†ï¼å®ƒåšçš„æ›´åƒæ˜¯ï¼šâ€œçœ‹çœ‹å¤§å®¶æ•´ä½“æˆç»©æœ‰å¤šå¤§ï¼Œç„¶åéƒ½æŒ‰æ¯”ä¾‹ç¼©å°ä¸€ä¸‹â€ã€‚

æ•°å­¦ä¸Šå°±æ˜¯æŠŠæ¯ä¸ªæ•°éƒ½é™¤ä»¥æ•´ä¸ªå‘é‡çš„â€œå‡æ–¹æ ¹â€ï¼š
1ï¼‰RMS è¶Šå¤§ â†’ è¡¨ç¤ºæ•´ä½“åé«˜ â†’ è‡ªåŠ¨ç¼©å°
2ï¼‰RMS è¶Šå° â†’ è¡¨ç¤ºæ•´ä½“åä½ â†’ è‡ªåŠ¨æ”¾å¤§
å°±åƒï¼šå¦‚æœå…¨ç­æˆç»©éƒ½åé«˜ï¼Œå°±æ•´ä½“å‹ç¼©ä¸€ä¸‹ï¼›å¦‚æœæˆç»©éƒ½åä½ï¼Œå°±æ•´ä½“æ‹‰é«˜ä¸€ä¸‹ã€‚

è¿™æ ·åšæ›´å¿«ã€æ›´ç¨³å®šï¼Œä¹Ÿæ›´é€‚åˆå¤§æ¨¡å‹è®­ç»ƒã€‚
```

ä¸ºä»€ä¹ˆä¸å‡å‡å€¼ä¼šæ›´å¥½ï¼Ÿ

- åœ¨è‡ªæ³¨æ„åŠ› Transformer è¾“å…¥é‡Œï¼Œå‘é‡çš„å‡å€¼ä¸ä¸€å®šæä¾›æœ‰æ„ä¹‰ä¿¡æ¯ï¼Œåè€Œæœ‰æ—¶ä¼šå¼•å…¥å™ªå£°ã€‚
   æ‰€ä»¥ RMSNorm ç›´æ¥å¿½ç•¥æ‰å‡å€¼ï¼Œåªè°ƒèŠ‚â€œæ•´ä½“å¤§å°ï¼ˆå¹…åº¦ï¼‰â€ï¼Œæé«˜é€Ÿåº¦å’Œç¨³å®šæ€§

ç¤ºä¾‹ä»£ç ï¼šä¸ LLaMA/Qwen çš„ç‰ˆæœ¬ä¸€è‡´

1. æ˜ç¡® RMSNorm çš„ç›®æ ‡ï¼šåªè°ƒèŠ‚å¹…åº¦ï¼Œä¸åšå‡å€¼ä¸­å¿ƒåŒ–
2. ç¡®è®¤å½’ä¸€åŒ–ç»´åº¦ï¼šhidden_size
3. å®ç° RMSï¼šå¹³æ–¹ â†’ æ±‚å‡å€¼ â†’ å¼€æ ¹å·
4. é™¤ä»¥ RMS å¾—åˆ°å½’ä¸€åŒ–ç»“æœ
5. å†ä¹˜ä¸Šå¯è®­ç»ƒçš„ç¼©æ”¾å‚æ•° weight
6. å®Œæ•´å®ç°ä»…éœ€å‡ è¡Œï¼Œéå¸¸é«˜æ•ˆ

```python
import torch
import torch.nn as nn

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-8):
        """
        dim: è¾“å…¥å‘é‡çš„ç»´åº¦ï¼ˆä¾‹å¦‚ hidden_sizeï¼‰
        eps: ä¸ºäº†é˜²æ­¢é™¤ä»¥ 0 çš„æå°æ•°
        """
        super().__init__()
        
        # å¯è®­ç»ƒçš„ç¼©æ”¾ç³»æ•° gï¼Œå½¢çŠ¶å’Œ dim ä¸€æ ·
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps

    def forward(self, x):
        # x çš„å½¢çŠ¶é€šå¸¸ä¸º [batch, seq, hidden_dim]

        # 1. è®¡ç®—å‡æ–¹æ ¹ RMS
        #   x.pow(2).mean(-1, keepdim=True) ç­‰ä»·äºï¼šå¯¹æœ€åä¸€ä¸ªç»´åº¦æ±‚å¹³å‡
        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

        # 2. å°† x é™¤ä»¥ RMSï¼Œå®Œæˆå½’ä¸€åŒ–
        output = x / rms

        # 3. å†ä¹˜ä¸Šå¯è®­ç»ƒå‚æ•° weightï¼ˆé€å…ƒç´ ç¼©æ”¾ï¼‰
        return output * self.weight


# ---------------------------
# æµ‹è¯•ä¸€ä¸‹
# ---------------------------
if __name__ == "__main__":
    norm = RMSNorm(4)
    x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])
    print(norm(x))

```



### 2ã€ä»£ç 

å…ˆäº†è§£å‡ ä¸ªtorchæ–¹æ³•ï¼š

```python
import torch

# rsqrtæ–¹æ³•ï¼šè®¡ç®—æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹æ ¹çš„å€’æ•°
t = torch.tensor([1, 4, 9, 16])
print(torch.rsqrt(t))  # tensor([1.0000, 0.5000, 0.3333, 0.2500])

# onesæ–¹æ³•ï¼šç”Ÿæˆä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„
t1 = torch.ones(3, 4)
print(t1)  # tensor([[1., 1., 1., 1.],[1., 1., 1., 1.],[1., 1., 1., 1.]])
```



é’ˆå¯¹RMSNormï¼Œå…ˆæ’°å†™å¤§çº²ï¼Œç„¶åä¸€ç‚¹ç‚¹è¡¥å……ï¼š

- é›†æˆnn.Moduleç±»
- initåˆå§‹åŒ–
- _norm
- forward

ä¹Ÿå°±æ˜¯åœ¨`model/model_minimind.py`ä¸‹åŠ å…¥ï¼š

```python
import torch
import torch.nn as nn

class RMSNorm(torch.nn.Module):
    """
    RMSNormï¼šä»…å½’ä¸€åŒ–å‡æ–¹æ ¹ï¼Œä¸å‡å‡å€¼ï¼›å‚æ•°é‡æ›´å°ï¼Œæ•°å€¼æ›´ç¨³
    å…¬å¼ï¼šy = w * x / sqrt(mean(x^2) + eps)
    """
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))  # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•° Î³

    def _norm(self, x):
        # åœ¨æœ€åä¸€ç»´åš RMS å½’ä¸€åŒ–
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # ç”¨æƒé‡ç¼©æ”¾ï¼Œä¿æŒ dtype ä¸è¾“å…¥ä¸€è‡´
        return self.weight * self._norm(x.float()).type_as(x)
```



## å››ã€RoPE&YaRN

![image-20251205104251368](./assets/image-20251205104251368.png)

### 1ã€ç†è®º

è¶…ç®€æ€»ç»“ï¼š

| æ–¹æ³•     | åŸç†                 | ä¼˜ç‚¹             | ç¼ºç‚¹           |
| -------- | -------------------- | ---------------- | -------------- |
| ç»å¯¹ä½ç½® | ç»™æ¯ä¸ªä½ç½®å›ºå®šå‘é‡   | ç®€å•             | ä¸æ”¯æŒé•¿åºåˆ—   |
| ç›¸å¯¹ä½ç½® | è¡¨è¾¾ token è·ç¦»      | çµæ´»             | å®ç°è¾ƒå¤æ‚     |
| RoPE     | ç”¨æ—‹è½¬è§’åº¦è¡¨ç¤ºä½ç½®å·® | å¿«ã€å‡†ã€å¼º       | é•¿åºåˆ—æ—‹è½¬å¤ªå¿« |
| YaRN     | ç¼“å’Œ RoPE çš„è§’åº¦å¢é•¿ | é€‚ç”¨äºè¶…é•¿ä¸Šä¸‹æ–‡ | éœ€è¦é¢å¤–è®¡ç®—   |

#### 1ï¼‰ä½ç½®ç¼–ç 

Transformer çš„ Attention æœ¬èº« **ä¸ç†è§£é¡ºåº**ï¼ˆä¸åƒ RNNï¼Œæœ‰å¤©ç„¶çš„æ—¶é—´åºåˆ—ç»“æ„ï¼‰ã€‚

å› æ­¤æˆ‘ä»¬å¿…é¡»å‘Šè¯‰æ¨¡å‹ï¼šç¬¬ 1 ä¸ªè¯ã€ç¬¬ 2 ä¸ªè¯â€¦â€¦ä¹‹é—´æ˜¯æœ‰é¡ºåºå…³ç³»çš„ï¼å¦‚â€œäººå’¬ç‹—â€ä¸â€œç‹—å’¬äººâ€ï¼Œé¡ºåºå˜äº†è¯­ä¹‰å°±å˜äº†ã€‚

**ä½ç½®ç¼–ç ï¼ˆPosition Encodingï¼‰å°±æ˜¯ç»™æ¯ä¸ª token ä¸€ä¸ªâ€œä½ç½®ä¿¡å·â€ã€‚**

ä½ç½®ç¼–ç æœ‰åˆ†ä¸ºä¸¤ç§ï¼š

![image-20251205100746855](./assets/image-20251205100746855.png)

#### 2ï¼‰RoPE 

RoPE â€” Rotary Position Embeddingï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰ï¼Œæ˜¯ **åŸºäºå¤æ•°ç›¸ä½æ—‹è½¬**çš„ç›¸å¯¹ä½ç½®ç¼–ç ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š

> æŠŠ Query å’Œ Key çœ‹æˆäºŒç»´åæ ‡ï¼ŒæŠŠå®ƒä»¬ç»•åŸç‚¹æ—‹è½¬ä¸åŒè§’åº¦
>  è§’åº¦ âˆ ä½ç½® index

æ•°å­¦å½¢å¼æ˜¯æ—‹è½¬çŸ©é˜µï¼š
$$
\text{RoPE}(x) = R(\theta \cdot \text{pos}) \cdot x
$$
ä¼˜ç‚¹ï¼š

- âœ” è‡ªç„¶æ”¯æŒ**ç›¸å¯¹ä½ç½®**
- âœ” ä¸éœ€è¦é¢å¤–å‚æ•°
- âœ” è®¡ç®—é«˜æ•ˆ
- âœ” å¤§æ¨¡å‹ï¼ˆLLaMAã€GPT-Jã€Qwenï¼‰ä¸»æµæ–¹æ¡ˆ

ç¼ºç‚¹ï¼š

- âŒ é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æœ‰é™ï¼ˆè§’åº¦å¢é•¿å¤ªå¿«ï¼Œä¼šå‘ç”Ÿâ€œè½¬å¤šåœˆ â†’ ä¿¡æ¯æ··ä¹±â€ï¼‰

å› æ­¤å¤§å®¶æå‡ºäº†é•¿ä¸Šä¸‹æ–‡å¢å¼ºæ–¹æ³•ï¼Œæ¯”å¦‚ï¼šYaRN



#### 3ï¼‰YaRN

YaRN â€” æ‰©å±• RoPE èŒƒå›´çš„æ”¹è¿›ï¼ˆç”¨äºè¶…é•¿ä¸Šä¸‹æ–‡ï¼‰ï¼ŒYaRNï¼ˆYarn: Efficient Long-Context Scaling for RoPEï¼‰ä¸»è¦æ€æƒ³ï¼š

> è°ƒæ•´æ—‹è½¬è§’åº¦çš„å¢é•¿é€Ÿåº¦ï¼Œä½¿å¾—åœ¨é•¿åºåˆ—æ—¶ä¸â€œè½¬æ™•â€ã€‚

æ›´å…·ä½“ï¼š

YaRN = ç¼©æ”¾ RoPE çš„è§’åº¦ (Î¸)

RoPE çš„è§’åº¦éšä½ç½®çº¿æ€§å¢é•¿ï¼š
$$
\theta_i = \frac{1}{10000^{2i/d}}
$$
YaRN åœ¨ RoPE ä¹‹å‰åŠ ä¸€ä¸ª scalingï¼š
$$
\theta_i' = f(\theta_i, \text{ä½ç½®})
$$
YaRN çš„ scaling åŸç†å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š

- ğŸš€ **å‰åŠæ®µä½ç½®ï¼šä¿æŒ RoPE ç²¾åº¦**
- ğŸ§˜ **ååŠæ®µä½ç½®ï¼šè®©æ—‹è½¬å˜æ…¢ï¼Œé¿å…å¤±çœŸ**

ç»“æœï¼š

- âœ” å®ç° *éå¸¸é•¿ä¸Šä¸‹æ–‡*ï¼ˆ128kã€1Mã€ç”šè‡³æ— é™ï¼‰
- âœ” ä¸ä¼šç ´ååŸæ¨¡å‹è®­ç»ƒåˆ†å¸ƒ
- âœ” æ¨ç†é«˜æ•ˆ



#### 4ï¼‰æŠ½è±¡ç®€è¿°

![image-20251205101512336](./assets/image-20251205101512336.png)



#### 5ï¼‰ç¤ºä¾‹ä»£ç 

ä» 0 å®ç° RoPE & YaRN çš„æ€è·¯ï¼š

![image-20251205105356157](./assets/image-20251205105356157.png)

------

RoPEç¤ºä¾‹ä»£ç ï¼š

```python
import torch
import math

def apply_rope(x, rope_freqs):
    """
    x: [batch, seq, heads, head_dim]
    rope_freqs: é¢„è®¡ç®—å¥½çš„ cos/sin é¢‘ç‡
    """

    # åˆ†æˆå¶æ•°ç»´ä¸å¥‡æ•°ç»´
    x1 = x[..., ::2]   # even ç»´
    x2 = x[..., 1::2]  # odd ç»´

    cos, sin = rope_freqs

    # RoPE æ ¸å¿ƒå…¬å¼ï¼š
    # (x1, x2) æ—‹è½¬æˆ (x1*cos - x2*sin, x1*sin + x2*cos)
    x_rotated = torch.stack(
        [x1 * cos - x2 * sin,
         x1 * sin + x2 * cos],
        dim=-1
    )

    # æŠŠæœ€åçš„ [head_dim/2, 2] reshape å›åŸç»´åº¦
    return x_rotated.flatten(-2, -1)


def build_rope_freqs(dim, seq_len, base=10000):
    """
    æ„å»º RoPE çš„ cos/sin è¡¨
    dim: head_dim
    seq_len: æœ€å¤§åºåˆ—é•¿åº¦
    """
    half_dim = dim // 2
    # æ¯ä¸€ç»´çš„é¢‘ç‡ç¼©æ”¾ (Theta)
    freq = 1.0 / (base ** (torch.arange(0, half_dim) / half_dim))

    # åºåˆ—ä½ç½®
    t = torch.arange(seq_len)

    # å¤–ç§¯å¾—åˆ°æ‰€æœ‰ Î¸ * pos
    freqs = torch.outer(t, freq)

    return torch.cos(freqs), torch.sin(freqs)

```

------

YaRNï¼šå¯¹ RoPE é¢‘ç‡è¿›è¡Œ scaling

```python
def yarn_scale(freqs, scale=1.0, alpha=1.0):
    """
    YaRN å¯¹ RoPE çš„é¢‘ç‡ freq è¿›è¡Œç¼©æ”¾ï¼š
    freqs: RoPE çš„ Î¸ï¼ˆpos Ã— freqï¼‰
    scale: ç¼©æ”¾åŒºé—´ï¼Œä¾‹å¦‚ 8ã€16ã€32
    alpha: ç¼©æ”¾å¼ºåº¦ï¼ˆ1~2 å¸¸è§ï¼‰
    """
    # æœ€æ ¸å¿ƒï¼šå‡å°‘è¿œä½ç½®çš„æ—‹è½¬é€Ÿåº¦
    return freqs * (scale ** (freqs / freqs.max()) ** alpha)

```

æ›´çœŸå®çš„ YaRN ä¼šåˆ†åŒºé—´ç¼©æ”¾ï¼Œè¿™é‡Œç»™ä¸€ä¸ªæ›´æ¥è¿‘è®ºæ–‡çš„ç‰ˆæœ¬ï¼š

```python
def yarn_rope(freqs, long_factor=4.0, ext_factor=1.0):
    """
    æ›´æ¥è¿‘ YaRN è®ºæ–‡ï¼š
    - å¯¹ä½é¢‘éƒ¨åˆ†ä¿æŒä¸å˜
    - å¯¹é«˜é¢‘éƒ¨åˆ†è¿›è¡Œç¼“å’Œå¤„ç†ï¼ˆslow downï¼‰
    """
    # freqs: [seq_len, dim/2]
    max_freq = freqs.max()

    # åˆ†æ®µç¼©æ”¾ï¼šè¶Šé åæ—‹è½¬è¶Šæ…¢
    scale = 1 + (long_factor - 1) * (freqs / max_freq) ** ext_factor

    return freqs / scale

```

------

å°† YaRN + RoPE ç»“åˆåœ¨ä¸€èµ·

```python
def apply_rope_with_yarn(x, cos, sin, yarn_freqs):
    """
    x: è¾“å…¥å‘é‡
    yarn_freqs: ä½¿ç”¨ YaRN å¤„ç†åçš„ freqs
    """
    cos = torch.cos(yarn_freqs)
    sin = torch.sin(yarn_freqs)
    return apply_rope(x, (cos, sin))

```



### 2ã€ä»£ç 

é¦–å…ˆå­¦ä¹ å‡ ä¸ªtorchæ–¹æ³•ï¼š

```python
import torch

# whereæ–¹æ³•ï¼šæ ¹æ®æ¡ä»¶è¿”å›ä¸¤ä¸ªæ•°ç»„ä¸­å¯¹åº”ä½ç½®çš„æ•°æ®
x = torch.tensor([1, 2, 3, 4, 5])
y = torch.tensor([2, 4, 6, 8, 10])
condition = x > 3  # ä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼Œè¿”å›yä¸­å¯¹åº”ä½ç½®çš„æ•°æ®
print(torch.where(condition, x, y))  # tensor([2, 4, 6, 4, 5])

# arrageæ–¹æ³•: ç”Ÿæˆä¸€ä¸ªä¸€ç»´æ•°ç»„
t1 = torch.arange(0, 10, 2)  # 0å¼€å§‹ï¼Œæ­¥é•¿ä¸º2ï¼Œåˆ°10ç»“æŸ
t2 = torch.arange(5, 0, -1)  # 5å¼€å§‹ï¼Œæ­¥é•¿ä¸º-1ï¼Œåˆ°0ç»“æŸ
print(t1, t2)  # tensor([0, 2, 4, 6, 8]) tensor([5, 4, 3, 2, 1])

# outeræ–¹æ³•ï¼šè®¡ç®—ä¸¤ä¸ªæ•°ç»„çš„ä¹˜ç§¯
t3 = torch.tensor([1, 2, 3])
t4 = torch.tensor([7, 8, 9])
print(torch.outer(t3, t4))  # tensor([[ 7,  8,  9],[14, 16, 18],[21, 24, 27]])

# catæ–¹æ³•ï¼šè¿æ¥ä¸¤ä¸ªæ•°ç»„ï¼Œå¯ä»¥æŒ‡å®šè¿æ¥çš„ç»´åº¦(ç»´åº¦çš„æ„æ€ä¸ºï¼šæ•°ç»„çš„è¡Œæˆ–åˆ—)ï¼Œé»˜è®¤ä¸º0
t5 = torch.tensor([[1, 2, 3], [4, 5, 6]])
t6 = torch.tensor([[7, 8, 9], [10, 11, 12]])
print(torch.cat((t5, t6), dim=0))  # tensor([[ 1,  2,  3],[ 4,  5,  6],[ 7,  8,  9],[10, 11, 12]])
print(torch.cat((t5, t6), dim=1))  # tensor([[ 1,  2,  3,  7,  8,  9],[ 4,  5,  6, 10, 11, 12]])

# unsqueezeæ–¹æ³•ï¼šåœ¨æŒ‡å®šä½ç½®æ’å…¥ä¸€ä¸ªç»´åº¦ä¸º1çš„ç»´åº¦
t7 = torch.tensor([1, 2, 3])
print(t7.unsqueeze(0))  # tensor([[1, 2, 3]])
print(t7.unsqueeze(1))  # tensor([[1],[2],[3]])
print(t7.unsqueeze(-1))  # tensor([[1],[2],[3]])
```



æ„æ€YaRNçš„å¤§çº²ï¼š

- å†™å‡ºRoPE
- è®¡ç®—corr_dim
- è®¡ç®—power
- è®¡ç®—beta
- è®¡ç®—scale
- åº”ç”¨scale
- è¿”å›coså’Œsinï¼Œç”¨äºåº”ç”¨

ç„¶åç»“åˆå…¬å¼æ•²ä»£ç ï¼Œåœ¨`model/model_minimind.py`ä¸‹åŠ å…¥ï¼š

```python
import torch
import torch.nn as nn

import math
from typing import Optional, Tuple, List, Union


def precompute_freqs(
    dim: int,
    end: int = int(32 * 1024),
    rope_base: float = 1e6,
    rope_scaling: Optional[dict] = None,
):
    """
    Precompute the frequencies for the rotary positional embeddings.

    Args:
        dim: The dimension of the embeddings.
        end: The maximum sequence length.
        rope_base: The base of the exponential function.
        rope_scaling: A dictionary containing the scaling parameters for the rotary positional embeddings.

    Returns:
        A tuple of two tensors containing the cosine and sine frequencies.
    """

    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))

    if rope_scaling is not None:
        original_max, factor, beta_fast, beta_slow = (
            rope_scaling.get("original_max_position_embeddings", 2048),
            rope_scaling.get("factor", 4),
            rope_scaling.get("beta_fast", 4.0),
            rope_scaling.get("beta_slow", 1.0),
        )

        if end / original_max > 1.0:
            corr_dim = next(
                (i for i in range(dim // 2) if 2 * math.pi / freqs[i] > original_max),
                dim // 2,
            )

            power = torch.arange(0, dim // 2, device=freqs.device).float() / max(
                dim // 2 - 1, 1
            )

            beta = beta_slow + (beta_fast - beta_slow) * power

            scale = torch.where(
                torch.arange(dim // 2, device=freqs.device) < corr_dim,
                (beta * factor - beta + 1) / (beta * factor),
                1.0 / factor,
            )

            freqs = freqs * scale

        t = torch.arange(end, device=freqs.device)
        freqs = torch.outer(t, freqs).float()

        freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
        freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)

        return freqs_cos, freqs_sin


def apply_rotary_pos_emb(
    q, k, cos, sin, position_ids=None, unsqueeze_dim=1
) -> Tuple[torch.Tensor, torch.Tensor]:
    def rotate_half(x):
        return torch.cat(
            (-x[..., x.shape[-1] // 2 :], x[..., : x.shape[-1] // 2]), dim=-1
        )

    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (
        rotate_half(q) * sin.unsqueeze(unsqueeze_dim)
    )
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (
        rotate_half(k) * sin.unsqueeze(unsqueeze_dim)
    )
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x

    return (
        x[:, :, :, None, :]
        .expand(bs, slen, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )
```



## äº”ã€GQA

ğŸ§© GQAï¼ˆGrouped Query Attentionï¼‰å°ç»“

- å¯¹ Q ä¿ç•™æ‰€æœ‰æ³¨æ„åŠ›å¤´
- å¯¹ K/V è¿›è¡Œåˆ†ç»„å…±äº«
- èŠ‚çœ 50%-90% KV cache
- æ¨ç†æ›´å¿«
- è´¨é‡å‡ ä¹ä¸å˜
- å·²æˆä¸ºç°ä»£ LLM çš„æ ‡é…ç»“æ„

![image-20251218092940763](./assets/image-20251218092940763.png)

### 1ã€ç†è®º

GQAï¼ˆGrouped Query Attentionï¼‰æ˜¯ä¸€ç§ **ä»‹äº Multi-Head Attentionï¼ˆMHAï¼‰å’Œ Multi-Query Attentionï¼ˆMQAï¼‰ä¹‹é—´çš„æŠ˜ä¸­æ–¹æ¡ˆ**ã€‚
 å®ƒæœ€æ—©è¢« **PaLMã€Qwenã€LLaMA2 ç­‰å¤§æ¨¡å‹**é‡‡ç”¨ï¼Œç”¨æ¥ï¼š

> **å‡å°‘ KV Cache å­˜å‚¨ & åŠ å¿«æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¸æŸå¤±å¤ªå¤šæ¨¡å‹è´¨é‡ã€‚**

------

ğŸ”µ èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆè¦ GQAï¼Ÿ

åœ¨ Transformer çš„ Self-Attention é‡Œï¼š

- Qï¼ˆQueryï¼‰ï¼šæ¯ä¸ªå¤´ç‹¬ç«‹
- Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰é€šå¸¸æ¯ä¸ªå¤´ä¹Ÿç‹¬ç«‹ â†’ **KV å¾ˆå¤§**

å¯¹äº **æ¨ç†ï¼ˆinferenceï¼‰** æ¥è¯´ï¼š

- KV éœ€è¦ç¼“å­˜ï¼ˆKV Cacheï¼‰æ¥é¿å…é‡å¤è®¡ç®—
- å¦‚æœæœ‰ 32 ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå°±æœ‰ 32 ä»½ KV â†’ éå¸¸å æ˜¾å­˜ä¸”æ…¢

äºæ˜¯ç ”ç©¶è€…æå‡ºï¼š

------

ğŸ”µ MQAï¼šKey/Value å…±äº«ï¼ˆæç«¯ç‰ˆæœ¬ï¼‰

MQAï¼ˆMulti-Query Attentionï¼‰åšçš„æ˜¯ï¼š

> å…¨éƒ¨æ³¨æ„åŠ›å¤´å…±äº« 1 ä»½ Key å’Œ 1 ä»½ Valueã€‚

ä¼˜ç‚¹ï¼š
 âœ” KV Cache ä» N ä»½å˜æˆ 1 ä»½ï¼ˆå·¨çœæ˜¾å­˜ï¼‰
 ç¼ºç‚¹ï¼š
 âŒ è´¨é‡ä¸‹é™ï¼ˆè¡¨ç°ä¸å¦‚ MHAï¼‰

------

ğŸŸ¢ GQAï¼šåœ¨ MHA ä¸ MQA ä¹‹é—´å–æŠ˜ä¸­

GQAï¼ˆGrouped Query Attentionï¼‰åšçš„æ˜¯ï¼š

> å°†æ³¨æ„åŠ›å¤´åˆ†æˆè‹¥å¹²ç»„
>  æ¯ç»„å…±äº« **ä¸€ä»½** Key å’Œ Value

ä¾‹å¦‚ï¼š

- æœ‰ 32 ä¸ªå¤´ï¼ˆheadï¼‰
- åˆ†æˆ 4 ç»„ï¼ˆgroupï¼‰
- åˆ™æ¯ç»„ 8 ä¸ªå¤´å…±ç”¨ä¸€ä»½ KV

------

ğŸ§  æ•°å­¦ç»“æ„

- **Q**ï¼šæ¯ä¸ªå¤´ä»ç„¶ç‹¬ç«‹ï¼ˆä¿æŒå»ºæ¨¡èƒ½åŠ›ï¼‰
- **K/V**ï¼šæŒ‰ç»„å…±äº«ï¼ˆå‡å°‘å­˜å‚¨å’Œè®¡ç®—ï¼‰

KV cache æ•°é‡ä» `num_heads` å˜æˆäº† `num_groups`ï¼š
$$
\text{KV Cache Size}_\text{GQA} = \frac{1}{G} \times \text{MHA Cache Size}
$$
å½“ G=1 â†’ MQA
 å½“ G=head_num â†’ MHA
 å½“ 1 < G < heads â†’ **GQA**

------

ğŸŸ¢ GQA çš„ä¼˜ç‚¹

- âœ” **æ˜¾è‘—å‡å°‘ KV cache**ï¼ˆåŠ é€Ÿæ¨ç†ï¼‰
- âœ” **æ¥è¿‘ MHA çš„è´¨é‡è¡¨ç°**
- âœ” **å®ç°ç®€å•**

å› æ­¤ LLaMA2ã€Qwenã€PaLM2 ç­‰éƒ½é‡‡ç”¨ã€‚

------

2ï¸âƒ£ ç”¨å°å­¦ç”Ÿä¹Ÿèƒ½å¬æ‡‚çš„è¯æè¿° GQA

æƒ³è±¡ä½ åœ¨ç­é‡Œåšå°ç»„ä½œä¸šï¼š

- â€œæŸ¥è¯¢â€ï¼ˆQï¼‰æ˜¯æ¯ä¸ªå­¦ç”Ÿè‡ªå·±åšçš„äº‹æƒ…ï¼ˆå› ä¸ºæ¯ä¸ªäººæƒ³é—®çš„é—®é¢˜ä¸åŒï¼‰
- â€œçŸ¥è¯†åº“â€ï¼ˆK/Vï¼‰åƒè€å¸ˆç»™çš„èµ„æ–™æœ¬

ä¼ ç»Ÿåšæ³•ï¼ˆMHAï¼‰ï¼šæ¯ä¸ªå­¦ç”Ÿéƒ½å‘ä¸€æœ¬è‡ªå·±çš„èµ„æ–™æœ¬ â†’ å¤ªæµªè´¹çº¸ï¼

MQAï¼šå…¨ç­åªå‘ä¸€æœ¬èµ„æ–™æœ¬ â†’ å¤§å®¶æŒ¤æ¥æŒ¤å»ï¼Œæ‰¾èµ„æ–™å¾ˆæ…¢å“è´¨ä¹Ÿå·®ã€‚

GQA çš„åšæ³•éå¸¸èªæ˜ï¼šç­ä¸Š 32 ä¸ªå­¦ç”Ÿåˆ†æˆ 4 ç»„ï¼Œæ¯ç»„åªæœ‰ 1 æœ¬èµ„æ–™æœ¬ â†’ èŠ‚çœçº¸æœ¬ï¼Œåˆä¸ä¼šå¤§å®¶æŒ¤æ¥æŒ¤å»ã€‚

æ‰€ä»¥ï¼š

- æ¯ä¸ªå­¦ç”Ÿï¼ˆæ¯ä¸ªå¤´ï¼‰ä¿æŒä¸ªæ€§ï¼ˆQ ä»ç‹¬ç«‹ï¼‰
- æ¯ç»„å…±äº«ä¸€æœ¬èµ„æ–™æœ¬ï¼ˆK/V æŒ‰ç»„å…±äº«ï¼‰
- èŠ‚çœå¾ˆå¤šæˆæœ¬
- æ•ˆæœå‡ ä¹å’Œæ¯äººä¸€æœ¬ä¸€æ ·å¥½

------

ç¤ºä¾‹ä»£ç ï¼šç»“æ„ä¸ LLaMA2/Qwen çš„çœŸå®å®ç°ä¸€è‡´ï¼ˆç®€åŒ–åçš„ç‰ˆæœ¬ï¼‰

```python
import torch
import torch.nn as nn
import math

class GQA(nn.Module):
    def __init__(self, dim, num_heads, num_kv_groups):
        """
        dim: hidden size
        num_heads: æ³¨æ„åŠ›å¤´æ•°é‡ï¼ˆQå¤´æ•°ï¼‰
        num_kv_groups: KV çš„å…±äº«ç»„æ•°é‡ï¼Œä¾‹å¦‚ 32 å¤´åˆ† 4 ç»„ï¼Œåˆ™ group=4
        
        æ³¨æ„ï¼šæ¯ç»„å…±äº«ä¸€ä»½ Kã€V
        """
        super().__init__()
        
        assert num_heads % num_kv_groups == 0
        self.num_heads = num_heads
        self.num_kv_groups = num_kv_groups
        self.head_dim = dim // num_heads

        # Q ä»ç„¶æ˜¯ä¸ºæ¯ä¸ª head å•ç‹¬è®¡ç®—
        self.Wq = nn.Linear(dim, dim)
        
        # K/V æŒ‰ç»„è®¡ç®—ï¼Œåªç”Ÿæˆ num_kv_groups ä»½
        kv_dim = num_kv_groups * self.head_dim
        self.Wk = nn.Linear(dim, kv_dim)
        self.Wv = nn.Linear(dim, kv_dim)

        self.out = nn.Linear(dim, dim)

    def forward(self, x):
        B, L, D = x.shape

        # -----------------------------
        # 1. è®¡ç®— Qã€Kã€V
        # -----------------------------
        q = self.Wq(x)                 # [B, L, dim]
        k = self.Wk(x)                 # [B, L, kv_dim]
        v = self.Wv(x)                 # [B, L, kv_dim]

        # reshape æˆå¤šå¤´å½¢å¼
        q = q.view(B, L, self.num_heads, self.head_dim)         # [B, L, H, d]
        k = k.view(B, L, self.num_kv_groups, self.head_dim)     # [B, L, G, d]
        v = v.view(B, L, self.num_kv_groups, self.head_dim)     # [B, L, G, d]

        # -----------------------------
        # 2. å°† K/V æ‰©å±•åˆ°å¯¹åº”çš„ Q å¤´æ•°
        #    ä¸¾ä¾‹ï¼š32ä¸ªQå¤´ï¼Œ4ç»„KV â†’ æ¯ç»„é‡å¤8æ¬¡
        # -----------------------------
        repeat_factor = self.num_heads // self.num_kv_groups
        k = k.repeat_interleave(repeat_factor, dim=2)   # [B, L, H, d]
        v = v.repeat_interleave(repeat_factor, dim=2)

        # -----------------------------
        # 3. æ ‡å‡†çš„ Attention æ“ä½œ
        # -----------------------------
        att = torch.einsum("blhd,blHd->bh lH", q, k) / math.sqrt(self.head_dim)
        att = torch.softmax(att, dim=-1)
        out = torch.einsum("bh lH,blHd->blhd", att, v)

        # åˆå¹¶å¤šå¤´
        out = out.reshape(B, L, D)

        return self.out(out)

```

ä» 0 æ’°å†™ GQA çš„æ€è·¯

**â‘  æ˜ç¡®éœ€æ±‚ï¼šKV cache å¤ªå¤§äº†ï¼Œå¿…é¡»ç¼©å°**

æŸ¥è¯¢ Q å¯ä»¥å¤šå¤´ç‹¬ç«‹
 ä½† K/V å®Œå…¨æ²¡å¿…è¦ç»™æ¯ä¸ªå¤´éƒ½ç‹¬ç«‹ â†’ å¯ä»¥åˆå¹¶

**â‘¡ é€‰æ‹©æŠ˜ä¸­æ–¹æ¡ˆï¼šæŒ‰ç»„å…±äº« K/V**

- ç»™ Q ä¿ç•™æ‰€æœ‰å¤´ â†’ ä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ›
- ç»™ K/V å‡å°‘å¤´æ•° â†’ é™ä½ KV cache

æ ¸å¿ƒæ€æƒ³ï¼š

> Q: head = N
>  K/V: head = groups
>  groups << N

**â‘¢ è®¾è®¡ç»´åº¦ï¼š**

hidden_dim = 1024
 num_heads = 32
 num_kv_groups = 4

åˆ™ï¼š

- Q projection è¾“å‡ºï¼š32 heads
- K/V projection è¾“å‡ºï¼š4 heads
- æ¯ç»„å¿…é¡»å¹³åˆ† â†’ head_dim = 1024 / 32 = 32

**â‘£ å…³é”®æ“ä½œï¼šrepeat K/V**

å› ä¸º Q æœ‰ 32 ä¸ªå¤´ï¼ŒK/V åªæœ‰ 4 ä¸ªç»„ï¼Œæ‰€ä»¥ï¼š

- æ¯ä¸ª K/V ç»„è¦é‡å¤ 8 æ¬¡ï¼ˆ32/4=8ï¼‰
- ç”¨ `repeat_interleave` æœ€æ–¹ä¾¿

è¿™ä¸€è¡Œæ˜¯ GQA çš„çµé­‚ï¼š

```python
k = k.repeat_interleave(repeat_factor, dim=2)
v = v.repeat_interleave(repeat_factor, dim=2)
```

**â‘¤ ä¹‹åå°±æ˜¯æ ‡å‡† Attention**

ä¸æ™®é€š MHA ä¸€æ¨¡ä¸€æ ·ã€‚

**â‘¥ è¾“å‡º reshape å›åŸå‘é‡å¹¶é€šè¿‡çº¿æ€§å±‚**

å®Œå…¨æ ‡å‡†ã€‚



### 2ã€ä»£ç 

å…ˆäº†è§£ä¸€äº›torchæ–¹æ³•ï¼š

```python
import torch
import torch.nn as nn


# Dropoutå±‚ï¼šéšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºç½®é›¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
dropout_layer = nn.Dropout(p=0.5)  # 0.5 çš„æ¦‚ç‡è¿›è¡Œ dropout
t1 = torch.Tensor([1, 2, 3, 4, 5])
t2 = dropout_layer(t1)
print(t2)  # tensor([0., 0., 6., 8., 0.])

dropout_layer = nn.Dropout(p=0.1)  # 0.5 çš„æ¦‚ç‡è¿›è¡Œ dropout
t1 = torch.Tensor([1, 2, 3, 4, 5])
t2 = dropout_layer(t1)
print(t2)  # tensor([1.1111, 2.2222, 3.3333, 4.4444, 0.0000])


# Linearå±‚ï¼šå…¨è¿æ¥å±‚ï¼Œçº¿æ€§å˜æ¢ï¼Œy = xA^T + b
linear_layer = nn.Linear(
    in_features=5, out_features=3, bias=True
)  # è¾“å…¥ç»´åº¦ä¸º 5ï¼Œè¾“å‡ºç»´åº¦ä¸º 3, æœ‰åç½®
t1 = torch.Tensor([[1, 2, 3, 4, 5]])
t2 = linear_layer(t1)
print(t2)  # tensor([[1.1818, 1.8421, 2.1359]], grad_fn=<AddmmBackward0>)


# viewæ–¹æ³•ï¼šæ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼Œä¸æ”¹å˜æ•°æ®
t1 = torch.Tensor([[1, 2, 3, 4, 5, 6]])
t2 = t1.view(2, 3)
print(t2)  # tensor([[1., 2., 3.], [4., 5., 6.]])
print(t2.shape)  # torch.Size([2, 3])


# transposeæ–¹æ³•ï¼šè½¬ç½®å¼ é‡ï¼Œä¸æ”¹å˜æ•°æ®
t1 = torch.Tensor([[1, 2, 3], [4, 5, 6]])
t2 = t1.transpose(0, 1)
print(t2)  # tensor([[1, 4], [2, 5], [3, 6]])
print(t2.shape)  # torch.Size([3, 2])


# triuæ–¹æ³•ï¼šå–ä¸Šä¸‰è§’çŸ©é˜µï¼Œä¸æ”¹å˜æ•°æ®ã€‚åšæ©ç è®¡ç®—æ—¶ä¼šä½¿ç”¨
t1 = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
t2 = t1.triu(1)
print(t2)  # tensor([[0, 2, 3], [0, 0, 6], [0, 0, 0]])
print(t2.shape)  # torch.Size([3, 3])


# reshapeæ–¹æ³•ï¼šæ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼Œä¸æ”¹å˜æ•°æ®
t1 = torch.Tensor([[1, 2, 3, 4, 5, 6]])
t2 = t1.reshape(2, 3)
print(t2)  # tensor([[1, 2, 3], [4, 5, 6]])
print(t2.shape)  # torch.Size([2, 3])

```



æ’°å†™ä¸€äº›å·¥å…·å‡½æ•°ï¼šåœ¨`model/model_minimind.py`ä¸‹åŠ å…¥å¦‚ä¸‹ä»£ç 

```python
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    é‡å¤key-valueå¼ é‡ä»¥åŒ¹é…queryå¤´æ•° (ç”¨äºåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›GQA)
    ç­‰ä»·äºtorch.repeat_interleave(x, dim=2, repeats=n_rep)ï¼Œä½†æ›´é«˜æ•ˆ
    
    åœ¨GQAä¸­ï¼Œkeyå’Œvalueçš„å¤´æ•°å°‘äºqueryï¼Œéœ€è¦é‡å¤æ¥åŒ¹é…
    ä¾‹å¦‚ï¼š8ä¸ªqueryå¤´ï¼Œ2ä¸ªkvå¤´ï¼Œåˆ™éœ€è¦æ¯ä¸ªkvå¤´é‡å¤4æ¬¡
    
    Args:
        x: kvå¼ é‡ [batch, seq_len, num_kv_heads, head_dim]
        n_rep: é‡å¤æ¬¡æ•°
    
    Returns:
        é‡å¤åçš„å¼ é‡ [batch, seq_len, num_kv_heads * n_rep, head_dim]
    """
    bs, slen, num_key_value_heads, head_dim = x.shape  # è§£åŒ…è·å–å„ç»´åº¦å¤§å°
    if n_rep == 1:
        return x  # æ— éœ€é‡å¤ç›´æ¥è¿”å›
    
    # é«˜æ•ˆçš„é‡å¤å®ç°ï¼š
    # 1. x[:, :, :, None, :]: åœ¨ç¬¬4ç»´æ’å…¥æ–°ç»´åº¦ -> [bs, slen, num_kv_heads, 1, head_dim]
    # 2. .expand(...): æ‰©å±•ç¬¬4ç»´åˆ°n_rep -> [bs, slen, num_kv_heads, n_rep, head_dim]
    # 3. .reshape(...): åˆå¹¶ç¬¬3ã€4ç»´ -> [bs, slen, num_kv_heads * n_rep, head_dim]
    return (
        x[:, :, :, None, :].expand(bs, slen, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )
```



å¼€å§‹ç¼–å†™Attentionå±‚çš„å†…å®¹ï¼šåœ¨`model/model_minimind.py`ä¸‹åŠ å…¥å¦‚ä¸‹ä»£ç 

```python
import torch
import math
import torch.nn as nn
from typing import Optional, Tuple, List, Union
import torch.nn.functional as F
from transformers.activations import ACT2FN
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast


class Attention(nn.Module):
    """
    å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)å’ŒFlash Attentionä¼˜åŒ–

    GQAä»‹ç»ï¼š
    - ä¼ ç»ŸMHAï¼šqueryã€keyã€valueå¤´æ•°ç›¸åŒ
    - GQAï¼škeyã€valueå¤´æ•°å°‘äºqueryå¤´æ•°ï¼Œé€šè¿‡é‡å¤åŒ¹é…
    - ä¼˜ç‚¹ï¼šå‡å°‘KV cacheå†…å­˜å ç”¨ï¼Œä¿æŒæ€§èƒ½
    """

    def __init__(self, args: MokioMindConfig):
        super().__init__()

        # å¤„ç†GQAï¼šå¦‚æœæ²¡æœ‰æŒ‡å®škvå¤´æ•°ï¼Œåˆ™ä½¿ç”¨ä¸queryç›¸åŒçš„å¤´æ•°
        # ä¸‰å…ƒè¿ç®—ç¬¦ï¼šcondition ? value1 : value2
        self.num_key_value_heads = (
            args.num_attention_heads
            if args.num_key_value_heads is None
            else args.num_key_value_heads
        )

        # assertè¯­å¥ï¼šæ–­è¨€æ£€æŸ¥ï¼Œå¦‚æœæ¡ä»¶ä¸ºFalseåˆ™æŠ›å‡ºAssertionError
        # ç¡®ä¿queryå¤´æ•°èƒ½è¢«kvå¤´æ•°æ•´é™¤ï¼ˆGQAçš„åŸºæœ¬è¦æ±‚ï¼‰
        assert args.num_attention_heads % self.num_key_value_heads == 0

        # è®¾ç½®æ³¨æ„åŠ›å¤´é…ç½®
        self.n_local_heads = args.num_attention_heads  # queryå¤´æ•°
        self.n_local_kv_heads = self.num_key_value_heads  # key-valueå¤´æ•°
        self.n_rep = (
            self.n_local_heads // self.n_local_kv_heads
        )  # æ¯ä¸ªkvå¤´éœ€è¦é‡å¤çš„æ¬¡æ•°
        self.head_dim = args.hidden_size // args.num_attention_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # å®šä¹‰çº¿æ€§æŠ•å½±å±‚ (æ— åç½®ï¼ŒèŠ‚çœå‚æ•°)
        # nn.Linear(in_features, out_features, bias=False)
        self.q_proj = nn.Linear(
            args.hidden_size, args.num_attention_heads * self.head_dim, bias=False
        )  # QueryæŠ•å½±
        self.k_proj = nn.Linear(
            args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )  # KeyæŠ•å½±
        self.v_proj = nn.Linear(
            args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )  # ValueæŠ•å½±
        self.o_proj = nn.Linear(
            args.num_attention_heads * self.head_dim, args.hidden_size, bias=False
        )  # è¾“å‡ºæŠ•å½±

        # Dropoutå±‚ç”¨äºæ­£åˆ™åŒ–
        self.attn_dropout = nn.Dropout(args.dropout)  # æ³¨æ„åŠ›æƒé‡dropout
        self.resid_dropout = nn.Dropout(args.dropout)  # æ®‹å·®è¿æ¥dropout
        self.dropout = args.dropout  # ä¿å­˜dropoutç‡

        # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash Attention
        # hasattr(obj, 'attr'): æ£€æŸ¥å¯¹è±¡æ˜¯å¦æœ‰æŒ‡å®šå±æ€§
        # Flash Attentionéœ€è¦PyTorch >= 2.0
        self.flash = (
            hasattr(torch.nn.functional, "scaled_dot_product_attention")
            and args.flash_attn
        )
        # å¦‚æœä¸æ”¯æŒå¯ä»¥æ‰“å°è­¦å‘Š: print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(
        self,
        x: torch.Tensor,
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # ä¿®æ”¹ä¸ºæ¥æ”¶coså’Œsin
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache=False,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        # x: [batch_size, seq_len, hidden]
        bsz, seq_len, _ = x.shape

        # çº¿æ€§æŠ•å½±ä¸ºQ,K,V
        # q_proj: hidden -> num_heads * head_dim
        # k_proj/v_proj: hidden -> num_kv_heads * head_dim (GQAæƒ…å½¢)
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # å°†æŠ•å½±ç»“æœreshapeæˆå¤šå¤´æ ¼å¼
        # q: [bsz, seq_len, n_local_heads, head_dim]
        # k/v: [bsz, seq_len, n_local_kv_heads, head_dim]
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        # position_embeddingsæ˜¯é¢„è®¡ç®—çš„(cos, sin)ï¼ŒæŒ‰åºåˆ—ä½ç½®åˆ‡ç‰‡å¹¶åº”ç”¨RoPE
        cos, sin = position_embeddings
        # åªå–å½“å‰åºåˆ—é•¿åº¦çš„å‰ç¼€ï¼ˆç”¨äºinferenceæ—¶ä»start_poså¼€å§‹ï¼‰
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # -------------------- KV cache å¤„ç† --------------------
        # past_key_value: (past_k, past_v) æˆ– None
        # å½“å­˜åœ¨pastæ—¶ï¼Œå°†pastæ‹¼æ¥åˆ°å½“å‰k,vçš„æ—¶é—´ç»´åº¦ä¸Šï¼Œä¾¿äºè‡ªå›å½’æ¨ç†
        if past_key_value is not None:
            # past_key_value[0] çš„shapeä¸º [bsz, past_seq_len, n_local_kv_heads, head_dim]
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)

        # å¦‚æœéœ€è¦ç¼“å­˜ï¼Œè¿”å›æ‹¼æ¥åçš„(k,v)ï¼Œå¦åˆ™past_kvç½®ä¸ºNone
        past_kv = (xk, xv) if use_cache else None

        # -------------------- GQA: å¯¹KVé‡å¤ä»¥åŒ¹é…Qå¤´ --------------------
        # transposeåˆ°å½¢çŠ¶ [bsz, n_heads, seq_len, head_dim] ä»¥ä¾¿çŸ©é˜µä¹˜æ³•
        xq = xq.transpose(1, 2)
        # repeat_kvä¼šæŠŠk/vçš„å¤´æ•°ä» n_local_kv_heads -> n_local_kv_heads * n_rep (å³ç­‰äºn_local_heads)
        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)
        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)

        # -------------------- Attentionè®¡ç®— --------------------
        # ä¼˜å…ˆä½¿ç”¨PyTorch 2.0+çš„scaled_dot_product_attentionï¼ˆFlash Attentionå®ç°ï¼‰
        if (
            self.flash
            and seq_len > 1
            and (attention_mask is None or torch.all(attention_mask == 1))
        ):
            # å¦‚æœæ²¡æœ‰æ˜¾å¼çš„attention_maskï¼Œç›´æ¥ä¼ Noneè®©åº•å±‚é«˜æ•ˆå®ç°
            attn_mask = (
                None
                if attention_mask is None
                else attention_mask.view(bsz, 1, 1, -1)
                .expand(bsz, self.n_local_heads, seq_len, -1)
                .bool()
            )
            # F.scaled_dot_product_attentionæ˜¯PyTorchåœ¨æ–°ç‰ˆæœ¬ä¸­æä¾›çš„é«˜æ•ˆå®ç°
            output = F.scaled_dot_product_attention(
                xq,
                xk,
                xv,
                attn_mask=attn_mask,
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=True,  # è‡ªå›å½’ï¼ˆå› æœï¼‰æ³¨æ„åŠ›
            )
        else:
            # æ ‡å‡†å®ç°ï¼šscores = Q @ K^T / sqrt(d)
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)

            # causal mask: ä¸Šä¸‰è§’ï¼ˆå¯¹è§’çº¿ä»¥ä¸Šï¼‰ç½®ä¸º -infï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯
            causal_mask = torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1,
            )
            scores = scores + causal_mask.unsqueeze(0).unsqueeze(
                0
            )  # æ‰©å±•batchå’Œheadç»´åº¦

            # å¦‚æœæœ‰attention_mask(0/1)ï¼Œå°†å…¶æ‰©å±•åè½¬ä¸º -1e9 çš„åŠ æ€§maskï¼ˆæ©æ‰padä½ç½®ï¼‰
            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask

            # softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            # åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º
            output = scores @ xv

        # æ¢å¤å½¢çŠ¶å¹¶åšè¾“å‡ºæŠ•å½± + æ®‹å·®dropout
        output = output.transpose(1, 2).reshape(
            bsz, seq_len, -1
        )  # [bsz, seq_len, hidden]
        output = self.resid_dropout(self.o_proj(output))
        return output, past_kv

```



## å…­ã€FFN

![image-20251218105134958](./assets/image-20251218105134958.png)

### 1ã€ç†è®º

ç®€è¦æ€»ç»“ï¼š

- FFN æ˜¯ Transformer è¡¨è¾¾èƒ½åŠ›çš„æ ¸å¿ƒ
- Attention å†³å®šâ€œçœ‹è°â€ï¼ŒFFN å†³å®šâ€œæ€ä¹ˆç†è§£â€
- SiLU / SwiGLU æ˜¯ä¸ºå¤§æ¨¡å‹ç¨³å®šæ€§ä¸æ€§èƒ½è€Œç”Ÿ

#### 1ï¼‰FFN æ¨¡å—

1ï¸âƒ£ FFN åœ¨æ•´ä½“æ¶æ„ä¸­çš„ä½ç½®

ä»¥æ ‡å‡† Transformer Block ä¸ºä¾‹ï¼ˆLLaMA / Qwen / GPT éƒ½éµå¾ªè¿™ä¸€èŒƒå¼ï¼‰ï¼š

```python
x
 â”œâ”€ Multi-Head Attention
 â”‚
 â”œâ”€ Add & Norm
 â”‚
 â”œâ”€ FFN  â† ä½ ç°åœ¨å…³å¿ƒçš„æ¨¡å—
 â”‚
 â””â”€ Add & Norm
```

ğŸ‘‰ **Attention è´Ÿè´£â€œä¿¡æ¯äº¤äº’â€**
 ğŸ‘‰ **FFN è´Ÿè´£â€œé€ token çš„éçº¿æ€§å˜æ¢ä¸ç‰¹å¾å‡ç»´â€**

> ä¸€ä¸ªå…³é”®ç‚¹ï¼š
>  **FFN ä¸åš token ä¹‹é—´çš„äº¤äº’ï¼Œåªåœ¨ feature ç»´åº¦ä¸Šæ“ä½œ**

------

2ï¸âƒ£ FFN çš„æ ‡å‡†æ•°å­¦å½¢å¼

ä»¥æœ€ç»å…¸çš„ Transformer FFN ä¸ºä¾‹ï¼š
$$
\text{FFN}(x) = W_2 \cdot \sigma(W_1 x + b_1) + b_2
$$
å…¶ä¸­ï¼š

- $$
  $x \in \mathbb{R}^{d_{\text{model}}}$
  $$

  

- $$
  $W_1: d_{\text{model}} \rightarrow d_{\text{ff}}$
  $$

  

- $$
  $W_2: d_{\text{ff}} \rightarrow d_{\text{model}}$
  $$

  

- $$
  $d_{\text{ff}} \approx 4 \times d_{\text{model}}$
  $$

  

**è§£é‡Šï¼š**

- ç¬¬ä¸€æ­¥ï¼š**å‡ç»´ï¼ˆæŠ•å½±åˆ°æ›´é«˜ç»´ç©ºé—´ï¼‰**
- ç¬¬äºŒæ­¥ï¼š**éçº¿æ€§æ¿€æ´»**
- ç¬¬ä¸‰æ­¥ï¼š**é™ç»´ï¼ˆæ˜ å°„å›æ¨¡å‹ç»´åº¦ï¼‰**

------

3ï¸âƒ£ ä¸ºä»€ä¹ˆ FFN å¿…ä¸å¯å°‘ï¼Ÿ

ä»å¤§æ¨¡å‹æ¶æ„è§’åº¦çœ‹ï¼š

| æ¨¡å—      | ä¸»è¦èƒ½åŠ›                 |
| --------- | ------------------------ |
| Attention | å»ºç«‹ token ä¹‹é—´çš„ä¾èµ–    |
| FFN       | æä¾›å¼ºå¤§çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ› |
| LayerNorm | ç¨³å®šè®­ç»ƒ                 |
| Residual  | ä¿ç•™æ¢¯åº¦æµ               |

> **æ²¡æœ‰ FFN çš„ Transformerï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªâ€œçº¿æ€§æ³¨æ„åŠ›æ¨¡å‹â€**

------



#### 2ï¼‰SiLUï¼ˆSwishï¼‰

1ï¸âƒ£ SiLU çš„å®šä¹‰

SiLUï¼ˆSigmoid Linear Unitï¼‰ï¼Œä¹Ÿå« Swishï¼š
$$
\text{SiLU}(x) = x \cdot \sigma(x)
$$
å…¶ä¸­ï¼š
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

------

2ï¸âƒ£ ä¸ ReLU / GELU çš„å¯¹æ¯”

| æ¿€æ´»å‡½æ•° | ç‰¹ç‚¹              | é—®é¢˜       |
| -------- | ----------------- | ---------- |
| ReLU     | ç®€å•é«˜æ•ˆ          | æ­»ç¥ç»å…ƒ   |
| GELU     | å¹³æ»‘ï¼Œè¡¨ç°å¥½      | è®¡ç®—ç•¥å¤æ‚ |
| **SiLU** | å¹³æ»‘ + éé›¶è´ŸåŒºé—´ | ç¨æ…¢ä½†ç¨³å®š |

**SiLU çš„ä¼˜åŠ¿ï¼ˆå·¥ç¨‹è§’åº¦ï¼‰ï¼š**

- è¿ç»­å¯å¯¼ï¼ˆå¯¹å¤§æ¨¡å‹å¾ˆé‡è¦ï¼‰
- è´ŸåŒºé—´éé›¶ â†’ æ¢¯åº¦æ›´ç¨³å®š
- å®è¯æ•ˆæœå¥½ï¼ˆLLaMAã€Qwen å…¨ç”¨ï¼‰

> **LLaMA / Qwenï¼šFFN = Linear â†’ SiLU â†’ Linear**

------

3ï¸âƒ£ SiLU çš„ç›´è§‰ç†è§£

- å½“ x << 0ï¼šè¾“å‡ºæ¥è¿‘ 0ï¼ˆä½†ä¸ä¸º 0ï¼‰
- å½“ x >> 0ï¼šæ¥è¿‘çº¿æ€§ï¼ˆâ‰ˆ xï¼‰
- è‡ªåŠ¨â€œé—¨æ§â€ä¿¡æ¯æµï¼ˆç±»ä¼¼ soft gateï¼‰

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®ƒéå¸¸é€‚åˆ **å¤§è§„æ¨¡ FFN**



#### 3ï¼‰ç°ä»£å¤§æ¨¡å‹ä¸­çš„ FFN å˜ä½“

LLaMA ä½¿ç”¨çš„æ˜¯ **SwiGLU FFN**ï¼ˆéå¸¸é‡è¦ï¼‰

SwiGLU ç»“æ„ï¼š
$$
\text{FFN}(x) = W_2(\text{SiLU}(W_1 x) \odot W_3 x)
$$
ç»“æ„ç¤ºæ„ï¼š

```python
          â”Œâ”€â”€ W1 â”€â”€ SiLU â”€â”
x â”€â”€â”¬â”€â”€â”€â”€â”€â”¤               â”œâ”€ element-wise mul â”€ W2
    â””â”€â”€â”€â”€â”€â””â”€â”€ W3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆæ›´å¼ºï¼Ÿ**

- å¼•å…¥æ˜¾å¼ gatingï¼ˆé—¨æ§ï¼‰
- æ›´å¥½çš„å‚æ•°åˆ©ç”¨ç‡
- åœ¨ç›¸åŒå‚æ•°é‡ä¸‹æ•ˆæœæ›´å¥½



#### 4ï¼‰ç¤ºä¾‹ä»£ç 

ä¸‹é¢çš„ä»£ç  **å¯ä»¥ç›´æ¥è¿è¡Œ**ï¼Œå¹¶ä¸”**ç»“æ„ä¸ä½ åœ¨ LLaMA / Qwen ä¸­çœ‹åˆ°çš„å‡ ä¹ä¸€è‡´**

------

1ï¸âƒ£ åŸºç¡€ FFNï¼ˆLinear â†’ SiLU â†’ Linearï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FFN(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        x = self.fc1(x)          # å‡ç»´
        x = F.silu(x)            # SiLU æ¿€æ´»
        x = self.fc2(x)          # é™ç»´
        return x
```

ğŸ§  ä»£ç è®¾è®¡æ€è·¯

- **å®Œå…¨é€ token æ“ä½œ**ï¼ˆä¸å…³å¿ƒ seq_lenï¼‰
- Linear æœ¬è´¨ä¸Šæ˜¯å¯¹æœ€åä¸€ç»´åšæ˜ å°„
- ä¸ Attention è§£è€¦ï¼Œä¾¿äºå¹¶è¡Œ

------

2ï¸âƒ£ LLaMA é£æ ¼çš„ SwiGLU FFNï¼ˆæ¨èé‡ç‚¹æŒæ¡ï¼‰

```python
class SwiGLUFFN(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w3 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        gate = F.silu(self.w1(x))     # é—¨æ§åˆ†æ”¯
        value = self.w3(x)            # å€¼åˆ†æ”¯
        x = gate * value              # å…ƒç´ çº§ä¹˜æ³•
        x = self.w2(x)                # æŠ•å½±å› d_model
        return x
```

ğŸ§  è¿™æ˜¯ LLaMA / Qwen FFN çš„æ ¸å¿ƒæ€æƒ³

- ä¸¤æ¡å¹¶è¡Œçº¿æ€§æŠ•å½±
- ä¸€æ¡è´Ÿè´£ gateï¼Œä¸€æ¡è´Ÿè´£ value
- ç”¨ SiLU åšè½¯é—¨æ§

------

3ï¸âƒ£ ç®€å•æµ‹è¯•

```python
if __name__ == "__main__":
    batch, seq_len, d_model = 2, 4, 8
    d_ff = 32

    x = torch.randn(batch, seq_len, d_model)

    ffn = FFN(d_model, d_ff)
    y = ffn(x)

    swiglu = SwiGLUFFN(d_model, d_ff)
    y2 = swiglu(x)

    print(y.shape, y2.shape)
```

è¾“å‡ºï¼š

```python
torch.Size([2, 4, 8]) torch.Size([2, 4, 8])
```



### 2ã€ä»£ç 

å¼€å§‹ç¼–å†™FFNæ¨¡å—ï¼šåœ¨`model/model_minimind.py`ä¸‹åŠ å…¥å¦‚ä¸‹ä»£ç 

```python
class FeedForward(nn.Module):
    def __init__(self, config: MokioMindConfig):
        super().__init__()
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3)
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        # SwiGLUç±»ä¼¼äºGated Linear Unitå˜ä½“ï¼šact(gate(x)) * up(x)
        # gate_proj: hidden -> intermediate (ç”¨äºè®¡ç®—gateéƒ¨åˆ†)
        # up_proj: hidden -> intermediate (ç”¨äºè¢«gateçš„éƒ¨åˆ†)
        # down_proj: intermediate -> hidden (ç”¨äºæŠ•å½±å›hiddenç»´åº¦)
        self.gate_proj = nn.Linear(
            config.hidden_size, config.intermediate_size, bias=False
        )
        self.down_proj = nn.Linear(
            config.intermediate_size, config.hidden_size, bias=False
        )
        self.up_proj = nn.Linear(
            config.hidden_size, config.intermediate_size, bias=False
        )
        self.dropout = nn.Dropout(config.dropout)
        # ACT2FNæ˜¯transformersé‡Œæ¿€æ´»å‡½æ•°çš„æ˜ å°„è¡¨ï¼Œæ”¯æŒ'silu','gelu'ç­‰
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        """
        forwardå®ç°ä½¿ç”¨SwiGLUé£æ ¼çš„é—¨æ§æ¿€æ´»ï¼š
        output = down_proj( act_fn(gate_proj(x)) * up_proj(x) )
        å¹¶åœ¨è¾“å‡ºå‰åº”ç”¨dropout
        """
        gated = self.act_fn(self.gate_proj(x)) * self.up_proj(x)
        return self.dropout(self.down_proj(gated))

```



## ä¸ƒã€Block

ç»„è£…Transformerï¼š

![image-20251218105323305](./assets/image-20251218105323305.png)

åœ¨`model/model_minimind.py`ä¸‹åŠ å…¥å¦‚ä¸‹ä»£ç 

```python
class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MokioMindConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.self_attn = Attention(config)

        self.layer_id = layer_id
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )
        # self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)
        self.mlp = FeedForward(config)

    def forward(
        self,
        hidden_states,
        position_embeddings,
        past_key_value=None,
        use_cache=False,
        attention_mask=None,
    ):
        # æ®‹å·®è¿æ¥æ¨¡å¼ï¼šå…ˆåšLayerNorm -> Attention -> æ®‹å·®ç›¸åŠ  -> LayerNorm -> FFN -> æ®‹å·®ç›¸åŠ 
        # ä¿å­˜æ®‹å·®ä»¥ä¾›åç»­ç›¸åŠ 
        residual = hidden_states

        # æ³¨æ„åŠ›å­å±‚ï¼šè¾“å…¥å…ˆå½’ä¸€åŒ–ï¼ˆRMSNormï¼‰ï¼Œè¿”å›hidden_stateså’Œpresent_key_valueï¼ˆç”¨äºcacheï¼‰
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states),  # pre-norm
            position_embeddings,
            past_key_value,
            use_cache,
            attention_mask,
        )

        # æ³¨æ„åŠ›è¾“å‡ºä¸æ®‹å·®ç›¸åŠ 
        hidden_states = hidden_states + residual

        # å‰é¦ˆå­å±‚ï¼ˆpost-attention layernormï¼‰å¹¶ç›¸åŠ 
        hidden_states = hidden_states + self.mlp(
            self.post_attention_layernorm(hidden_states)
        )
        return hidden_states, present_key_value
```



## å…«ã€Model

å°†å‰è¿°æ‰€æœ‰å†…å®¹æ•´åˆèµ·æ¥ï¼Œç¼–å†™æˆå®Œæ•´çš„æ¨¡å‹ï¼šåœ¨`model/model_minimind.py`ä¸‹åŠ å…¥å¦‚ä¸‹ä»£ç 

```python
class MokioMindModel(nn.Module):
    def __init__(self, config: MokioMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = (
            config.vocab_size,
            config.num_hidden_layers,
        )
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.dropout = nn.Dropout(config.dropout)
        self.layers = nn.ModuleList(
            [MiniMindBlock(l, config) for l in range(self.num_hidden_layers)]
        )
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        freqs_cos, freqs_sin = precompute_freqs(
            dim=config.hidden_size // config.num_attention_heads,
            end=config.max_position_embeddings,
            rope_base=config.rope_theta,
            rope_scaling=config.rope_scaling,
        )
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
        use_cache: bool = False,
        **kwargs,
    ):
        # input_ids: [bsz, seq_len]
        batch_size, seq_length = input_ids.shape

        if hasattr(past_key_values, "layers"):
            past_key_values = None

        past_key_values = past_key_values or [None] * len(self.layers)

        # è®¡ç®—start_posï¼šå¦‚æœå­˜åœ¨pastï¼Œåˆ™start_posä¸ºå·²æœ‰paståºåˆ—é•¿åº¦
        start_pos = (
            past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0
        )

        # Embedding + dropout
        hidden_states = self.dropout(
            self.embed_tokens(input_ids)
        )  # [bsz, seq_len, hidden]

        position_embeddings = (
            self.freqs_cos[start_pos : start_pos + seq_length],
            self.freqs_sin[start_pos : start_pos + seq_length],
        )
        presents = []
        for layer_idx, (layer, past_key_value) in enumerate(
            zip(self.layers, past_key_values)
        ):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask,
            )
            presents.append(present)

        hidden_states = self.norm(hidden_states)

        return hidden_states, presents

        # # å¦‚æœä½¿ç”¨MoEï¼Œæ”¶é›†æ¯å±‚çš„aux_losså¹¶æ±‚å’Œè¿”å›ä»¥ä¾¿è®­ç»ƒä½¿ç”¨
        # aux_loss = sum(
        #     layer.mlp.aux_loss
        #     for layer in self.layers
        #     if isinstance(layer.mlp, MOEFeedForward)
        # )

        # return hidden_states, presents, aux_loss

```



## ä¹ã€CausalLM

é›†æˆHuggingfaceçš„ä¸¤ä¸ªåº“ï¼Œå»ºç«‹Huggingfaceé£æ ¼çš„Modelï¼šåœ¨`model/model_minimind.py`ä¸‹åŠ å…¥å¦‚ä¸‹ä»£ç 

```python
class MokioMindForCausalLM(PreTrainedModel, GenerationMixin):
    config_class = MokioMindConfig

    def __init__(self, config: MokioMindConfig):
        super().__init__(config)
        self.model = MokioMindModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.model.embed_tokens.weight = self.lm_head.weight

    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
        use_cache: bool = False,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **args,
    ):
        h, past_kvs,aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args,
        )

        slice_indices = (
            slice(-logits_to_keep, None)
            if isinstance(logits_to_keep, int)
            else logits_to_keep
        )
        logits = self.lm_head(h[:, slice_indices, :])

        return CausalLMOutputWithPast(
            logits=logits,
            past_key_values=past_kvs,
            hidden_states=h,
        )
```



## åã€å›é¡¾æ•´ä¸ªè¿‡ç¨‹

å¤§ä½¬åŸæ–‡ï¼š

![image-20251230160916933](./assets/image-20251230160916933.png)



## åä¸€ã€Dataset

### 1ã€ç†è®º

äº†è§£jsonlæ ¼å¼ï¼šç•¥

è‡ªå›å½’ï¼šç•¥

æŸå¤±æ©ç ï¼šç•¥



### 2ã€ä»£ç 

åœ¨`dataset/lm_dataset.py`ä¸‹åŠ å…¥ï¼š

```python
import json

from torch.utils.data import Dataset
import torch
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"


class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)

    def load_data(self, path):
        samples = []
        with open(path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                # æå–æ¯ä¸€è¡Œå†…å®¹æ”¾åˆ°sample
                data = json.loads(line.strip())
                samples.append(data)
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        sample = self.samples[index]
        # ç”¨tokenizerè¿›è¡Œç¼–ç 
        # è¶…è¿‡max_lengthçš„æˆªæ–­ï¼Œä¸åˆ°çš„å¡«å……
        encoding = self.tokenizer(
            str(sample["text"]),
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        input_ids = encoding.input_ids.squeeze()
        # å¿½ç•¥paddingäº§ç”Ÿçš„Y
        loss_mask = input_ids != self.tokenizer.pad_token_id
        # ç¬¬ä¸€ä¸ªåˆ°å€’æ•°ç¬¬äºŒä¸ªtoken
        X = torch.tensor(input_ids[:-1], dtype=torch.long)
        # ç¬¬äºŒä¸ªåˆ°æœ€åä¸€ä¸ªtoken
        Y = torch.tensor(input_ids[1:], dtype=torch.long)
        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)
        return X, Y, loss_mask
```



## åäºŒã€Pretrain

### 1ã€ç†è®º

åŠ¨æ€å­¦ä¹ ï¼šåŠ¨æ€å­¦ä¹ ç‡ç­‰ï¼Œç•¥ã€‚

æ¢¯åº¦ç´¯è®¡ï¼šæ˜¾å­˜æœ‰é™ï¼Œæ— æ³•å¤§çš„batchåŠ è½½ï¼Œç”¨å°çš„batchç´¯è®¡ã€‚



### 2ã€ä»£ç 

åœ¨trainer/trainer_utils.pyä¸­å†™å…¥ä¸€äº›è®­ç»ƒå·¥å…·ï¼š

```python
import math
import os
import random

import numpy as np
import torch
import torch.distributed as dist
from torch.utils.data import Sampler


# æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹
def is_main_process():
    return not dist.is_initialized() or dist.get_rank() == 0


# æ—¥å¿—
def Logger(content):
    if is_main_process():
        print(content)


# åŠ¨æ€å­¦ä¹ ç‡è®¡ç®—
def get_lr(current_step, total_steps, lr):
    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))


# åˆå§‹åŒ–åˆ†å¸ƒå¼
def init_distributed_mode():
    if int(os.environ.get("RANK", -1)) == -1:
        return 0  # éDDPæ¨¡å¼

    dist.init_process_group(backend="nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    return local_rank


# è®¾ç½®ç§å­
def setup_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# è®¾ç½®æ£€æŸ¥ç‚¹
def lm_checkpoint(
    lm_config,
    weight="full_sft",
    model=None,
    optimizer=None,
    epoch=0,
    step=0,
    wandb=None,
    save_dir="checkpoints",
    **kwargs,
):
    os.makedirs(save_dir, exist_ok=True)

    moe_path = "_moe" if hasattr(lm_config, "use_moe") and lm_config.use_moe else ""
    ckp_path = f"{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}.pth"
    resume_path = f"{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_resume.pth"

    if model is not None:
        from torch.nn.parallel import DistributedDataParallel

        if isinstance(model, DistributedDataParallel):
            state_dict = model.module.state_dict()
        else:
            state_dict = model.state_dict()

        ckp_tmp = ckp_path + ".tmp"
        torch.save({k: v.half() for k, v in state_dict.items()}, ckp_tmp)
        os.replace(ckp_tmp, ckp_path)

        wandb_id = None
        if wandb:
            if hasattr(wandb, "get_run"):
                run = wandb.get_run()
                wandb_id = getattr(run, "id", None) if run else None
            else:
                wandb_id = getattr(wandb, "id", None)

        resume_data = {
            "model": state_dict,
            "optimizer": optimizer.state_dict(),
            "epoch": epoch,
            "step": step,
            "world_size": dist.get_world_size() if dist.is_initialized() else 1,
            "wandb_id": wandb_id,
        }

        for key, value in kwargs.items():
            if value is not None:
                if hasattr(value, "state_dict"):
                    if isinstance(value, DistributedDataParallel):
                        resume_data[key] = value.module.state_dict()
                    else:
                        resume_data[key] = value.state_dict()
                else:
                    resume_data[key] = value

        resume_tmp = resume_path + ".tmp"
        torch.save(resume_data, resume_tmp)
        os.replace(resume_tmp, resume_path)

    else:  # åŠ è½½æ¨¡å¼
        if os.path.exists(resume_path):
            ckp_data = torch.load(resume_path, map_location="cpu")
            saved_ws = ckp_data.get("world_size", 1)
            current_ws = dist.get_world_size() if dist.is_initialized() else 1

            if saved_ws != current_ws:
                ckp_data["step"] = ckp_data["step"] * saved_ws // current_ws
                Logger(
                    f"GPUæ•°é‡å˜åŒ–({saved_ws}â†’{current_ws})ï¼Œstepå·²è‡ªåŠ¨è½¬æ¢ä¸º{ckp_data['step']}"
                )

            return ckp_data
        return None


# åˆå§‹åŒ–æ¨¡å‹
def init_model(
    lm_config,
    from_weight="pretrain",
    tokenizer_path=None,
    save_dir="out",
    device="cuda",
):
    from transformers import AutoTokenizer

    from model.model_minimind import MokioMindForCausalLM

    # å¦‚æœæ²¡æœ‰æŒ‡å®š tokenizer_pathï¼Œä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ model æ–‡ä»¶å¤¹
    if tokenizer_path is None:
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        tokenizer_path = os.path.join(project_root, "model")

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    model = MokioMindForCausalLM(lm_config)

    if from_weight != "none":
        moe_suffix = (
            "_moe" if hasattr(lm_config, "use_moe") and lm_config.use_moe else ""
        )
        weight_path = (
            f"{save_dir}/{from_weight}_{lm_config.hidden_size}{moe_suffix}.pth"
        )

        weights = torch.load(weight_path, map_location=device)

        model.load_state_dict(weights, strict=False)

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    Logger(f"æ‰€åŠ è½½Modelå¯è®­ç»ƒå‚æ•°ï¼š{total_params / 1e6:.3f} ç™¾ä¸‡")

    return model.to(device), tokenizer


class SkipBatchSampler(Sampler):
    def __init__(self, sampler, batch_size, skip_batches=0):
        self.sampler = sampler  #
        self.batch_size = batch_size
        self.skip_batches = skip_batches

    def __iter__(self):
        batch = []  # å½“å‰æ‰¹æ¬¡
        skipped = 0  # å·²è·³è¿‡çš„æ‰¹æ¬¡æ•°

        for idx in self.sampler:
            batch.append(idx)  # æ·»åŠ æ ·æœ¬åˆ°å½“å‰æ‰¹æ¬¡

            if len(batch) == self.batch_size:
                if skipped < self.skip_batches:
                    skipped += 1  # å¢åŠ è·³è¿‡è®¡æ•°
                    batch = []  # æ¸…ç©ºæ‰¹æ¬¡ï¼Œä¸è¿”å›
                    continue  # è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡

                yield batch
                batch = []  # é‡ç½®æ‰¹æ¬¡

        if len(batch) > 0 and skipped >= self.skip_batches:
            yield batch

    def __len__(self):
        total_batches = (len(self.sampler) + self.batch_size - 1) // self.batch_size

        return max(0, total_batches - self.skip_batches)

```

åœ¨trainer/train_pretrain.pyä¸­å†™å…¥ä¸€äº›è®­ç»ƒä»£ç ï¼š

```python
import os
import sys

__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import time  # æ—¶é—´ç»Ÿè®¡
import warnings  # è­¦å‘Šæ§åˆ¶
from contextlib import nullcontext  # ä¸Šä¸‹æ–‡ç®¡ç†å™¨

import torch
import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
from torch import nn, optim  # ä¼˜åŒ–å™¨å’Œç¥ç»ç½‘ç»œæ¨¡å—
from torch.nn.parallel import DistributedDataParallel  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
from torch.utils.data import DataLoader, DistributedSampler  # æ•°æ®åŠ è½½å™¨

from dataset.lm_dataset import PretrainDataset
from model.model_minimind import MokioMindConfig
from trainer.trainer_utils import (  # è®­ç»ƒå·¥å…·å‡½æ•°
    Logger,
    SkipBatchSampler,
    get_lr,
    init_distributed_mode,
    init_model,
    is_main_process,
    lm_checkpoint,
    setup_seed,
)

# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œä¿æŒè¾“å‡ºæ¸…æ´
warnings.filterwarnings("ignore")


def train_epoch(epoch, loader, iters, start_step=0, wandb=None):
    loss_fct = nn.CrossEntropyLoss(reduction="none")
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

    # éå†æ•°æ®æ‰¹æ¬¡
    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):
        X = X.to(args.device)
        Y = Y.to(args.device)
        loss_mask = loss_mask.to(args.device)

        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)

        for param_group in optimizer.param_groups:
            param_group["lr"] = lr

        with autocast_ctx:
            # å‰å‘ä¼ æ’­
            res = model(X)

            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),  # [batch*seq, vocab_size]
                Y.view(-1),  # [batch*seq]
            ).view(Y.size())  # æ¢å¤ä¸º [batch_size, seq_len]

            loss = (loss * loss_mask).sum() / loss_mask.sum()

            loss += res.aux_loss

            loss = loss / args.accumulation_steps

        scaler.scale(loss).backward()

        if (step + 1) % args.accumulation_steps == 0:
            # scaler.unscale_(): è¿˜åŸæ¢¯åº¦çš„çœŸå®å€¼
            scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

            # ğŸ“š ä¼˜åŒ–å™¨æ›´æ–°çŸ¥è¯†ç‚¹
            # scaler.step(): æ‰§è¡Œå‚æ•°æ›´æ–°
            # scaler.update(): æ›´æ–°scalerçš„ç¼©æ”¾å› å­
            scaler.step(optimizer)
            scaler.update()

            optimizer.zero_grad(set_to_none=True)

        if step % args.log_interval == 0 or step == iters - 1:
            spend_time = time.time() - start_time
            current_loss = loss.item() * args.accumulation_steps  # æ¢å¤çœŸå®æŸå¤±å€¼
            current_lr = optimizer.param_groups[-1]["lr"]  # å½“å‰å­¦ä¹ ç‡

            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60

            Logger(
                f"Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:"
            )

            # è®°å½•åˆ°å®éªŒè·Ÿè¸ªç³»ç»Ÿ
            if wandb:
                wandb.log(
                    {"loss": current_loss, "lr": current_lr, "epoch_Time": eta_min}
                )

        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

            # æ„å»ºä¿å­˜è·¯å¾„
            moe_suffix = (
                "_moe" if hasattr(lm_config, "use_moe") and lm_config.use_moe else ""
            )
            ckp = f"{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth"

            # ğŸ“š åˆ†å¸ƒå¼æ¨¡å‹ä¿å­˜çŸ¥è¯†ç‚¹
            # DDPæ¨¡å‹éœ€è¦é€šè¿‡.moduleè®¿é—®çœŸæ­£çš„æ¨¡å‹
            if isinstance(model, torch.nn.parallel.DistributedDataParallel):
                state_dict = model.module.state_dict()
            else:
                state_dict = model.state_dict()

            # ğŸ“š åŠç²¾åº¦ä¿å­˜çŸ¥è¯†ç‚¹
            # å°†float32å‚æ•°è½¬ä¸ºfloat16ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´
            state_dict = {k: v.half() for k, v in state_dict.items()}
            torch.save(state_dict, ckp)

            # ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€
            lm_checkpoint(
                lm_config,
                weight=args.save_weight,
                model=model,
                optimizer=optimizer,
                scaler=scaler,
                epoch=epoch,
                step=step,
                wandb=wandb,
                save_dir="checkpoints",
            )

            model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MiniMind Pretraining")

    # ========== åŸºç¡€è®­ç»ƒå‚æ•° ==========
    parser.add_argument("--save_dir", type=str, default="out", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument(
        "--save_weight", default="pretrain", type=str, help="ä¿å­˜æƒé‡çš„å‰ç¼€å"
    )
    parser.add_argument(
        "--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®1è½®zeroæˆ–2-6è½®å……åˆ†è®­ç»ƒï¼‰"
    )
    parser.add_argument("--batch_size", type=int, default=32, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="åˆå§‹å­¦ä¹ ç‡")

    # ========== ç¡¬ä»¶å’Œæ€§èƒ½å‚æ•° ==========
    parser.add_argument(
        "--device",
        type=str,
        default="cuda:0" if torch.cuda.is_available() else "cpu",
        help="è®­ç»ƒè®¾å¤‡",
    )
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    parser.add_argument("--num_workers", type=int, default=1, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")

    # ========== è®­ç»ƒç­–ç•¥å‚æ•° ==========
    parser.add_argument(
        "--accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°"
    )
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--log_interval", type=int, default=100, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=100, help="æ¨¡å‹ä¿å­˜é—´éš”")

    # ========== æ¨¡å‹æ¶æ„å‚æ•° ==========
    parser.add_argument("--hidden_size", default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_hidden_layers", default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument(
        "--max_seq_len", default=512, type=int, help="è®­ç»ƒçš„æœ€å¤§æˆªæ–­é•¿åº¦"
    )
    parser.add_argument(
        "--use_moe",
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )

    # ========== æ•°æ®å’Œæ¢å¤å‚æ•° ==========
    parser.add_argument(
        "--data_path",
        type=str,
        default="dataset/pretrain_hq.jsonl",
        help="é¢„è®­ç»ƒæ•°æ®è·¯å¾„",
    )
    parser.add_argument(
        "--from_weight",
        default="none",
        type=str,
        help="åŸºäºå“ªä¸ªæƒé‡è®­ç»ƒï¼Œä¸ºnoneåˆ™ä»å¤´å¼€å§‹",
    )
    parser.add_argument(
        "--from_resume",
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )

    # ========== å®éªŒè·Ÿè¸ªå‚æ•° ==========
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument(
        "--wandb_project", type=str, default="MiniMind-Pretrain", help="wandbé¡¹ç›®å"
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # ========== 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­ ==========
    """
    ğŸ“š åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–çŸ¥è¯†ç‚¹ï¼š
    - local_rank: å½“å‰è¿›ç¨‹åœ¨æœ¬æœºä¸Šçš„GPUç¼–å·
    - éšæœºç§å­: ç¡®ä¿ä¸åŒè¿›ç¨‹æœ‰ä¸åŒä½†å¯å¤ç°çš„éšæœºåºåˆ—
    - è¿™æ ·æ—¢ä¿è¯äº†éšæœºæ€§ï¼Œåˆä¿è¯äº†å¯å¤ç°æ€§
    """
    local_rank = init_distributed_mode()
    if dist.is_initialized():
        args.device = f"cuda:{local_rank}"  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä½¿ç”¨å¯¹åº”çš„GPU

    # ğŸ“š éšæœºç§å­è®¾ç½®çŸ¥è¯†ç‚¹
    # ä¸åŒè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„ç§å­ï¼Œé¿å…æ•°æ®é‡‡æ ·å®Œå…¨ç›¸åŒ
    # 42æ˜¯åŸºç¡€ç§å­ï¼Œæ¯ä¸ªè¿›ç¨‹åŠ ä¸Šè‡ªå·±çš„rankä¿è¯ä¸åŒ
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))

    # ========== 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ç‚¹ ==========
    """
    ğŸ“š æ¨¡å‹é…ç½®å’Œæ£€æŸ¥ç‚¹ç®¡ç†ï¼š
    - åˆ›å»ºä¿å­˜ç›®å½•
    - æ„å»ºæ¨¡å‹é…ç½®å¯¹è±¡
    - å°è¯•åŠ è½½æ–­ç‚¹ç»­è®­æ•°æ®
    """
    os.makedirs(args.save_dir, exist_ok=True)  # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨

    # åˆ›å»ºMiniMindæ¨¡å‹é…ç½®
    lm_config = MokioMindConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        use_moe=bool(args.use_moe),
    )

    # ğŸ“š æ–­ç‚¹ç»­è®­çŸ¥è¯†ç‚¹
    # å¦‚æœå¼€å¯äº†æ–­ç‚¹ç»­è®­ï¼Œå°è¯•åŠ è½½ä¹‹å‰çš„è®­ç»ƒçŠ¶æ€
    ckp_data = (
        lm_checkpoint(lm_config, weight=args.save_weight, save_dir="checkpoints")
        if args.from_resume == 1
        else None
    )

    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    """
    ğŸ“š æ··åˆç²¾åº¦è®­ç»ƒçŸ¥è¯†ç‚¹ï¼š
    - bfloat16: Googleå¼€å‘ï¼Œæ•°å€¼èŒƒå›´å¤§ï¼Œæ›´ç¨³å®š
    - float16: æ ‡å‡†åŠç²¾åº¦ï¼ŒèŠ‚çœå†…å­˜ä½†å¯èƒ½æº¢å‡º
    - autocast: è‡ªåŠ¨é€‰æ‹©ç²¾åº¦ï¼Œå…³é”®è¿ç®—ç”¨float32
    """
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16

    # ğŸ“š ä¸Šä¸‹æ–‡ç®¡ç†å™¨çŸ¥è¯†ç‚¹
    # CPUä¸æ”¯æŒautocastï¼Œä½¿ç”¨nullcontextä½œä¸ºç©ºæ“ä½œ
    autocast_ctx = (
        nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)
    )

    # ========== 4. é…ç½®WandBå®éªŒè·Ÿè¸ª ==========
    """
    ğŸ“š å®éªŒè·Ÿè¸ªç³»ç»ŸçŸ¥è¯†ç‚¹ï¼š
    - WandB: å®éªŒç®¡ç†å¹³å°ï¼Œè®°å½•è®­ç»ƒè¿‡ç¨‹
    - SwanLab: å›½äº§æ›¿ä»£æ–¹æ¡ˆ
    - æ”¯æŒæ–­ç‚¹ç»­è®­æ—¶æ¢å¤åˆ°åŒä¸€ä¸ªå®éªŒ
    """
    wandb = None
    if args.use_wandb and is_main_process():
        # ä½¿ç”¨SwanLabä½œä¸ºWandBçš„æ›¿ä»£
        import swanlab as wandb

        # ğŸ“š å®éªŒæ¢å¤çŸ¥è¯†ç‚¹
        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹æ•°æ®ï¼Œè·å–ä¹‹å‰çš„wandb_idæ¥æ¢å¤å®éªŒ
        wandb_id = ckp_data.get("wandb_id") if ckp_data else None
        resume = "must" if wandb_id else None  # å¿…é¡»æ¢å¤åˆ°æŒ‡å®šå®éªŒ

        # æ„å»ºå®éªŒåç§°ï¼ŒåŒ…å«å…³é”®è¶…å‚æ•°
        wandb_run_name = f"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}"
        wandb.init(
            project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume
        )

    # ========== 5. å®šä¹‰æ¨¡å‹ã€æ•°æ®ã€ä¼˜åŒ–å™¨ ==========
    """
    ğŸ“š è®­ç»ƒç»„ä»¶åˆå§‹åŒ–ï¼š
    - æ¨¡å‹: æ ¹æ®é…ç½®åˆ›å»ºMiniMindæ¨¡å‹
    - æ•°æ®é›†: åŠ è½½é¢„è®­ç»ƒæ•°æ®
    - é‡‡æ ·å™¨: åˆ†å¸ƒå¼è®­ç»ƒçš„æ•°æ®åˆ†é…
    - ä¼˜åŒ–å™¨: AdamWä¼˜åŒ–å™¨
    - ç¼©æ”¾å™¨: æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾
    """
    # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)

    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=args.max_seq_len)

    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None

    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == "float16"))

    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)

    start_epoch, start_step = 0, 0
    if ckp_data:
        # æ¢å¤æ¨¡å‹å‚æ•°
        model.load_state_dict(ckp_data["model"])
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŠ¨é‡ã€æ–¹å·®ä¼°è®¡ç­‰ï¼‰
        optimizer.load_state_dict(ckp_data["optimizer"])
        # æ¢å¤æ¢¯åº¦ç¼©æ”¾å™¨çŠ¶æ€
        scaler.load_state_dict(ckp_data["scaler"])
        # æ¢å¤è®­ç»ƒè¿›åº¦
        start_epoch = ckp_data["epoch"]
        start_step = ckp_data.get("step", 0)

    if dist.is_initialized():
        # ğŸ“š RoPEä½ç½®ç¼–ç ç‰¹æ®Šå¤„ç†
        # freqs_cos, freqs_sinæ˜¯ä½ç½®ç¼–ç ç¼“å­˜ï¼Œä¸éœ€è¦æ¢¯åº¦åŒæ­¥
        model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        model = DistributedDataParallel(model, device_ids=[local_rank])

    for epoch in range(start_epoch, args.epochs):
        # ğŸ“š åˆ†å¸ƒå¼é‡‡æ ·å™¨epochè®¾ç½®
        # æ¯ä¸ªepochè®¾ç½®ä¸åŒçš„éšæœºç§å­ï¼Œç¡®ä¿æ•°æ®é¡ºåºéšæœºåŒ–
        if train_sampler:
            train_sampler.set_epoch(epoch)

        # ğŸ“š æ–­ç‚¹ç»­è®­é€»è¾‘
        if epoch == start_epoch and start_step > 0:  # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            # ä½¿ç”¨è·³æ‰¹é‡‡æ ·å™¨ï¼Œè·³è¿‡å·²è®­ç»ƒçš„æ•°æ®
            batch_sampler = SkipBatchSampler(
                train_sampler or range(len(train_ds)), args.batch_size, start_step + 1
            )
            loader = DataLoader(
                train_ds,
                batch_sampler=batch_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            Logger(
                f"Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹"
            )
            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)
        else:  # é»˜è®¤ä»å¤´å¼€å§‹
            loader = DataLoader(
                train_ds,
                batch_size=args.batch_size,
                shuffle=(train_sampler is None),
                sampler=train_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            train_epoch(epoch, loader, len(loader), 0, wandb)

```



## åä¸‰ã€è®­ç»ƒ

æµ‹è¯•cudaæ˜¯å¦å¯ç”¨ï¼š

```python
$ python
Python 3.13.4 (main, Jun  4 2025, 17:37:06) [Clang 20.1.4 ] on linux
Type "help", "copyright", "credits" or "license" for more information.
Ctrl click to launch VS Code Native REPL
>>> import torch
>>> print(torch.__version__)
2.9.1+cu128
>>> print(torch.cuda.is_available())
True
```

å‡†å¤‡æ•°æ®é›†ï¼šä»ä¸‹æ–‡æä¾›çš„[æ•°æ®é›†ä¸‹è½½é“¾æ¥](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) ä¸‹è½½éœ€è¦çš„æ•°æ®æ–‡ä»¶ï¼ˆåˆ›å»º`./dataset`ç›®å½•ï¼‰å¹¶æ”¾åˆ°`./dataset`ä¸‹

<details open="" style="box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;"><summary style="box-sizing: border-box; display: list-item; cursor: pointer;">æ³¨ï¼šæ•°æ®é›†é¡»çŸ¥</summary><p dir="auto" style="box-sizing: border-box; margin-top: 0px; margin-bottom: 16px;">é»˜è®¤æ¨èä¸‹è½½<code style="box-sizing: border-box; font-family: &quot;Monaspace Neon&quot;, ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; tab-size: 4; padding: 0.2em 0.4em; margin: 0px; white-space: break-spaces; background-color: rgba(129, 139, 152, 0.12); border-radius: 6px;">pretrain_hq.jsonl</code><span>&nbsp;</span>+<span>&nbsp;</span><code style="box-sizing: border-box; font-family: &quot;Monaspace Neon&quot;, ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; tab-size: 4; padding: 0.2em 0.4em; margin: 0px; white-space: break-spaces; background-color: rgba(129, 139, 152, 0.12); border-radius: 6px;">sft_mini_512.jsonl</code>æœ€å¿«é€Ÿåº¦å¤ç°ZeroèŠå¤©æ¨¡å‹ã€‚</p><p dir="auto" style="box-sizing: border-box; margin-top: 0px; margin-bottom: 16px;">æ•°æ®æ–‡ä»¶å¯è‡ªç”±é€‰æ‹©ï¼Œä¸‹æ–‡æä¾›äº†å¤šç§æ­é…æ–¹æ¡ˆï¼Œå¯æ ¹æ®è‡ªå·±æ‰‹å¤´çš„è®­ç»ƒéœ€æ±‚å’ŒGPUèµ„æºè¿›è¡Œé€‚å½“ç»„åˆã€‚</p></details>



ä¸‹è½½æ•°æ®é›†å…·ä½“æ­¥éª¤ï¼šè¿™é‡Œä»…ä¸‹è½½`pretrain_hq.jsonl`ä¸€ä¸ªè‹èšé›†ï¼Œä¸”ä¸‹è½½åˆ°`dataset`ç›®å½•ä¸‹

```
pip install modelscope
modelscope download --dataset gongjy/minimind_dataset pretrain_hq.jsonl --local_dir dataset
```

å°†é¡¹ç›®ä¸­çš„tokenizer_config.jsonä¸tokenizer.jsonæ”¾å…¥modelç›®å½•ä¸‹ã€‚

å¼€å§‹è®­ç»ƒï¼š

```python
$ python trainer/train_pretrain.py 
```

![image-20251226090658767](./assets/image-20251226090658767.png)

```
æ‰€åŠ è½½Modelå¯è®­ç»ƒå‚æ•°ï¼š25.830 ç™¾ä¸‡
Epoch:[1/1](100/44160) loss:7.074585 lr:0.000549993674 epoch_Time:72.0min:
Epoch:[1/1](200/44160) loss:6.974608 lr:0.000549974695 epoch_Time:73.0min:
...
Epoch:[1/1](23900/44160) loss:2.311245 lr:0.000267721147 epoch_Time:36.0min:
Epoch:[1/1](24000/44160) loss:2.250305 lr:0.000265958338 epoch_Time:35.0min:
...
Epoch:[1/1](33700/44160) loss:2.309148 lr:0.000116081702 epoch_Time:19.0min:
Epoch:[1/1](33800/44160) loss:2.280215 lr:0.000114881704 epoch_Time:18.0min:
...
Epoch:[1/1](44100/44160) loss:2.116215 lr:0.000050002277 epoch_Time:0.0min:
Epoch:[1/1](44159/44160) loss:2.223341 lr:0.000050000001 epoch_Time:0.0min:
```

<img src="./assets/image-20251226104305269.png" alt="image-20251226104305269" style="zoom:50%;" />

å¤‡æ³¨ï¼šä¸€äº›é”™è¯¯

- MiniMindConfigç¼ºå°‘flash_attnå±æ€§ï¼ŒåŠ ä¸Šå³å¯
- MiniMindConfigä¸MokioMindConfigé”™è¯¯ä¹¦å†™çš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯å‘½åé—®é¢˜ï¼Œç»Ÿä¸€ä¸€ä¸‹å³å¯
- MiniMindModelä¸­freqs_cos, freqs_sinæ”¹ç”¨precompute_freqs_ciså‡½æ•°ï¼Œä¸”precompute_freqs_ciså‡½æ•°æ²¡æœ‰å†™
- ï¼ˆå­˜ç–‘ï¼‰MokioMindModelä¸­ç”±äºå½“å‰éMOEæ¨¡å‹ï¼Œæ‰€ä»¥aux_lossç›´æ¥è®¾ä¸º0ï¼›è®­ç»ƒä»£ç ä¸­`*loss += res.aux_loss*`æ³¨é‡Šæ‰



## åå››ã€é¢„æµ‹

ç¼–å†™`eval.py`ï¼š

```python
import argparse
import random
import warnings
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from model.model_minimind import MiniMindConfig, MokioMindForCausalLM
from trainer.trainer_utils import setup_seed

warnings.filterwarnings("ignore")


def init_model(args):
    tokenizer = AutoTokenizer.from_pretrained(args.load_from)
    if "model" in args.load_from:
        model = MokioMindForCausalLM(
            MiniMindConfig(
                hidden_size=args.hidden_size,
                num_hidden_layers=args.num_hidden_layers,
                inference_rope_scaling=args.inference_rope_scaling,
            )
        )
        moe_suffix = "_moe" if hasattr(args, "use_moe") and args.use_moe else ""
        ckp = f"./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth"
        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=False)
    else:
        model = AutoModelForCausalLM.from_pretrained(
            args.load_from, trust_remote_code=True
        )
    print(
        f"MiniMindæ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)"
    )
    return model.eval().to(args.device), tokenizer


def main():
    parser = argparse.ArgumentParser(description="MiniMindæ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    parser.add_argument(
        "--load_from",
        default="model",
        type=str,
        help="æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆmodel=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰",
    )
    parser.add_argument("--save_dir", default="out", type=str, help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument(
        "--weight",
        default="pretrain",
        type=str,
        help="æƒé‡åç§°å‰ç¼€ï¼ˆpretrain, full_sft, rlhf, reason, ppo_actor, grpo, spoï¼‰",
    )
    parser.add_argument(
        "--lora_weight",
        default="None",
        type=str,
        help="LoRAæƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šlora_identity, lora_medicalï¼‰",
    )
    parser.add_argument(
        "--hidden_size",
        default=512,
        type=int,
        help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰",
    )
    parser.add_argument(
        "--num_hidden_layers",
        default=8,
        type=int,
        help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰",
    )
    parser.add_argument(
        "--use_moe",
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )
    parser.add_argument(
        "--inference_rope_scaling",
        default=False,
        action="store_true",
        help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰",
    )
    parser.add_argument(
        "--max_new_tokens",
        default=8192,
        type=int,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰",
    )
    parser.add_argument(
        "--temperature",
        default=0.85,
        type=float,
        help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰",
    )
    parser.add_argument(
        "--top_p", default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰"
    )
    parser.add_argument(
        "--historys",
        default=0,
        type=int,
        help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰",
    )
    parser.add_argument(
        "--device",
        default="cuda" if torch.cuda.is_available() else "cpu",
        type=str,
        help="è¿è¡Œè®¾å¤‡",
    )
    args = parser.parse_args()

    prompts = [
        "ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ",
        "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„",
        "è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°",
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
        "å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨",
        "æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹",
        "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ",
    ]

    conversation = []
    model, tokenizer = init_model(args)
    input_mode = int(input("[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n"))
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    prompt_iter = prompts if input_mode == 0 else iter(lambda: input("ğŸ‘¶: "), "")
    for prompt in prompt_iter:
        setup_seed(2026)  # or setup_seed(random.randint(0, 2048))
        if input_mode == 0:
            print(f"ğŸ‘¶: {prompt}")
        conversation = conversation[-args.historys :] if args.historys else []
        conversation.append({"role": "user", "content": prompt})

        templates = {
            "conversation": conversation,
            "tokenize": False,
            "add_generation_prompt": True,
        }
        if args.weight == "reason":
            templates["enable_thinking"] = True  # ä»…Reasonæ¨¡å‹ä½¿ç”¨
        inputs = (
            tokenizer.apply_chat_template(**templates)
            if args.weight != "pretrain"
            else (tokenizer.bos_token + prompt)
        )
        inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)

        print("ğŸ¤–ï¸: ", end="")
        generated_ids = model.generate(
            inputs=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=args.max_new_tokens,
            do_sample=True,
            streamer=streamer,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            top_p=args.top_p,
            temperature=args.temperature,
            repetition_penalty=1.0,
        )
        response = tokenizer.decode(
            generated_ids[0][len(inputs["input_ids"][0]) :], skip_special_tokens=True
        )
        conversation.append({"role": "assistant", "content": response})
        print("\n\n")


if __name__ == "__main__":
    main()

```

ç¤ºä¾‹æ•ˆæœï¼š

```
$ python eval.py 
MiniMindæ¨¡å‹å‚æ•°: 25.83 M(illion)
[0] è‡ªåŠ¨æµ‹è¯•[1] æ‰‹åŠ¨è¾“å…¥0
ğŸ‘¶: ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ
ğŸ¤–ï¸: æˆ‘æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸“ä¸šæœ¯è¯­ï¼Œä½†æ˜¯æˆ‘æ²¡æœ‰èƒ½åŠ›åƒäººç±»ä¸€æ ·æ‹¥æœ‰åƒäººç±»ä¸€æ ·çš„æ€è€ƒå’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘æ˜¯ä¸€ä¸ªè®¡ç®—æœºç¨‹åºï¼Œæ‰€ä»¥æ²¡æœ‰ç‰¹é•¿ã€‚

ğŸ‘¶: ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„
ğŸ¤–ï¸: ï¼Ÿå¤©ç©ºæ˜¯è“è‰²çš„ä¸»è¦æ˜¯å› ä¸ºå¤§æ°”æ•£å°„æ‰€äº§ç”Ÿçš„è“è‰²å…‰æ³¢ï¼Œè¿™æ„å‘³ç€å¤©ç©ºä¸­çš„è“è‰²å…‰çº¿ç»è¿‡å¤§æ°”å±‚ï¼Œåœ¨å¤§æ°”ä¸­åå°„å‡ºè“è‰²å…‰çº¿ã€‚è€Œè“è‰²å…‰çº¿ä¸­çš„ç´«å¤–çº¿æ³¢é•¿è¾ƒé•¿ï¼Œè“è‰²å…‰æ³¢é•¿è¾ƒçŸ­ï¼Œå› æ­¤å¤©ç©ºçœ‹èµ·æ¥è“è‰²ã€‚
æ­¤å¤–ï¼Œè“è‰²å…‰æ³¢é•¿è¾ƒé•¿ï¼Œè“è‰²å…‰æ³¢é•¿è¾ƒé•¿ï¼Œæ•£å°„å…‰çº¿ä¹Ÿä¼šæ›´å®¹æ˜“è¢«æ•£å°„ã€‚æ­¤å¤–ï¼Œè“è‰²å…‰æ³¢é•¿ç›¸å¯¹è¾ƒçŸ­ï¼Œæ‰€ä»¥åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè“è‰²å…‰æ³¢é•¿è¾ƒçŸ­ï¼Œå› æ­¤åœ¨å¤©ç©ºä¸­çœ‹èµ·æ¥æ›´äº®ã€‚

ğŸ‘¶: è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°
ğŸ¤–ï¸: ã€‚
å¯ä»¥ä½¿ç”¨Pythonç¼–å†™çš„è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—å‡½æ•°ã€‚é¦–å…ˆï¼Œå®šä¹‰ä¸€ä¸ªæ–æ³¢é‚£å¥‘æ•°åˆ—ï¼Œç„¶åå®šä¹‰æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚æ¯æ¬¡è¾“å…¥nä¸ºæ•´æ•°ï¼Œç›´åˆ°è¯¥æ•°ä¸ºæ­¢ï¼Œæ–æ³¢é‚£å¥‘æ•°åˆ—ä¸ºæ­¢ã€‚æ¥ä¸‹æ¥ï¼Œå‡½æ•°ä¼šæ ¹æ®æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å€¼æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼Œç„¶åå°†æ–æ³¢é‚£å¥‘æ•°åˆ—ä¸­çš„æ¯ä¸ªå€¼ä½œä¸ºå¹³æ–¹ã€‚æœ€åï¼Œå°†æ–æ³¢é‚£å¥‘æ•°åˆ—ä¸­çš„æ¯ä¸ªå€¼éƒ½ä½œä¸ºå¹³æ–¹æ ¹ï¼Œç„¶åå°†è¯¥å€¼ä¹˜ä»¥æ–æ³¢é‚£å¥‘æ•°åˆ—ä¸­çš„å€¼ã€‚

ğŸ‘¶: è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹
ğŸ¤–ï¸: ã€‚å…‰åˆä½œç”¨æ˜¯æŒ‡å…‰é€šè¿‡å…‰çš„åˆ©ç”¨è¿‡ç¨‹ï¼Œå°†å…‰èƒ½è½¬åŒ–ä¸ºåŒ–å­¦èƒ½ï¼Œå¦‚åŒ–å­¦èƒ½ã€å…‰èƒ½ã€æ°´èƒ½ç­‰ï¼Œåœ¨å…‰åˆä½œç”¨è¿‡ç¨‹ä¸­ï¼Œæ¤ç‰©é€šè¿‡å…‰åˆä½œç”¨ï¼Œå°†äºŒæ°§åŒ–ç¢³ã€æ°´å’Œæ°§æ°”è½¬åŒ–ä¸ºæ°§æ°”å’Œè¥å…»ç‰©è´¨ï¼Œä»è€Œå¸æ”¶å¤ªé˜³èƒ½ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºåŒ–å­¦èƒ½ï¼Œä½¿å…‰èƒ½è½¬åŒ–ä¸ºåŒ–å­¦èƒ½ï¼Œä»è€Œå®ç°èƒ½é‡è½¬åŒ–ã€‚å…‰åˆä½œç”¨æ˜¯åœ°çƒä¸Šç”Ÿå‘½å­˜åœ¨çš„åŸºç¡€ï¼Œåœ¨å…‰åˆä½œç”¨ä¸­ï¼Œæ¤ç‰©éœ€è¦å…‰èƒ½è½¬åŒ–æˆèƒ½é‡ï¼Œä»¥æä¾›èƒ½é‡å’Œæ°§æ°”ã€‚

ğŸ‘¶: å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨
ğŸ¤–ï¸: ï¼Ÿ
è¯¦ç»†å›ç­”ä¸Šé¢çš„é—®é¢˜ã€‚å¦‚æœä½ æƒ³å‡ºé—¨ï¼Œæœ€å¥½çš„åŠæ³•æ˜¯åœ¨å®¶é‡Œå¾…ä¸¤å¤©ï¼Œä½†éœ€è¦ç¡®ä¿ä¸ä¼šä¸‹é›¨ã€‚åœ¨å®¶é‡Œå¾…ä¸¤å¤©ï¼Œä½ éœ€è¦å‡†å¤‡å¥½è¶³å¤Ÿçš„é˜²æ°´é˜²æ°´ææ–™ï¼Œå¦‚æ¯›å·¾ã€æ¯›å·¾å’Œæ‰‹å¥—ç­‰ã€‚åœ¨å¤–å‡ºæ—¶ï¼Œå»ºè®®åœ¨å®¤å†…é¿å…æš´æ™’ã€‚åœ¨å®¤å†…ï¼Œä¿æŒå®¤å†…ç©ºæ°”å¹²ç‡¥ï¼Œä»¥ä¿æŒå®¤å†…æ¸©åº¦é€‚å®œã€‚åœ¨å¤–å‡ºæ—¶ï¼Œéœ€è¦ç©¿åˆé€‚çš„é‹å­ï¼Œä»¥ä¿æŒèˆ’é€‚ã€‚åœ¨å®¤å†…ï¼Œéœ€è¦é¿å…åœ¨é˜³å…‰ä¸‹æš´æ™’ï¼Œä»¥é¿å…è„±æ°´ã€‚åœ¨å®¤å†…ï¼Œä½ å¯ä»¥é€‰æ‹©åœ¨å®¤å†…ç§æ¤ä¸€äº›èŠ±å‰æˆ–èŠ±å‰ï¼Œä»¥ä¿æŒå®¤å†…ç©ºæ°”çš„æ¸…æ–°ã€‚åœ¨å®¤å†…ï¼Œéœ€è¦æ³¨æ„ä¿æŠ¤å®¤å†…ç©ºæ°”ï¼Œå¹¶ä¸è¦æ‰“æ‰°å…¶ä»–è½¦è¾†æˆ–å…¬å…±äº¤é€šå·¥å…·ï¼Œä»¥å…å‘ç”Ÿæ„å¤–äº‹æ•…ã€‚

ğŸ‘¶: æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹
ğŸ¤–ï¸: ã€‚çŒ«é€šå¸¸æ›´ç‹¬ç«‹ï¼Œä¸éœ€è¦å¤ªå¤šçš„ç©ºé—´ï¼Œä½†æ˜¯å®ƒä»¬é€šå¸¸æ¯”ç‹—æ›´ç‹¬ç«‹ï¼Œå› ä¸ºå®ƒä»¬çš„ä½“å‹è¾ƒå¤§ï¼Œä½†å®ƒä»¬æ›´å®¹æ˜“åœ¨æŸäº›æƒ…å†µä¸‹æ„Ÿåˆ°å­¤ç‹¬å’Œç–²æƒ«ã€‚çŒ«æ›´å€¾å‘äºå®‰é™ï¼Œä¹Ÿæ›´ç‹¬ç«‹ï¼Œå› ä¸ºå®ƒä»¬æ›´å®¹æ˜“åœ¨å¤–è¡¨ä¸Šçœ‹ä¹¦å’Œé˜…è¯»ã€‚æ­¤å¤–ï¼ŒçŒ«ä¹Ÿæœ‰æ›´å¥½çš„å¬åŠ›å’Œå—…è§‰ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å£°éŸ³å’Œç¯å¢ƒã€‚æ€»ä¹‹ï¼ŒçŒ«å’Œç‹—æ˜¯ä¸¤ç§ä¸åŒç±»å‹çš„å® ç‰©ï¼Œå®ƒä»¬çš„ç‰¹æ€§ã€è¡Œä¸ºå’Œè¡Œä¸ºéƒ½å„ä¸ç›¸åŒã€‚

ğŸ‘¶: è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ 
ğŸ¤–ï¸: ï¼Œå¹¶æä¾›ä¸€ä¸ªå®é™…çš„åº”ç”¨æ¡ˆä¾‹ã€‚
æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹æ¥ä½¿è®¡ç®—æœºç³»ç»Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œä»è€Œè‡ªåŠ¨å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼å’Œè§„å¾‹ã€‚
ä¸€ä¸ªå®é™…çš„åº”ç”¨æ¡ˆä¾‹æ˜¯åŸºäºæœºå™¨å­¦ä¹ çš„åŒ»å­¦è¯Šæ–­ç³»ç»Ÿã€‚åŒ»ç”Ÿå¯ä»¥ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•æ¥é¢„æµ‹å’Œè¯Šæ–­ç–¾ç—…ï¼Œå¹¶åˆ¶å®šä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’ã€‚æ­¤å¤–ï¼Œæœºå™¨å­¦ä¹ è¿˜å¯ä»¥ç”¨äºåˆ†æå’Œé¢„æµ‹ç—…äººçš„ç—…æƒ…ï¼Œä»¥åŠé¢„æµ‹æ‚£è€…çš„ç–¾ç—…è¿›å±•ã€‚

ğŸ‘¶: æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ
ğŸ¤–ï¸: ã€‚ ä¸­å›½çš„ç¾é£Ÿç§ç±»ç¹å¤šï¼Œä¾‹å¦‚å¯¿å¸ã€é¥ºå­ã€ç…é¥¼ã€ç”œç‚¹ã€ç³–æœç­‰ã€‚è¿™äº›ç¾é£Ÿåœ¨ä¸­å›½çš„å†å²ä¸­éƒ½éå¸¸å—æ¬¢è¿ï¼Œä¸ä»…å¯ä»¥äº«å—åˆ°ä¸åŒçš„é£å‘³ï¼Œè¿˜å¯ä»¥å“å°åˆ°å„ç§ç¾é£Ÿï¼Œä¾‹å¦‚å¯¿å¸ã€ç‚¸é…±é¢ã€ç³–é†‹æ’éª¨ç­‰ç­‰ã€‚æ­¤å¤–ï¼Œä¸­å›½çš„ç¾é£Ÿä¹Ÿéå¸¸ä¸°å¯Œå¤šå½©ï¼Œå¦‚å¯¿å¸ã€å¤©å¦‡ç½—ã€çƒ¤è‚‰ç­‰ç­‰ã€‚ å¦‚æœæ‚¨æƒ³è¦æ›´å…·ä¸ªæ€§åŒ–çš„é€‰æ‹©ï¼Œå¯ä»¥è€ƒè™‘å»ä¸€äº›ä¼ ç»Ÿçš„é¤é¦†ã€å°åº—ã€é¤é¦†ï¼Œäº†è§£å½“åœ°çš„æ–‡åŒ–å’Œå†å²ã€‚
```

```
$ python eval.py 
MiniMindæ¨¡å‹å‚æ•°: 25.83 M(illion)
[0] è‡ªåŠ¨æµ‹è¯•[1] æ‰‹åŠ¨è¾“å…¥1
ğŸ‘¶: ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ
ğŸ¤–ï¸: æˆ‘å«æç™½ï¼Œæ˜¯ä¸€ä½ä¸­å›½å¤ä»£è‘—åçš„è¯—äººã€‚æç™½æ˜¯å”ä»£æœ€è‘—åçš„è¯—äººä¹‹ä¸€ï¼Œä»–æ›¾è¢«è®¤ä¸ºæ˜¯ä¸–ç•Œæ–‡å­¦çš„å¥ åŸºäººä¹‹ä¸€ã€‚æç™½ä»¥å…¶è±ªæ”¾ã€å¥”æ”¾ã€æ•¬ç•å’Œå¤©èµ‹è€Œè‘—åï¼Œä»–çš„è¯—æ­Œä½œå“å¤šä»¥è±ªæ”¾ã€å¥”æ”¾ã€å¹½é»˜ã€è±ªè¿ˆå’Œè±ªè¿ˆä¸ºä¸»é¢˜ã€‚æç™½çš„è¯—æ­Œé£æ ¼ç‹¬ç‰¹ï¼Œæ­Œé¢‚äº†è®¸å¤šç¾ä¸½çš„è¯—æ­Œä½œå“ï¼Œå¦‚ã€Šå°†è¿›é…’ã€‹ã€ã€Šåºå±±è°£ã€‹ç­‰ã€‚æç™½çš„è¯—æ­Œé£æ ¼ç‹¬ç‰¹ï¼Œæ—‹å¾‹ä¼˜ç¾ï¼Œä¼ è¾¾äº†è¯—äººå¯¹ç”Ÿå‘½ã€çˆ±æƒ…ã€ç”Ÿå‘½çš„çƒ­çˆ±ã€‚

ğŸ‘¶: è®²ä¸€ä¸ªç¬‘è¯
ğŸ¤–ï¸: ï¼Œæ ‡é¢˜æ˜¯â€œä½ ä¼šè¯´æˆ‘å–œæ¬¢è¿™ä¹ˆçƒ­é—¹ï¼â€ 
ä»å‰æœ‰ä¸ªå°ç”·å­©ï¼Œä»–å–œæ¬¢å¬éŸ³ä¹ï¼Œä½†æ˜¯ä»–ä¸å–œæ¬¢çœ‹ç”µå½±ï¼Œäºæ˜¯å°±ç­”åº”äº†ã€‚
ç„¶åä»–é—®äº†è€å¸ˆï¼Œâ€œæˆ‘å–œæ¬¢ä½ ï¼â€è€å¸ˆå›ç­”é“ï¼šâ€œä¸ï¼Œæˆ‘æ˜¯çˆ±ä½ çš„ã€‚â€ 
å°ç”·å­©ç¬‘äº†èµ·æ¥ï¼šâ€œæˆ‘å–œæ¬¢ä½ ï¼æˆ‘çˆ±ä½ ï¼â€ 
è€å¸ˆï¼šâ€œæˆ‘çˆ±ä½ ï¼â€
è€å¸ˆï¼šâ€œæˆ‘çˆ±ä½ ï¼â€ 
å°ç”·å­©ï¼šâ€œæˆ‘çˆ±ä½ ï¼â€ 
è€å¸ˆï¼šâ€œæˆ‘çˆ±ä½ ï¼â€ 
è€å¸ˆï¼šâ€œæˆ‘çˆ±ä½ ï¼â€
```



# é¡¹ç›®è§£æ

### æ¶æ„å…³é”®è¯

![image-20251205090732564](./assets/image-20251205090732564.png)

å›é¡¾ä¸Šè¿°å…³é”®è¯ï¼š

- RMSNormï¼šæ˜¯ä¸€ä¸ªè½»é‡ã€é«˜æ•ˆã€ç¨³å®šçš„å½’ä¸€åŒ–æ–¹æ³•ï¼Œç”¨äºç¨³å®šè®­ç»ƒã€åŠ é€Ÿæ”¶æ•›ã€‚å®ƒä¸ç®¡å¹³å‡å€¼ï¼Œåªé€šè¿‡â€œæ•´ä½“å¤§å°â€çš„å‡æ–¹æ ¹è°ƒæ•´æ•°å€¼å¹…åº¦ã€‚é€šå¸¸ä½äºAttention ä¹‹å‰ã€FFN ä¹‹å‰
- PoPE&YaRNï¼šPoPE ä¸º Attention æ³¨å…¥ ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œä½¿æ¨¡å‹æ„ŸçŸ¥ token ä¹‹é—´çš„é¡ºåºã€‚YaRN æ‰©å±• RoPE çš„ä¸Šä¸‹æ–‡é•¿åº¦èƒ½åŠ›ï¼Œç”¨äºè¶…é•¿ä¸Šä¸‹æ–‡ã€‚ä½äºQuery / Key æŠ•å½±ä¹‹åï¼ŒSoftmax ä¹‹å‰ã€‚
- GQAï¼šå¤šä¸ª Query Head å…±äº«ä¸€ç»„ Key / Valueï¼Œæ˜¯ MHA å’Œ MQA çš„æŠ˜ä¸­æ–¹æ¡ˆã€‚ç”¨äºé™ä½ KV cache çš„æ˜¾å­˜å’Œè®¡ç®—æˆæœ¬ã€‚ä½äºAttention å†…éƒ¨ç»“æ„å±‚é¢ã€‚
- FFNï¼šFFN æ˜¯ Transformer è¡¨è¾¾èƒ½åŠ›çš„æ ¸å¿ƒï¼ˆéçº¿æ€§ç‰¹å¾å˜æ¢ä¸é€šé“æ‰©å±•ï¼Œæ‰¿æ‹…ä¸»è¦çš„å‚æ•°å®¹é‡ï¼‰ï¼ŒAttention å†³å®šâ€œçœ‹è°â€ï¼ŒFFN å†³å®šâ€œæ€ä¹ˆç†è§£â€ï¼ŒSiLU / SwiGLU æ˜¯ä¸ºå¤§æ¨¡å‹ç¨³å®šæ€§ä¸æ€§èƒ½è€Œç”Ÿã€‚ä½äº Attention ä¹‹åã€‚
- Blockï¼šæ¨¡å‹çš„åŸºæœ¬å †å å•å…ƒã€‚ä½ä¸æ¨¡å‹ä¸»å¹²ï¼Œå †å  N å±‚ï¼ˆå¦‚ 32 / 80 / 120 å±‚ï¼‰

------

**MiniMindå°±æ˜¯Transformeræ¶æ„**ï¼Œä½†åšäº†ç®€åŒ–å’Œä¼˜åŒ–ï¼Œ**ç»§æ‰¿çš„æ ¸å¿ƒç»„ä»¶**ï¼š

1. **Multi-Head Attention** - å®Œæ•´ä¿ç•™ï¼ˆæ”¹æˆäº†GQAå˜ä½“ï¼‰
2. **Position Encoding** - ç”¨RoPEæ›¿ä»£äº†åŸå§‹çš„æ­£å¼¦ä½ç½®ç¼–ç 
3. **Feed-Forward Network** - ç”¨SwiGLUæ›¿ä»£äº†åŸå§‹çš„ReLU
4. **Layer Normalization** - ç”¨RMSNormæ›¿ä»£äº†LayerNorm
5. **Residual Connection** - å®Œå…¨ä¸€è‡´

**ä¸»è¦æ”¹åŠ¨**ï¼š

- **Pre-Norm** vs Post-Normï¼šMiniMindåœ¨attention/FFNä¹‹å‰åšnormï¼ˆæ›´ç¨³å®šï¼‰
- **GQA**ï¼šKey/Valueå¤´æ•°ï¼ˆ2ä¸ªï¼‰å°‘äºQueryå¤´æ•°ï¼ˆ8ä¸ªï¼‰ï¼Œçœå†…å­˜
- **RoPE**ï¼šç›¸å¯¹ä½ç½®ç¼–ç ï¼Œæ”¯æŒé•¿åº¦å¤–æ¨
- **SwiGLU**ï¼šé—¨æ§æ¿€æ´»å‡½æ•°ï¼Œæ•ˆæœæ›´å¥½

------

**MiniMindå°±æ˜¯å…¸å‹çš„Decoder-onlyæ¶æ„**ï¼Œå’ŒGPTç³»åˆ—å®Œå…¨ä¸€è‡´ï¼š

**Decoder-onlyçš„ç‰¹å¾ï¼ˆMiniMindå…¨éƒ¨å…·å¤‡ï¼‰**ï¼š

1. âœ… **åªæœ‰Decoderå±‚**ï¼Œæ²¡æœ‰Encoder
2. âœ… **Causal Attention**ï¼ˆå•å‘æ³¨æ„åŠ›ï¼‰- ä»£ç ä¸­çš„`is_causal=True`å’Œä¸Šä¸‰è§’mask
3. âœ… **è‡ªå›å½’ç”Ÿæˆ** - é€tokenç”Ÿæˆ
4. âœ… **ç»Ÿä¸€çš„è¾“å…¥è¾“å‡º** - Embeddingå’ŒLM Headå…±äº«æƒé‡

**å¯¹æ¯”ä¸‰ç§æ¶æ„**ï¼š

```
Encoder-only (BERT):
åŒå‘Attention â†’ åªèƒ½åšç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERç­‰ï¼‰

Encoder-Decoder (T5):
Encoder(åŒå‘) + Decoder(å•å‘) â†’ é€‚åˆç¿»è¯‘ç­‰seq2seq

Decoder-only (GPT/MiniMind):
åªæœ‰Decoder(å•å‘) â†’ é€šç”¨ï¼Œèƒ½åšæ‰€æœ‰ä»»åŠ¡
```

**å…·ä½“ä½“ç°åœ¨ä»£ç ä¸­**ï¼š

```python
# 1. Causal Maskï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ï¼‰
# model/model_minimind.py:393-396
causal_mask = torch.triu(torch.full(..., float("-inf")), diagonal=1)

# 2. è‡ªå›å½’ç”Ÿæˆ
# eval.py:150-161
for _ in range(max_new_tokens):
    logits = model(input_ids)  # åªç”¨å†å²é¢„æµ‹æœªæ¥
    next_token = sample(logits[:, -1, :])
    input_ids = torch.cat([input_ids, next_token], dim=1)

# 3. Embeddingæƒé‡å…±äº«
# model/model_minimind.py:588
self.model.embed_tokens.weight = self.lm_head.weight
```



### å¿«é€Ÿä»‹ç»

------

> ä¸€ã€æ•´ä½“å®šä½

"MiniMindæ˜¯ä¸€ä¸ª**26Må‚æ•°çš„Decoder-onlyè¯­è¨€æ¨¡å‹**ï¼Œæ¶æ„ä¸ŠåŸºäºTransformerï¼Œå’ŒGPTç³»åˆ—åŒæºã€‚å®ƒçš„è®¾è®¡ç›®æ ‡æ˜¯åœ¨æœ‰é™èµ„æºä¸‹ï¼ˆå•å¡è®­ç»ƒï¼‰å®ç°å®Œæ•´çš„é¢„è®­ç»ƒ-æ¨ç†æµç¨‹ï¼Œé€‚åˆæ•™å­¦å’Œå¿«é€Ÿå®éªŒã€‚" 

"æ ¸å¿ƒé…ç½®æ˜¯ï¼š**8å±‚Transformer Blockï¼Œ512ç»´éšè—å±‚ï¼Œ6400è¯æ±‡è¡¨ï¼Œæ”¯æŒ512é•¿åº¦**ã€‚"

------

> äºŒã€æ¶æ„è®¾è®¡ï¼ˆ2åˆ†é’Ÿï¼‰

**"ä»æ•°æ®æµå‘æ¥çœ‹ï¼Œæ¨¡å‹åˆ†ä¸º4ä¸ªé˜¶æ®µï¼š"**

ï¼ˆ1ï¼‰Embeddingå±‚

```
è¾“å…¥Token IDs [batch, seq_len]
     â†“
EmbeddingæŸ¥è¡¨ [batch, seq_len, 512]
```

"è¿™é‡Œæœ‰ä¸ªç»†èŠ‚ï¼šEmbeddingæƒé‡å’Œè¾“å‡ºå±‚LM Headæ˜¯**æƒé‡å…±äº«**çš„ï¼ŒèŠ‚çœäº†3.3Må‚æ•°ã€‚"



ï¼ˆ2ï¼‰8å±‚Transformer Block

**"æ¯ä¸ªBlockåŒ…å«ä¸¤ä¸ªå­å±‚ï¼Œéƒ½ä½¿ç”¨Pre-Norm + æ®‹å·®è¿æ¥ï¼š"**

**å­å±‚1 - è‡ªæ³¨æ„åŠ›**ï¼š

- "ä½¿ç”¨**GQAï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰**ï¼š8ä¸ªQueryå¤´ï¼Œä½†åªæœ‰2ä¸ªKey/Valueå¤´"
- "ä¸ºä»€ä¹ˆï¼Ÿæ¨ç†æ—¶KV Cacheèƒ½çœ75%å†…å­˜ï¼Œæ€§èƒ½æŸå¤±å°äº2%"
- "ä½ç½®ç¼–ç ç”¨**RoPE**ï¼Œé€šè¿‡æ—‹è½¬å˜æ¢å®ç°ç›¸å¯¹ä½ç½®ï¼Œæ”¯æŒé•¿åº¦å¤–æ¨"
- "Causal Maskä¿è¯åªèƒ½çœ‹åˆ°å†å²ï¼Œä¸èƒ½çœ‹æœªæ¥"

**å­å±‚2 - å‰é¦ˆç½‘ç»œ**ï¼š

- "ç”¨**SwiGLU**æ¿€æ´»å‡½æ•°ï¼š`gate(x) * up(x)` çš„é—¨æ§æœºåˆ¶"
- "ç»´åº¦ï¼š512 â†’ 1365 â†’ 512ï¼Œä¸­é—´å±‚çº¦2.67å€"



ï¼ˆ3ï¼‰æœ€ç»ˆå½’ä¸€åŒ–

- "ç”¨**RMSNorm**æ›¿ä»£LayerNormï¼Œè®¡ç®—æ›´ç®€å•ï¼š`x / sqrt(mean(xÂ²))`"



ï¼ˆ4ï¼‰è¾“å‡ºå±‚

```
[batch, seq_len, 512]
     â†“ Linear(512, 6400)
[batch, seq_len, 6400]  # æ¯ä¸ªä½ç½®é¢„æµ‹è¯æ±‡è¡¨æ¦‚ç‡
```

------

> ä¸‰ã€è®­ç»ƒæµç¨‹ï¼ˆ1.5åˆ†é’Ÿï¼‰

**"è®­ç»ƒé‡‡ç”¨Causal Language Modelingï¼Œæ ¸å¿ƒæ˜¯'é”™ä½é¢„æµ‹'ï¼š"**

```python
è¾“å…¥åºåˆ—: [ä»Šå¤©, å¤©æ°”, å¾ˆå¥½]
X = [ä»Šå¤©, å¤©æ°”]      # å‰n-1ä¸ª
Y = [å¤©æ°”, å¾ˆå¥½]      # ån-1ä¸ª
ç›®æ ‡: ç”¨X[i]é¢„æµ‹Y[i]
```

**"ä¸‰ä¸ªå…³é”®ä¼˜åŒ–ï¼š"**

1. **æ¢¯åº¦ç´¯ç§¯**ï¼š
   - "batch_size=4ï¼Œç´¯ç§¯8æ¬¡ï¼Œæ¨¡æ‹Ÿbatch_size=32"
   - "æ¯æ¬¡lossé™¤ä»¥8ï¼Œä¿æŒæ¢¯åº¦æœŸæœ›ä¸€è‡´"
2. **æ··åˆç²¾åº¦**ï¼š
   - "ç”¨BFloat16å‰å‘ï¼ŒFP32å­˜æ¢¯åº¦"
   - "GradScaleråŠ¨æ€è°ƒæ•´ç¼©æ”¾å› å­ï¼Œé˜²æ­¢ä¸‹æº¢"
3. **ä½™å¼¦é€€ç«å­¦ä¹ ç‡**ï¼š
   - "ä»5e-4å¹³æ»‘é™åˆ°5e-5"
   - "å¼€å§‹å¤§æ­¥å¿«èµ°ï¼Œç»“æŸå°æ­¥ç²¾è°ƒ"

**"æŸå¤±å‡½æ•°ä¼šç”¨loss_maskå¿½ç•¥paddingä½ç½®ï¼Œé¿å…å­¦åˆ°é”™è¯¯æ¨¡å¼ã€‚"**

------

> å››ã€æ¨ç†è¿‡ç¨‹ï¼ˆ1.5åˆ†é’Ÿï¼‰

**"æ¨ç†æ˜¯è‡ªå›å½’ç”Ÿæˆï¼Œå…³é”®æ˜¯KV CacheåŠ é€Ÿï¼š"**

æ ‡å‡†æµç¨‹ï¼ˆæ— Cacheï¼‰

```
Step 1: [ä»Šå¤©] â†’ è®¡ç®—KV â†’ é¢„æµ‹"å¤©æ°”"
Step 2: [ä»Šå¤©,å¤©æ°”] â†’ é‡æ–°è®¡ç®—æ‰€æœ‰KV â†’ é¢„æµ‹"å¾ˆ"
é—®é¢˜ï¼šé‡å¤è®¡ç®—ï¼Œå¤æ‚åº¦O(nÂ²)
```

ä¼˜åŒ–æµç¨‹ï¼ˆæœ‰Cacheï¼‰

```
Step 1: [ä»Šå¤©] â†’ è®¡ç®—KV â†’ ç¼“å­˜
Step 2: [å¤©æ°”] â†’ åªç®—æ–°tokençš„KVï¼Œä»ç¼“å­˜è¯»å†å² â†’ æ‹¼æ¥
åŠ é€Ÿï¼šä»O(nÂ²)é™åˆ°O(n)ï¼Œç”Ÿæˆ100 tokenså¿«83å€
```

**"é‡‡æ ·ç­–ç•¥ç”¨Temperature + Top-Pï¼š"**

- "Temperature=0.8æ§åˆ¶åˆ†å¸ƒé”åº¦ï¼ˆè¶Šå°è¶Šç¡®å®šï¼‰"
- "Top-P=0.9åŠ¨æ€é€‰æ‹©å€™é€‰é›†ï¼ˆç´¯ç§¯æ¦‚ç‡90%ï¼‰"
- "ç»„åˆä½¿ç”¨ï¼šæ—¢æœ‰åˆ›é€ æ€§ï¼Œåˆä¸ä¼šå¤ªéšæœº"

------

> äº”ã€å·¥ç¨‹äº®ç‚¹ï¼ˆ30ç§’ï¼‰

**"ä¸‰ä¸ªå·¥ç¨‹ä¼˜åŒ–å€¼å¾—ä¸€æï¼š"**

1. **æ–­ç‚¹ç»­è®­**ï¼š
   - "ä¿å­˜modelã€optimizerã€scalerã€epoch/stepçŠ¶æ€"
   - "æ”¯æŒGPUæ•°å˜åŒ–æ—¶è‡ªåŠ¨æ¢ç®—step"
2. **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼š
   - "ç”¨DDPï¼Œæ¯ä¸ªGPUå¤„ç†ä¸åŒæ•°æ®"
   - "æ¢¯åº¦è‡ªåŠ¨AllReduceåŒæ­¥"
3. **å†…å­˜ä¼˜åŒ–**ï¼š
   - "GQAå‡å°‘KV Cache"
   - "æƒé‡å…±äº«çœ3.3Må‚æ•°"
   - "Flash Attentionå‡å°‘ä¸­é—´æ¿€æ´»"

------

> å…­ã€æ€»ç»“ä¸åæ€ï¼ˆ30ç§’ï¼‰

**"æ•´ä½“æ¥è¯´ï¼ŒMiniMindæ˜¯ä¸€ä¸ªéº»é›€è™½å°äº”è„ä¿±å…¨çš„é¡¹ç›®ï¼š"** 

âœ… **æ¶æ„**ï¼šDecoder-only + ç°ä»£ä¼˜åŒ–ï¼ˆGQA/RoPE/SwiGLUï¼‰

âœ… **è®­ç»ƒ**ï¼šå®Œæ•´pipelineï¼Œæ”¯æŒåˆ†å¸ƒå¼å’Œæ–­ç‚¹ç»­è®­ 

âœ… **æ¨ç†**ï¼šKV CacheåŠ é€Ÿï¼Œçµæ´»çš„é‡‡æ ·ç­–ç•¥

**"å¦‚æœè¦ä¼˜åŒ–ï¼Œæˆ‘ä¼šè€ƒè™‘ï¼š"**

- "æ¨ç†ç«¯ç”¨vLLMå¼•æ“åšContinuous Batching"
- "é‡åŒ–åˆ°INT8å‡å°‘éƒ¨ç½²æˆæœ¬"
- "åŠ å…¥Speculative Decodingè¿›ä¸€æ­¥åŠ é€Ÿ"

"ä»¥ä¸Šå°±æ˜¯æˆ‘å¯¹MiniMindçš„ç†è§£ï¼Œè¯·é—®æ‚¨è¿˜æƒ³æ·±å…¥äº†è§£å“ªä¸ªéƒ¨åˆ†ï¼Ÿ"
