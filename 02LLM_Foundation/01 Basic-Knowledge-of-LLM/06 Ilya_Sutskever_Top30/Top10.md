## 2011.09 å¤æ‚åŠ¨åŠ›å­¦çš„ç¬¬ä¸€å®šå¾‹

åŸæ–‡åœ°å€ï¼š[The First Law of Complexodynamics](https://scottaaronson.blog/?p=762)

è§†é¢‘æ•™ç¨‹ï¼š[å¤æ‚ç†µ - å¤æ‚åŠ¨åŠ›å­¦ç¬¬ä¸€æ³•åˆ™ | æ··æ²Œä¸ç§©åºä¹‹é—´çš„æ–°ç‰©ç†å®šå¾‹](https://www.bilibili.com/video/BV1kUaszBEfm)

------

è¿™ç¯‡æ–‡ç« æ¢è®¨äº†ä¸€ä¸ªæœ‰è¶£çš„æ‚–è®ºï¼šåœ¨ä¸€ä¸ªå°é—­ç³»ç»Ÿä¸­ï¼Œç†µï¼ˆæ··ä¹±åº¦ï¼‰æ€»æ˜¯ä¸æ–­å¢åŠ ï¼Œè€Œç³»ç»Ÿçš„å¤æ‚æ€§å´ç»å†äº†ä¸€ä¸ªå…ˆå¢åŠ åå‡å°‘çš„è¿‡ç¨‹ã€‚ä¸è¿‡æ–‡ç« è¿˜æ˜¯çŒœæƒ³ã€‚

------

![image-20251120105538529](./assets/image-20251120105538529.png)

æƒ³è±¡ä½ é¢å‰æœ‰ä¸€æ¯åˆšå†²å¥½çš„**é»‘å’–å•¡**ï¼Œè¿˜æœ‰ä¸€æ¯**ç™½ç‰›å¥¶**ã€‚

1. **ç¬¬ä¸€é˜¶æ®µï¼ˆå¼€å§‹ï¼‰ï¼š** ä½ æŠŠç‰›å¥¶å€’åœ¨å’–å•¡ä¸Šé¢ï¼Œä½†è¿˜æ²¡æ…æ‹Œã€‚è¿™æ—¶å€™ï¼Œä¸Šé¢æ˜¯å…¨ç™½ï¼Œä¸‹é¢æ˜¯å…¨é»‘ã€‚
   - **æè¿°å®ƒå¾ˆç®€å•**ï¼šâ€œä¸Šé¢ç™½ï¼Œä¸‹é¢é»‘ã€‚â€
   - **çŠ¶æ€**ï¼šå¾ˆæ•´é½ï¼Œä½†ä¹ŸæŒºæ— èŠçš„ã€‚
2. **ç¬¬äºŒé˜¶æ®µï¼ˆä¸­é—´ï¼‰ï¼š** ä½ æ‹¿å‹ºå­æ…äº†ä¸¤ä¸‹ã€‚
   - **å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ** å’–å•¡å’Œç‰›å¥¶å¼€å§‹æ··åˆï¼Œä½ çœ‹åˆ°äº†æå…¶æ¼‚äº®çš„æ¼©æ¶¡ã€äº‘æœµä¸€æ ·çš„çº¹è·¯ï¼Œç”šè‡³åƒæ˜Ÿäº‘ä¸€æ ·çš„å›¾æ¡ˆã€‚
   - **æè¿°å®ƒå¾ˆéš¾**ï¼šä½ è¦ç”»å‡ºæ¯ä¸€æ¡æ›²çº¿ã€æ¯ä¸€ä¸ªæ¼©æ¶¡çš„å½¢çŠ¶ï¼Œè¿™å¤ªå¤æ‚äº†ï¼
   - **çŠ¶æ€**ï¼šè¿™å°±æ˜¯**â€œå¤æ‚â€**ã€‚å®ƒæ—¢ä¸æ˜¯æ•´é½çš„ï¼ˆä¸åƒå¼€å§‹ï¼‰ï¼Œä¹Ÿä¸æ˜¯ä¹±ç³Ÿç³Ÿçš„ï¼ˆä¸åƒæœ€åï¼‰ï¼Œå®ƒæœ‰**ç»“æ„**ï¼Œæ‰€ä»¥æœ€â€œæœ‰è¶£â€ã€‚
3. **ç¬¬ä¸‰é˜¶æ®µï¼ˆæœ€åï¼‰ï¼š** ä½ æ…äº†ä¸€åˆ†é’Ÿï¼Œå®Œå…¨æ…åŒ€äº†ã€‚
   - **å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ** æ¯å­é‡Œå˜æˆäº†ä¸€æ¯å‡åŒ€çš„æµ…è¤è‰²æ¶²ä½“ã€‚
   - **æè¿°å®ƒå¾ˆç®€å•**ï¼šâ€œè¿™å°±æ˜¯ä¸€æ¯è¤è‰²çš„æ¶²ä½“ã€‚â€
   - **çŠ¶æ€**ï¼šè™½ç„¶å®ƒæ˜¯æœ€â€œä¹±â€çš„ï¼ˆæ¯ä¸€æ»´å¥¶éƒ½éšæœºè·‘åˆ°äº†å’–å•¡é‡Œï¼‰ï¼Œä½†å› ä¸ºå®ƒå“ªå„¿éƒ½ä¸€æ ·ï¼Œæ²¡æœ‰ä»»ä½•å›¾æ¡ˆï¼Œæ‰€ä»¥åˆå˜å¾—æ— èŠäº†ã€‚

------

è§†é¢‘å°†é€šè¿‡ä¸€ä¸ªç®€å•çš„â€œç‰›å¥¶ä¸å’–å•¡æ··åˆâ€çš„ä¾‹å­ï¼Œå¸¦ä½ äº†è§£ç§‘å­¦å®¶ä»¬ä¸ºäº†ç²¾ç¡®å®šä¹‰â€œå¤æ‚æ€§â€æ‰€åšçš„å„ç§å°è¯•ï¼Œä»ç»å…¸çš„çƒ­åŠ›å­¦ç†µã€æŸ¯æ°å¤æ‚åº¦ï¼ˆKolmogorov Complexityï¼‰ï¼Œå†åˆ°â€œè€ç»ƒåº¦â€ï¼ˆSophisticationï¼‰ï¼Œå¹¶æœ€ç»ˆå¼•å‡ºä¸€ä¸ªå¯èƒ½å­˜åœ¨çš„ã€å…¨æ–°çš„ç‰©ç†å®šå¾‹ã€‚ 

æœ€å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬ä¼šå°†è¿™ä¸ªæ·±åˆ»çš„æ¦‚å¿µä¸äººå·¥æ™ºèƒ½è”ç³»èµ·æ¥ã€‚ä½ å°†ç†è§£ä¸ºä»€ä¹ˆIlya Sutskeverå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§†ä¸ºç»ˆæçš„ä¿¡æ¯å‹ç¼©å™¨ï¼Œä»¥åŠâ€œå¤æ‚åŠ¨åŠ›å­¦ç¬¬ä¸€æ³•åˆ™â€å¦‚ä½•ä¸ºæˆ‘ä»¬ç†è§£æ™ºèƒ½çš„æœ¬è´¨æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„ç†è®ºæ¡†æ¶ã€‚

------

Ilya Sutskever å°†è¿™ä¸ªå¸¦å…¥LLMï¼ŒLLMå¯ä»¥è§†ä½œå¤æ‚ç†µçš„å·…å³°ã€‚



## 2015.05 å¾ªç¯ç¥ç»ç½‘ç»œçš„ä¸å¯æ€è®®çš„æœ‰æ•ˆæ€§

åŸæ–‡åœ°å€ï¼šhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/

è§†é¢‘æ•™ç¨‹ï¼šhttps://www.bilibili.com/video/BV11nHUzUEyS

------

è¿™ç¯‡æ–‡ç« å……æ»¡äº†é­”æ³•ï¼šä¸€ä¸ªç®€å•çš„RNNæ¨¡å‹ï¼Œä»…ä»…é€šè¿‡é€ä¸ªå­—ç¬¦åœ°å­¦ä¹ ï¼Œå°±èƒ½æ¨¡ä»¿èå£«æ¯”äºšçš„æˆå‰§ã€ç¼–å†™çœ‹ä¼¼åˆç†çš„Linuxä»£ç ï¼Œç”šè‡³ç”Ÿæˆç»´åŸºç™¾ç§‘æ¡ç›®å’Œæ•°å­¦å…¬å¼ã€‚æˆ‘ä»¬å°†è¯¦ç»†è§£æRNNçš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œå¹¶é€šè¿‡å…­ä¸ªç”ŸåŠ¨æœ‰è¶£çš„å®ä¾‹ï¼Œäº²çœ¼è§è¯å®ƒæ˜¯å¦‚ä½•ä»é›¶å¼€å§‹æŒæ¡è¯­è¨€ã€é£æ ¼å’Œå¤æ‚ç»“æ„çš„ã€‚

------

RNNçš„æ¦‚å¿µï¼š

![image-20251125203302242](./assets/image-20251125203302242.png)

![image-20251125205211395](./assets/image-20251125205211395.png)

------

é€šè¿‡å…­ä¸ªç”ŸåŠ¨æœ‰è¶£çš„å®ä¾‹ï¼Œäº²çœ¼è§è¯å®ƒæ˜¯å¦‚ä½•ä»é›¶å¼€å§‹æŒæ¡è¯­è¨€ã€é£æ ¼å’Œå¤æ‚ç»“æ„çš„ï¼š

![image-20251125205359481](./assets/image-20251125205359481.png)

------

æœªæ¥å±•æœ›ï¼šRNNï¼LSTM å½“å‰çš„æŒ‘æˆ˜ä¸ç ”ç©¶æ–¹å‘

- è™½ç„¶ RNN å¾ˆå¼ºï¼Œä½†å®ƒä»¬ä¸»è¦æ˜¯â€œè®°å¿†â€å‹ï¼Œå¯¹çœŸæ­£çš„å½’çº³ã€æ¨ç†ã€æŠ½è±¡èƒ½åŠ›è¿˜ä¸å¤Ÿã€‚
- ä¼ ç»Ÿ RNN çš„çŠ¶æ€å¤§å°ä¸æ¯æ­¥è®¡ç®—æˆæœ¬è€¦åˆï¼šéšè—çŠ¶æ€è¶Šå¤§ã€æ¯æ­¥è¿ç®—è¶Šå¤šã€‚ç†æƒ³ä¸­å¸Œæœ›æœ‰å¤§â€œè®°å¿†â€ä½†æ¯æ¬¡æ›´æ–°è®¡ç®—ä»ç»´æŒä½æˆæœ¬ã€‚
- æåˆ°äº† Neural Turing Machine (NTM)ã€æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattentionï¼‰ç­‰æ–¹å‘ã€‚
- åœ¨è§†è§‰ã€è¯­éŸ³ã€è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ç­‰é¢†åŸŸï¼ŒRNN/LSTM æ­£è¢«è¶Šæ¥è¶Šå¹¿æ³›ä½¿ç”¨ã€‚

![image-20251125205619680](./assets/image-20251125205619680.png)

------

ä¸‹é¢å°±æ˜¯ä¸€ä¸ª**å®Œæ•´å¯è¿è¡Œçš„æœ€ç®€å­—ç¬¦çº§ RNN**ï¼ˆåªç”¨ NumPyï¼‰ï¼Œæ¼”ç¤ºäº†æ–‡ç« ä¸­æ‰€è¯´çš„ **â€œåºåˆ—å»ºæ¨¡â€â€œä»å­—ç¬¦å­¦ä¹ ç»“æ„â€â€œå‰å‘ã€åå‘ä¼ æ’­â€** ç­‰æœºåˆ¶ã€‚
 ä½ å¯ä»¥ç›´æ¥åŸºäºæ­¤ç»§ç»­æ‰©å±•ï¼Œä¾‹å¦‚ï¼š

- æ¢æˆæ›´é•¿çš„è®­ç»ƒæ–‡æœ¬
- ä¿®æ”¹éšè—å±‚å¤§å°
- å®ç°é‡‡æ ·ç”Ÿæˆæ–‡æœ¬
- æ”¹æˆ LSTM
- æ›´æ¥è¿‘ Karpathy æ–‡ä¸­â€œå­—ç¬¦çº§è¯­è¨€æ¨¡å‹â€çš„å®éªŒ

```python
import numpy as np

# Minimal character-level RNN demo (no external libraries)

text = "hello world\n"
chars = sorted(list(set(text)))
vocab_size = len(chars)
char_to_ix = {ch:i for i,ch in enumerate(chars)}
ix_to_char = {i:ch for ch,i in char_to_ix.items()}

# Hyperparameters
hidden_size = 32
seq_length = 5
learning_rate = 1e-1

# Model parameters
Wxh = np.random.randn(hidden_size, vocab_size)*0.01
Whh = np.random.randn(hidden_size, hidden_size)*0.01
Why = np.random.randn(vocab_size, hidden_size)*0.01
bh = np.zeros((hidden_size, 1))
by = np.zeros((vocab_size, 1))

def loss_and_gradients(inputs, targets, hprev):
    xs, hs, ys, ps = {}, {}, {}, {}
    hs[-1] = hprev
    loss = 0
    
    # Forward
    for t in range(len(inputs)):
        xs[t] = np.zeros((vocab_size,1))
        xs[t][inputs[t]] = 1
        hs[t] = np.tanh(Wxh @ xs[t] + Whh @ hs[t-1] + bh)
        ys[t] = Why @ hs[t] + by
        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))
        loss += -np.log(ps[t][targets[t],0])
    
    # Backward
    dWxh = np.zeros_like(Wxh)
    dWhh = np.zeros_like(Whh)
    dWhy = np.zeros_like(Why)
    dbh = np.zeros_like(bh)
    dby = np.zeros_like(by)
    dhnext = np.zeros_like(hs[0])
    
    for t in reversed(range(len(inputs))):
        dy = np.copy(ps[t])
        dy[targets[t]] -= 1
        dWhy += dy @ hs[t].T
        dby += dy
        dh = Why.T @ dy + dhnext
        dhraw = (1 - hs[t] * hs[t]) * dh
        dbh += dhraw
        dWxh += dhraw @ xs[t].T
        dWhh += dhraw @ hs[t-1].T
        dhnext = Whh.T @ dhraw
    
    # Clip gradients
    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
        np.clip(dparam, -5, 5, out=dparam)
    
    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]

# Simple training loop
n_iter = 50
hprev = np.zeros((hidden_size,1))
data = [char_to_ix[ch] for ch in text]

for i in range(n_iter):
    idx = 0
    if idx + seq_length + 1 >= len(data):
        hprev = np.zeros((hidden_size,1))
        idx = 0
    inputs = data[idx:idx+seq_length]
    targets = data[idx+1:idx+seq_length+1]
    
    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = loss_and_gradients(inputs, targets, hprev)
    
    # SGD
    Wxh -= learning_rate * dWxh
    Whh -= learning_rate * dWhh
    Why -= learning_rate * dWhy
    bh  -= learning_rate * dbh
    by  -= learning_rate * dby

print(loss)

```

ç¤ºä¾‹ç»“æœï¼š

```
0.16299322447956405
```



## 2015.08 ç†è§£ LSTM ç½‘ç»œ

åŸæ–‡åœ°å€ï¼šhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/

------

è¿™ç¯‡æ–‡ç« ä» RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰çš„é—®é¢˜å…¥æ‰‹ï¼Œå¼•å‡º LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ŒLong Short Term Memoryï¼‰çš„è®¾è®¡æ€è·¯å’Œå·¥ä½œæœºåˆ¶ã€‚æ–‡ç« ç»“æ„å¤§è‡´å¦‚ä¸‹ï¼š

1. ä»‹ç» RNNï¼šä¸ºä½• â€œå¾ªç¯â€ æ˜¯å¿…è¦çš„ï¼Œä»¥åŠ RNN çš„åŸºæœ¬ç»“æ„ã€‚ [colah.github.io+1](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. â€œé•¿æœŸä¾èµ–é—®é¢˜â€ï¼ˆLong-Term Dependenciesï¼‰ï¼šä¼ ç»Ÿ RNN åœ¨æ—¶é—´è·¨åº¦è¾ƒé•¿çš„ä¾èµ–ä¸Šè®­ç»ƒå›°éš¾ã€‚ [colah.github.io+1](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
3. LSTM çš„åŸºæœ¬æ€æƒ³ï¼šå¼•å…¥ cell stateï¼ˆç»†èƒçŠ¶æ€ï¼‰ã€é—¨æ§æœºåˆ¶ï¼ˆgatesï¼‰æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚ [colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
4. è¯¦ç»†é€æ­¥è§£æ LSTM æ¨¡å—å†…éƒ¨ï¼ˆæŒ‰æ—¶é—´æ­¥ tï¼‰â€” å¿˜è®°é—¨ï¼ˆforget gateï¼‰ã€è¾“å…¥é—¨ï¼ˆinput gateï¼‰ã€å€™é€‰å€¼ã€çŠ¶æ€æ›´æ–°ã€è¾“å‡ºé—¨ï¼ˆoutput gateï¼‰ç­‰ã€‚ [colah.github.io+1](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
5. LSTM çš„å˜ä½“ï¼ˆå¦‚ peephole connectionsã€GRU ç­‰ï¼‰ç®€è¦è®¨è®ºã€‚ [colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
6. æ€»ç»“ï¼šLSTM åœ¨å¾ˆå¤šåºåˆ—ä»»åŠ¡ä¸Šæ¯”ä¼ ç»Ÿ RNN å¥½å¾ˆå¤šï¼Œä½†ä¹Ÿä¸æ˜¯ç»ˆæè§£ç­”ï¼ŒAttention ç­‰æœºåˆ¶æ˜¯ä¸‹ä¸€æ­¥ã€‚ 

------

å…³é”®æ¦‚å¿µä¸æœºåˆ¶è§£æï¼š

![image-20251125215919524](./assets/image-20251125215919524.png)

![image-20251125215905764](./assets/image-20251125215905764.png)

------

ä»¥ Python + NumPy å†™ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ LSTM å•å…ƒï¼Œç”¨äºæ•™è‚²ç†è§£ï¼ˆå¹¶éå·¥ä¸šçº§æ•ˆç‡æˆ–ç¨³å®šæ€§ï¼‰ï¼Œå¸®åŠ© â€œè½å®â€ æ–‡ç« ä¸­çš„æ­¥éª¤ï¼š

```python
import numpy as np

def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

class SimpleLSTMCell:
    def __init__(self, input_dim, hidden_dim):
        # åˆå§‹åŒ–æƒé‡ï¼ˆè¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œä¸€æ¬¡æ€§å°† [h_{t-1}, x_t] è¿æ¥ä½œä¸ºè¾“å…¥ï¼‰
        self.hidden_dim = hidden_dim
        # å¿˜è®°é—¨
        self.W_f = np.random.randn(hidden_dim, hidden_dim + input_dim) * 0.1
        self.b_f = np.zeros((hidden_dim, 1))
        # è¾“å…¥é—¨
        self.W_i = np.random.randn(hidden_dim, hidden_dim + input_dim) * 0.1
        self.b_i = np.zeros((hidden_dim, 1))
        # å€™é€‰ C_t å€¼
        self.W_C = np.random.randn(hidden_dim, hidden_dim + input_dim) * 0.1
        self.b_C = np.zeros((hidden_dim, 1))
        # è¾“å‡ºé—¨
        self.W_o = np.random.randn(hidden_dim, hidden_dim + input_dim) * 0.1
        self.b_o = np.zeros((hidden_dim, 1))
        
    def forward(self, x_t, h_prev, C_prev):
        """å•æ­¥ forwardï¼Œè¿”å› h_t, C_t"""
        concat = np.vstack((h_prev, x_t))
        
        f_t = sigmoid(self.W_f.dot(concat) + self.b_f)
        i_t = sigmoid(self.W_i.dot(concat) + self.b_i)
        C_tilde = tanh(self.W_C.dot(concat) + self.b_C)
        C_t = f_t * C_prev + i_t * C_tilde
        o_t = sigmoid(self.W_o.dot(concat) + self.b_o)
        h_t = o_t * tanh(C_t)
        
        return h_t, C_t

# æµ‹è¯•è¿™ä¸ª cell
input_dim = 3
hidden_dim = 5
cell = SimpleLSTMCell(input_dim, hidden_dim)

# æ¨¡æ‹Ÿ 4 æ­¥è¾“å…¥
x_seq = [np.random.randn(input_dim,1) for _ in range(4)]
h = np.zeros((hidden_dim,1))
C = np.zeros((hidden_dim,1))

for t, x_t in enumerate(x_seq):
    h, C = cell.forward(x_t, h, C)
    print(f"step {t}, h = {h.ravel()}, C = {C.ravel()}")

```

è¯´æ˜ï¼š

- è¿™é‡Œ `SimpleLSTMCell` å®ç°äº†æ–‡ç« æ‰€è¿°çš„å››ä¸ªä¸»è¦é—¨ï¼ˆå¿˜è®°ã€è¾“å…¥ã€å€™é€‰ã€è¾“å‡ºï¼‰ã€‚
- æˆ‘ä»¬åœ¨æ¯ä¸€æ­¥å°†å‰ä¸€éšè—çŠ¶æ€ `h_prev` å’Œå½“å‰è¾“å…¥ `x_t` æ‹¼æ¥ï¼ˆconcatenateï¼‰åï¼Œç”¨äºå„ä¸ªé—¨çš„è®¡ç®—ã€‚
- æµ‹è¯•éƒ¨åˆ†æ¨¡æ‹Ÿäº†ä¸€ä¸ªé•¿åº¦ä¸º 4 çš„è¾“å…¥åºåˆ—ã€‚ä½ ä¼šçœ‹åˆ° `h` å’Œ `C` éšæ—¶é—´æ­¥å˜åŒ–ã€‚
- è™½ç„¶è¿™ä¸ªå®ç°æœªåšæ¢¯åº¦åå‘ã€è®­ç»ƒã€æ‰¹é‡åŒ–ã€ç¨³å®šåŒ–ï¼ˆå¦‚æ¢¯åº¦è£å‰ªã€dropoutã€LayerNorm ç­‰ï¼‰ï¼Œä½†è¶³å¤Ÿç”¨äºç†è§£æœºåˆ¶ã€‚

ä½ ä¹Ÿå¯ä»¥æ‰©å±•ï¼šä¾‹å¦‚å¤šå±‚ LSTMã€åŠ ä¸Šè¾“å‡ºå±‚ç”¨äºåˆ†ç±»ï¼å›å½’ä»»åŠ¡ã€ç”¨çœŸå®åºåˆ—ï¼ˆå¦‚æ–‡å­—ã€æ—¶é—´åºåˆ—ï¼‰è®­ç»ƒã€‚

------

è¡¥å……ï¼š

![image-20251125220045368](./assets/image-20251125220045368.png)





## 2014.09 Recurrent Neural Network Regularization

>We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.

è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/1409.2329v5



### æ¦‚è¿°

æ¦‚è¿°1ï¼š

- Authors: Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals
- The paper â€œRecurrent Neural Network Regularizationâ€ presents a novel method for applying dropout to Long Short-Term Memory (LSTM) networks to mitigate overfitting. **Traditional dropout techniques are ineffective for Recurrent Neural Networks (RNNs) due to noise amplification in recurrent connections,** which hampers learning. **The authors propose a specialized dropout application that targets only non-recurrent connections in LSTMs, preserving the networkâ€™s ability to retain information over long sequences while reducing overfitting.**
- The study demonstrates significant performance improvements across various tasks, including language modeling, speech recognition, machine translation, and image caption generation. In language modeling, regularized LSTMs achieved **better word-level perplexity** on the Penn Tree Bank dataset **compared to non-regularized models.** The medium and large regularized LSTMs showed **substantial reductions in perplexity**, highlighting the efficacy of the proposed method.
- **For speech recognition**, the authors tested their method on an internal Google Icelandic Speech dataset, showing that dropout improves **frame accuracy**, a critical metric correlating with Word Error Rate (WER). Regularized LSTMs achieved **better generalization**, indicating the potential of the proposed regularization technique for improving acoustic modeling.
- **In machine translation**, the method was evaluated on the WMTâ€™14 English to French dataset. The regularized LSTM outperformed non-regularized models, demonstrating **higher BLEU scores**, which measure translation quality. Although the regularized LSTM **did not surpass the phrase-based LIUM SMT system**, the results affirmed that dropout enhances translation performance.
- **The image caption generation task** involved testing the dropout variant on an LSTM model that **converts image vectors into captions**. The authors used the MSCOCO dataset for this evaluation. The results showed that dropout helps **improve caption quality**, with regularized models performing comparably to model ensembles.
- Overall, the paper establishes that correctly applying dropout to LSTMs effectively reduces overfitting and enhances performance across diverse applications. The authors suggest that this approach can be extended to other RNN architectures, potentially broadening the scope of improved regularization in neural networks.

------

**æ ¸å¿ƒé—®é¢˜**ï¼šä¼ ç»Ÿçš„ dropoutï¼ˆåœ¨å‰é¦ˆç½‘ç»œä¸Šéå¸¸æˆåŠŸï¼‰**ç›´æ¥åº”ç”¨åˆ° RNN/LSTM çš„â€œæ‰€æœ‰è¿æ¥â€ä¼šæŸå®³è®°å¿†èƒ½åŠ›**ï¼Œå› ä¸º dropout éšæ—¶é—´é€æ­¥å¹²æ‰°å¾ªç¯çŠ¶æ€ï¼Œé•¿æœŸè®°å¿†è¢«å™ªå£°å†²æ·¡ï¼Œä»è€Œ RNN éš¾ä»¥ä¿ç•™è·¨å¾ˆé•¿æ­¥çš„ä¿¡æ¯ã€‚

**ä¸»è¦è´¡çŒ® / ç»“è®º**ï¼š

1. **åªå¯¹éå¾ªç¯ï¼ˆnon-recurrentï¼‰è¿æ¥æ–½åŠ  dropout**ï¼šä¹Ÿå°±æ˜¯å¯¹ *è¾“å…¥â†’éšè—* æˆ– *éšè—â†’è¾“å‡º / å±‚é—´* çš„è¿æ¥ä½¿ç”¨ dropoutï¼Œä½†**ä¸è¦å¯¹éšè—â†’éšè—ï¼ˆrecurrentï¼‰è¿æ¥ä½¿ç”¨ dropout**ã€‚è¿™ç§åšæ³•æ—¢èƒ½è·å¾— dropout çš„æ­£åˆ™åŒ–æ•ˆæœï¼Œåˆä¸ä¼šç ´åè·¨æ—¶é—´æ­¥çš„è®°å¿†ä¼ é€’ã€‚
2. **ç›´è§‚åŸå› **ï¼šå¦‚æœåœ¨å¾ªç¯ï¼ˆéšè—â†’éšè—ï¼‰ä¸Š dropoutï¼Œåˆ™ä¸€ä¸ªé€šè¿‡å¤šä¸ªæ—¶é—´æ­¥å‘åä¼ æ’­çš„ä¿¡æ¯ä¼šåœ¨æ¯ä¸€æ­¥éƒ½è¢«ç½®é›¶çš„æ¦‚ç‡å½±å“ï¼Œå™ªå£°éšç€æ—¶é—´è¢«â€œæ”¾å¤§â€ï¼Œå¯¼è‡´é•¿æ—¶è®°å¿†éš¾ä»¥å­¦ä¹ ï¼›è€ŒæŠŠ dropout é™äºéå¾ªç¯è¿æ¥ï¼Œä¿¡æ¯æ²¿æ—¶é—´æµåŠ¨æ—¶åªä¼šè¢«å›ºå®šæ¬¡æ•°ï¼ˆä¸ç½‘ç»œæ·±åº¦æœ‰å…³ï¼‰æ‰€æ‰°åŠ¨ï¼Œä¸ä¾èµ–äºè·¨è¶Šæ—¶åºé•¿åº¦ã€‚è®ºæ–‡ç”¨å›¾ä¸è§£æè§£é‡Šäº†è¿™ä¸€ç‚¹ã€‚
3. **å®è¯ç»“æœ**ï¼šåœ¨è‹¥å¹²ä»»åŠ¡ï¼ˆPTB è¯­è¨€å»ºæ¨¡ã€è¯­éŸ³è¯†åˆ«ã€è‹±æ³•ç¿»è¯‘ã€å›¾åƒæè¿°ï¼‰ä¸Šï¼ŒæŒ‰ä¸Šè¿°æ–¹æ³• regularize çš„ LSTM æ˜¾è‘—é™ä½è¿‡æ‹Ÿåˆå¹¶æå‡éªŒè¯ / æµ‹è¯•æ€§èƒ½ï¼ˆè®ºæ–‡ç»™å‡º PTB ä¸Š perplexity ä¸ Machine Translation çš„ BLEU ç­‰è¡¨æ ¼ä½œä¸ºè¯æ®ï¼‰ã€‚ä¾‹å¦‚åœ¨ PTB ä¸Šï¼Œregularized LSTM æ˜¾è‘—ä¼˜äºéæ­£åˆ™åŒ– LSTMï¼ˆæ–‡ä¸­ç»™å‡ºæ•°å€¼è¡¨æ ¼ï¼‰ã€‚

**è®ºæ–‡ä»£ç ä¸å¤ç°**ï¼šä½œè€…æä¾›äº†ä»£ç é“¾æ¥ (GitHub) å¹¶åœ¨è®ºæ–‡ä¸­ç»™å‡ºè®­ç»ƒç»†èŠ‚ï¼ˆè¶…å‚ã€æ¢¯åº¦è£å‰ªã€å­¦ä¹ ç‡ schedule ç­‰ï¼‰ï¼Œè¿™æœ‰åŠ©äºå¤ç°å®éªŒã€‚

------

ç®€è¦è¯´æ˜ï¼š

- è®¾æƒ³ä¸€æ¡ä¿¡æ¯ä»æ—¶é—´ t0 ç»ç”± LSTM ä¼ åˆ°æ—¶é—´ t10ã€‚å¦‚æœåœ¨**å¾ªç¯æƒé‡**ä¸Šåš dropoutï¼Œé‚£ä¹ˆæ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½æœ‰å¯èƒ½â€œæ–­æ‰â€æˆ–æ”¾å¤§å™ªå£°ï¼Œä½¿å¾—è·¨æ—¶é—´çš„ä¿¡æ¯è¢«ä¸¥é‡ç ´åã€‚
- å¦‚æœæˆ‘ä»¬åªåœ¨ **è¾“å…¥ embedding â†’ LSTM** æˆ– **LSTM è¾“å‡º â†’ å…¨è¿æ¥** ä¸Šåš dropoutï¼ˆä¸”å¯¹äºå¤šå±‚ LSTMï¼Œåªåœ¨å±‚é—´çš„ â€œéå¾ªç¯â€ è·¯å¾„ä¸Šåš dropoutï¼‰ï¼Œé‚£ä¹ˆè¿™æ¡ä¿¡æ¯åœ¨æ—¶é—´æµåŠ¨æ—¶å¹¶ä¸ä¼šåœ¨æ¯ä¸€æ­¥è¢«éšæœºæ‰°åŠ¨ï¼Œä»è€Œä¿ç•™é•¿æœŸä¾èµ–èƒ½åŠ›ï¼ŒåŒæ—¶ä»å¯è·å¾— dropout å¸¦æ¥çš„æŠ—è¿‡æ‹Ÿåˆæ•ˆæœã€‚



### ç¤ºä¾‹ä»£ç 

------

ç¤ºä¾‹ä»£ç 1ï¼š**PyTorch ç¤ºä¾‹ï¼ˆæ¨èï¼‰** â€” é€¼è¿‘è®ºæ–‡ä¸­â€œå¯¹éå¾ªç¯è¿æ¥ dropoutâ€çš„å®ç°æ€è·¯ï¼ˆåœ¨ embedding å’Œè¾“å‡º projection ä¸Šä½¿ç”¨ dropoutï¼Œè€Œä¸ä½¿ç”¨ `nn.LSTM` çš„ `dropout` æ¥å¹²æ‰°å¾ªç¯ï¼‰ã€‚è¯·åœ¨æœ¬åœ°æœ‰ PyTorch ç¯å¢ƒæ—¶è¿è¡Œï¼ˆä¾‹å¦‚ `pip install torch`ï¼‰ã€‚

```python
# rnn_dropout_demo_pytorch.py
# ä¾èµ–: torch, matplotlib
# pip install torch matplotlib

import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt

# reproducibility
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

# tiny synthetic dataset
V = 50
L = 8
train_n = 200
val_n = 1000

def make_dataset(n):
    X = np.random.randint(0, V, size=(n, L))
    # simple target: sum of tokens mod V -> task learnable
    y = (X.sum(axis=1) % V).astype(np.int64)
    return X, y

X_train, y_train = make_dataset(train_n)
X_val, y_val = make_dataset(val_n)

class SeqDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(SeqDataset(X_train, y_train), batch_size=64, shuffle=True)
val_loader = DataLoader(SeqDataset(X_val, y_val), batch_size=256, shuffle=False)

class TinyLSTM(nn.Module):
    def __init__(self, vocab_size, emb=32, hid=64, drop_p=0.0):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, emb)
        # note: do not set dropout inside LSTM (that would apply between layers / or to recurrent part)
        self.lstm = nn.LSTM(input_size=emb, hidden_size=hid, num_layers=1, batch_first=True)
        self.input_dropout = nn.Dropout(p=drop_p)   # NON-recurrent dropout (input->LSTM)
        self.output_dropout = nn.Dropout(p=drop_p)  # NON-recurrent dropout (LSTM->fc)
        self.fc = nn.Linear(hid, vocab_size)
    def forward(self, x):
        e = self.embed(x)           # (batch, seq_len, emb)
        e = self.input_dropout(e)   # dropout on inputs only
        out, _ = self.lstm(e)       # recurrent weights untouched by dropout
        h = out[:, -1, :]           # use last timestep
        h = self.output_dropout(h)  # dropout before final layer
        return self.fc(h)

def train(model, loader, opt, loss_fn):
    model.train()
    total = 0.0
    for xb, yb in loader:
        logits = model(xb)
        loss = loss_fn(logits, yb)
        opt.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
        opt.step()
        total += loss.item() * xb.size(0)
    return total / len(loader.dataset)

def evaluate(model, loader, loss_fn):
    model.eval()
    total = 0.0
    with torch.no_grad():
        for xb, yb in loader:
            logits = model(xb)
            loss = loss_fn(logits, yb)
            total += loss.item() * xb.size(0)
    return total / len(loader.dataset)

def run_experiment(drop_p):
    model = TinyLSTM(V, emb=32, hid=128, drop_p=drop_p)
    opt = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.CrossEntropyLoss()
    train_losses, val_losses = [], []
    for epoch in range(12):
        tr = train(model, train_loader, opt, loss_fn)
        va = evaluate(model, val_loader, loss_fn)
        train_losses.append(tr)
        val_losses.append(va)
        print(f"drop={drop_p} epoch={epoch+1} train={tr:.4f} val={va:.4f}")
    return train_losses, val_losses

if __name__ == "__main__":
    tr_no, va_no = run_experiment(0.0)
    tr_do, va_do = run_experiment(0.5)

    plt.plot(tr_no, label='train no-drop')
    plt.plot(va_no, label='val no-drop')
    plt.plot(tr_do, label='train drop p=0.5')
    plt.plot(va_do, label='val drop p=0.5')
    plt.legend()
    plt.xlabel("Epoch")
    plt.ylabel("Cross-Entropy Loss")
    plt.title("Dropout on non-recurrent connections (toy demo)")
    plt.show()

```

------

ç¤ºä¾‹ä»£ç 2ï¼š**NumPy æç®€æ¼”ç¤ºï¼ˆå³æ—¶å¯è·‘ï¼‰** â€” ç”¨éå¸¸å°çš„ RNNï¼ˆé LSTMï¼‰å’Œç®€å•æ¢¯åº¦æ­¥éª¤ç¤ºä¾‹åŒ–â€œå¯¹è¾“å…¥åš dropout vs ä¸åšâ€çš„å·®å¼‚ï¼Œä¾¿äºåœ¨æ²¡æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶æ—¶ç›´è§‚è§‚å¯Ÿè®­ç»ƒ/éªŒè¯æŸå¤±çš„è¡Œä¸ºå·®å¼‚ã€‚

```python
# rnn_dropout_demo_numpy.py
# åªéœ€æ ‡å‡† Python + numpy + matplotlib

import numpy as np
import random
import matplotlib.pyplot as plt

np.random.seed(0)
random.seed(0)

V = 30   # vocab size
L = 6
train_n = 150
val_n = 500

def make_dataset(n):
    X = np.random.randint(0, V, size=(n, L))
    y = (X.sum(axis=1) % V)
    return X, y

X_tr, y_tr = make_dataset(train_n)
X_va, y_va = make_dataset(val_n)

# one-hot embeddings
def one_hot_batch(X):
    b, L = X.shape
    o = np.zeros((b, L, V), dtype=float)
    for i in range(b):
        o[i, np.arange(L), X[i]] = 1.0
    return o

# tiny RNN params
H = 40
Wx = np.random.randn(V, H) * 0.1  # input->hidden
Wh = np.random.randn(H, H) * 0.1  # hidden->hidden (recurrent)
Wo = np.random.randn(H, V) * 0.1  # hidden->output
bh = np.zeros(H)
bo = np.zeros(V)

def softmax(x):
    e = np.exp(x - x.max(axis=1, keepdims=True))
    return e / e.sum(axis=1, keepdims=True)

def forward_batch(X, dropout_p=0.0):
    # X: (batch, L)
    B = X.shape[0]
    xoh = one_hot_batch(X)  # (B,L,V)
    h = np.zeros((B, H))
    for t in range(L):
        xt = xoh[:, t, :]                   # (B,V)
        if dropout_p > 0:
            mask = (np.random.rand(*xt.shape) >= dropout_p).astype(float) / (1.0 - dropout_p)
            xt = xt * mask
        h = np.tanh(xt.dot(Wx) + h.dot(Wh) + bh)
    logits = h.dot(Wo) + bo
    probs = softmax(logits)
    return probs, h

def loss_and_acc(X, y, dropout_p=0.0):
    probs, _ = forward_batch(X, dropout_p=0.0)  # eval w/o dropout
    # cross-entropy
    idx = np.arange(len(y))
    loss = -np.mean(np.log(probs[idx, y] + 1e-12))
    preds = probs.argmax(axis=1)
    acc = (preds == y).mean()
    return loss, acc

# Very simple "training" to show trends (not full gradient descent impl) - we just update Wo a bit
def train_epoch(Wo, lr=0.03, dropout_p=0.0):
    # simple finite-diff-ish update on Wo to keep code short (not recommended for real training)
    # This is intentionally simplistic: the goal is to see train vs val loss trends changing with dropout.
    for i in range(0, train_n, 30):
        xb = X_tr[i:i+30]
        probs, h = forward_batch(xb, dropout_p=dropout_p)
        # gradient for Wo from cross-entropy
        yb = y_tr[i:i+30]
        B = len(yb)
        target = np.zeros_like(probs)
        target[np.arange(B), yb] = 1.0
        grad = h.T.dot(probs - target) / B
        Wo -= lr * grad
    return Wo

# run small experiments
epochs = 20
Wo1 = Wo.copy()
train_losses_0, val_losses_0 = [], []
for e in range(epochs):
    Wo1 = train_epoch(Wo1, lr=0.05, dropout_p=0.0)
    l_tr, _ = loss_and_acc(X_tr, y_tr, dropout_p=0.0)
    l_va, _ = loss_and_acc(X_va, y_va, dropout_p=0.0)
    train_losses_0.append(l_tr); val_losses_0.append(l_va)

# with input dropout during training
Wo2 = Wo.copy()
train_losses_1, val_losses_1 = [], []
for e in range(epochs):
    Wo2 = train_epoch(Wo2, lr=0.05, dropout_p=0.4)  # training uses input dropout
    l_tr, _ = loss_and_acc(X_tr, y_tr, dropout_p=0.0)
    l_va, _ = loss_and_acc(X_va, y_va, dropout_p=0.0)
    train_losses_1.append(l_tr); val_losses_1.append(l_va)

plt.plot(train_losses_0, label="train no-drop")
plt.plot(val_losses_0, label="val no-drop")
plt.plot(train_losses_1, label="train input-drop")
plt.plot(val_losses_1, label="val input-drop")
plt.legend(); plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("NumPy toy demo")
plt.show()

```

------



### QA

#### 1ï¼‰å…³äº non-recurrent

æ¦‚è¿°ï¼š

- ğŸ‘‰ **â€œéå¾ªç¯ï¼ˆnon-recurrentï¼‰è¿æ¥â€ = ä¸æ²¿æ—¶é—´ä¼ æ’­ä¿¡æ¯çš„è¿æ¥**
  - å¯¹å®ƒä»¬ dropoutï¼Œåªä¼šå½±å“æŸä¸€æ—¶é—´æ­¥ï¼Œä¸ä¼šç ´åé•¿æ—¶è®°å¿†ã€‚
- ğŸ‘‰ **â€œå¾ªç¯ï¼ˆrecurrentï¼‰è¿æ¥â€ = è´Ÿè´£æŠŠè®°å¿†ä»å‰ä¸€æ—¶åˆ»ä¼ åˆ°ä¸‹ä¸€æ—¶åˆ»çš„è¿æ¥**
  - å¯¹å®ƒ dropoutï¼Œå°±æ˜¯åœ¨æ—¶é—´ä¸Šçš„è®°å¿†é“¾è·¯ä¸Šåˆ¶é€ éšæœºæ–­è£‚ï¼Œä¼šæ¯æ‰ RNN/LSTM çš„èƒ½åŠ›ã€‚

<img src="./assets/image-20251127094752542.png" alt="image-20251127094752542" style="zoom: 80%;" />

<img src="./assets/image-20251127095007550.png" alt="image-20251127095007550" style="zoom: 80%;" />



#### 2ï¼‰å±‚é—´è¿æ¥ï¼ˆL1 â†’ L2ï¼‰ä¸è·¨æ—¶é—´

ğŸ‘‰ â€œè·¨å±‚è¿æ¥â€å‘ç”Ÿåœ¨**åŒä¸€æ—¶é—´æ­¥**ï¼Œä»…ç”¨äºæŠŠå½“å‰æ—¶é—´æ­¥çš„ç‰¹å¾åœ¨ä¸åŒæ·±åº¦é—´ä¼ é€’ï¼Œä¸æ¶‰åŠè·¨æ—¶é—´çš„è®°å¿†æµåŠ¨ï¼Œæ‰€ä»¥ä¸æ˜¯ recurrentã€‚

<img src="./assets/image-20251127095251891.png" alt="image-20251127095251891" style="zoom:80%;" />

<img src="./assets/image-20251127095613959.png" alt="image-20251127095613959" style="zoom:80%;" />



## 1993.08 Keeping Neural Networks Simple by Minimizing the Description Length of the Weights

è®ºæ–‡åœ°å€ï¼šhttps://www.cs.toronto.edu/~hinton/absps/colt93.pdf



### æ¦‚è¿°

æ¦‚è¿°1ï¼š

- Authors: Geoffrey E. Hinton and Drew van Camp
- The paper â€œKeeping Neural Networks Simple by Minimizing the Description Length of the Weightsâ€ by Hinton and van Camp introduces **a method to regularize neural networks by penalizing the information content in the weights**. The key idea is to **add Gaussian noise to the weights and adapt the noise level during training** to balance the trade-off between the networkâ€™s error and the complexity of the weights.
- The Minimum Description Length (MDL) Principle underpins this method, suggesting that the best model minimizes the total cost of describing both the model and the errors it makes. For neural networks, this translates to minimizing the bits required to encode the weights and the discrepancies between the predicted and actual outputs.
- By applying Gaussian noise to the weights, the authors effectively control the precision of weight values. This approach helps in reducing overfitting, especially in scenarios with limited training data. The noise level is adjusted to optimize the networkâ€™s performance while keeping the weights as simple as possible.
- The method involves computing the derivatives of both the expected squared error and the information content in the weights. These derivatives are calculated efficiently without resorting to time-consuming Monte Carlo simulations, provided the output units are linear.
- The authors introduce the concept of â€œnoisy weightsâ€ where adding Gaussian noise allows for a more compact encoding of the weights. This noisy weight approach leverages the MDL principle to communicate weights more efficiently, balancing the trade-off between weight precision and the networkâ€™s error.
- The study explores the application of this technique across different tasks, including language modeling, speech recognition, and image caption generation. The results show that the proposed regularization method significantly improves generalization by reducing overfitting.
- Additionally, the paper discusses the benefits of using an adaptive mixture of Gaussians for encoding the weights. This mixture model adapts to the distribution of the weights during training, further enhancing the networkâ€™s ability to generalize from limited data.
- Preliminary experiments on a high-dimensional task with scarce training data demonstrate that the new method allows for fitting complex non-linear models effectively. The results suggest that this approach is slightly better than traditional weight-decay methods, offering a new perspective on regularizing neural networks.
- The authors conclude by acknowledging that while the new method shows promise, more experimental work is needed to determine its competitiveness with other statistical techniques for handling non-linear tasks with limited training data. They also highlight the potential for further refinements to enhance its performance.

------

æ¦‚è¿°2ï¼š

- **æ ¸å¿ƒæƒ³æ³• â€” æœ€å°æè¿°é•¿åº¦ï¼ˆMDLï¼‰**
   MDL åŸåˆ™è®¤ä¸ºï¼šæœ€å¥½æ¨¡å‹æ˜¯èƒ½åŒæ—¶æœ€çŸ­åœ°**æè¿°æ¨¡å‹æœ¬èº«**ï¼ˆä¹Ÿå°±æ˜¯æƒé‡æ‰€éœ€çš„ä½æ•°ï¼‰å’Œ**æè¿°æ•°æ®è¯¯å·®**ï¼ˆæ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´å·®å€¼ï¼‰çš„æ¨¡å‹ã€‚æ¢å¥è¯è¯´ï¼Œä¸èƒ½åªè¿½æ±‚è®­ç»ƒè¯¯å·®å°ï¼Œè¿˜è¦ä¿è¯æƒé‡æœ¬èº«â€œç®€å•â€ï¼ˆä¿¡æ¯é‡å°ï¼‰ï¼Œå› ä¸ºæƒé‡è¶Šå¤æ‚è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆã€‚
- **æŠŠæè¿°é•¿åº¦å’Œå¸¸è§æŸå¤±è”ç³»èµ·æ¥**
  - å¦‚æœæŠŠæ•°æ®è¯¯å·®ï¼ˆæ¯ä¸ªè¾“å‡ºçš„è¯¯å·®ï¼‰å‡è®¾ä¸ºé«˜æ–¯å™ªå£°ï¼Œé‚£ä¹ˆç”¨ MDL æœ€å°åŒ–æ•°æ®è¯¯å·®çš„é¡¹ç­‰ä»·äºæœ€å°åŒ–å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚ï¼ˆè®ºæ–‡å…¬å¼ 1â€“3ï¼‰
  - å¦‚æœæŠŠæƒé‡çš„å…ˆéªŒå‡è®¾ä¸ºé›¶å‡å€¼é«˜æ–¯åˆ†å¸ƒï¼Œæƒé‡çš„æè¿°é•¿åº¦ï¼ˆlog æ¦‚ç‡ï¼‰æ˜¯ä¸æƒé‡å¹³æ–¹å’Œæˆæ­£æ¯”çš„â€”â€”è¿™å°±æ˜¯å¸¸è§çš„ **weight decay / L2 æ­£åˆ™åŒ–**ã€‚ï¼ˆè®ºæ–‡å…¬å¼ 4ï¼‰
- **ç¼ºç‚¹ä¸æ”¹è¿›**
  - ç®€å•çš„ weight decay å‡è®¾æ‰€æœ‰æƒé‡éƒ½ä»¥ç›¸åŒç²¾åº¦ç¼–ç ï¼ˆç›¸åŒçš„æ–¹å·®ï¼‰ï¼Œè¿™å¿½ç•¥äº†â€œæƒé‡å¯ä»¥ç”¨ä¸åŒç²¾åº¦ç¼–ç â€çš„äº‹å®ã€‚æœ‰äº›æƒé‡å¯ä»¥å¾ˆç²—ç³™ï¼ˆé«˜æ–¹å·®ï¼‰åœ°è¡¨ç¤ºè€Œä¸å½±å“è¾“å‡ºï¼›æœ‰äº›æƒé‡å¿…é¡»å¾ˆç²¾ç¡®ï¼ˆä½æ–¹å·®ï¼‰ã€‚å¦‚æœèƒ½ä¸ºæ¯ä¸ªæƒé‡ï¼ˆæˆ–æ¯ç»„æƒé‡ï¼‰é€‰æ‹©åˆé€‚çš„ç²¾åº¦ï¼Œä¼šæ›´èŠ‚çœæ¯”ç‰¹ï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒè¯¯å·®ã€‚
  - è®ºæ–‡æå‡ºå¯¹æƒé‡ä½¿ç”¨**å¸¦å™ªå£°çš„åéªŒåˆ†å¸ƒ**ï¼ˆposterior Qï¼‰ï¼Œä¹Ÿå°±æ˜¯æŠŠæƒé‡çœ‹æˆéšæœºå˜é‡ $w \sim Q(w)$ï¼Œå¹¶åŒæ—¶æŠŠç¼–ç å…ˆéªŒ $P(w)$ è®¾ä¸ºé«˜æ–¯ã€‚è¦æœ€å°åŒ–çš„ç›®æ ‡å˜ä¸ºï¼šæœŸæœ›ï¼ˆåœ¨ Q ä¸‹ï¼‰æ•°æ®è¯¯å·® + ç¼–ç æˆæœ¬ï¼ˆå³ $\text{KL}(Q\|P)$ æˆ–ä¸å¯¹ç§°æ•£åº¦ï¼‰ã€‚è¿™å®é™…ä¸Šæ˜¯å˜åˆ†è§†è§’ / è´å¶æ–¯æ€è·¯çš„é›å½¢ã€‚
- **â€œnoisy weightsâ€ çš„ç›´è§‰**
  - å…è®¸æƒé‡å¸¦æœ‰æ–¹å·®ï¼ˆå™ªå£°ï¼‰æ„å‘³ç€ï¼šæŸäº›æƒé‡çš„ posterior æ–¹å·®ä¼šå˜å¤§ï¼ˆè¡¨æ˜è¿™ä¸ªæƒé‡å¯ä»¥ç²—ç•¥ç¼–ç ï¼‰ï¼Œä»è€Œé™ä½ç¼–ç æˆæœ¬ï¼Œä½†ä¼šå¢åŠ æ•°æ®è¯¯å·®çš„æœŸæœ›å€¼ã€‚è®­ç»ƒçš„ç›®æ ‡æ˜¯å¹³è¡¡è¿™ä¸¤è€…â€”â€”è¿™æ˜¯ MDL çš„è‡ªç„¶è¯ é‡Šã€‚
- **å®é™…è®¡ç®—**
  - è®ºæ–‡åœ¨â€œæœ‰ä¸€å±‚éšè—å±‚ä¸”è¾“å‡ºå±‚çº¿æ€§â€çš„æ¶æ„ä¸‹ï¼Œæå‡ºäº†å¯ä»¥ç²¾ç¡®è®¡ç®—æœŸæœ›è¯¯å·®ä¸å…¶å¯¼æ•°çš„æ–¹æ³•ï¼Œä»è€Œé¿å…æ˜‚è´µçš„è’™ç‰¹å¡æ´›ä¼°è®¡ï¼ˆåœ¨å½“æ—¶éå¸¸å…³é”®ï¼‰ã€‚ç°ä»£å®è·µä¸­é€šå¸¸ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·æˆ–é‡å‚æ•°åŒ–æŠ€å·§ï¼ˆreparameterization trickï¼‰æ¥è¿‘ä¼¼è®¡ç®—æœŸæœ›é¡¹å¹¶å¯¹å‡å€¼å’Œæ–¹å·®åšæ¢¯åº¦æ›´æ–°ï¼ˆè¿™ä¸è®ºæ–‡æ€è·¯ä¸€è‡´ï¼‰ã€‚

ç›´è§‚æ¯”å–»ï¼š

- æŠŠæ¨¡å‹çš„æ¯ä¸ªæƒé‡çœ‹æˆâ€œæˆ‘ä»¬è¦å‘é€ç»™æ¥æ”¶è€…çš„æ•°â€ã€‚æœ‰ä¸¤ä»¶äº‹è¦ä»˜è´¹ï¼šå‘é€æƒé‡æœ¬èº«ï¼ˆè¶Šç²¾ç¡®è¶Šè´µï¼‰å’Œå‘é€æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„å‰©ä½™è¯¯å·®ã€‚MDL è¦æˆ‘ä»¬ç”¨æœ€å°‘çš„æ€»ä½æ•°å®Œæˆä¸¤ä»¶äº‹ï¼šç¼–ç æ¨¡å‹ + ç¼–ç è¯¯å·®ã€‚å…è®¸å¯¹æŸäº›æƒé‡â€œé©¬è™â€ä¸€äº›ï¼ˆå™ªå£°å¤§ï¼‰å¯ä»¥èŠ‚çœä½æ•°ï¼Œä½†ä¼šè®©è¯¯å·®ç¨å¾®å¢å¤§ã€‚åˆé€‚çš„æŠ˜ä¸­å°±æ˜¯æœ€å¥½çš„æ¨¡å‹ã€‚



### ç¤ºä¾‹ä»£ç 

ä¸‹é¢ä»£ç æ¼”ç¤ºä¸‰ç§æ–¹æ³•åœ¨ä¸€ä¸ªä¸€ç»´å›å½’ï¼ˆtoyï¼‰ä¸Šçš„å¯¹æ¯”ï¼š

- **Plain MLP**ï¼šåªæœ€å°åŒ– MSEã€‚
- **Weight-decay MLP**ï¼šMSE + L2ï¼ˆä»£è¡¨è®ºæ–‡é‡Œçš„å›ºå®šæ–¹å·®å…ˆéªŒï¼‰ã€‚
- **Variational / Noisy-weights MLP**ï¼šæ¯å±‚ç”¨å‚æ•°åŒ–çš„ posteriorï¼ˆå‡å€¼ + æ–¹å·®æ ‡é‡ï¼‰ï¼Œè®­ç»ƒæ—¶æŠŠç›®æ ‡è®¾ä¸ºæœŸæœ› MSEï¼ˆç”¨é‡‡æ ·è¿‘ä¼¼ï¼‰+ KL(Q||P)ã€‚

```python
# å¯å¤åˆ¶åˆ°æœ¬åœ°æˆ– Colab è¿è¡Œï¼ˆéœ€è¦å®‰è£… PyTorchï¼‰
import torch, math
import torch.nn as nn
import torch.optim as optim
torch.manual_seed(1)

# --- ç”Ÿæˆ toy æ•°æ® ---
def make_toy(n=120):
    x = torch.linspace(-4, 4, n).unsqueeze(1)
    y = (torch.sin(x*1.5) + 0.3*torch.randn_like(x)).float()
    return x.float(), y.float()

x_train, y_train = make_toy(120)
x_val, y_val = make_toy(120)

device = torch.device("cpu")
x_train, y_train, x_val, y_val = [t.to(device) for t in (x_train, y_train, x_val, y_val)]

# --- æ™®é€š MLP ---
class MLP(nn.Module):
    def __init__(self, hidden=8):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(1, hidden), nn.Tanh(), nn.Linear(hidden, 1))
    def forward(self, x): return self.net(x)

# --- å˜åˆ† / å¸¦å™ªå£°æƒé‡çš„ MLP ---
class VariationalMLP(nn.Module):
    def __init__(self, hidden=8, prior_var=1.0):
        super().__init__()
        # mu parameters (means) for each weight tensor/bias
        self.fc1_mu = nn.Parameter(torch.randn(hidden, 1)*0.1)
        self.fc1_bias_mu = nn.Parameter(torch.zeros(hidden))
        self.fc2_mu = nn.Parameter(torch.randn(1, hidden)*0.1)
        self.fc2_bias_mu = nn.Parameter(torch.zeros(1))
        # one scalar log-variance per layer (ç®€åŒ–ï¼Œä¾¿äºè§‚å¯Ÿ)
        self.logvar1 = nn.Parameter(torch.tensor(-6.0))
        self.logvar2 = nn.Parameter(torch.tensor(-6.0))
        self.prior_var = prior_var

    def sample_weights(self):
        w1 = self.fc1_mu + torch.randn_like(self.fc1_mu) * torch.exp(0.5*self.logvar1)
        b1 = self.fc1_bias_mu + torch.randn_like(self.fc1_bias_mu) * torch.exp(0.5*self.logvar1)
        w2 = self.fc2_mu + torch.randn_like(self.fc2_mu) * torch.exp(0.5*self.logvar2)
        b2 = self.fc2_bias_mu + torch.randn_like(self.fc2_bias_mu) * torch.exp(0.5*self.logvar2)
        return w1, b1, w2, b2

    def forward_with_sample(self, x):
        w1, b1, w2, b2 = self.sample_weights()
        h = torch.tanh(x @ w1.t() + b1)
        y = h @ w2.t() + b2
        return y

    def kl_term(self):
        # KL between Q=N(mu, var_q) and P=N(0, prior_var)
        def kl_gauss(mu, logvar, prior_var):
            var_q = torch.exp(logvar)
            # æŒ‰å…ƒç´ çš„ KLï¼Œsum åˆ°å¼ é‡çº§åˆ«
            kl = 0.5*(torch.log(prior_var) - logvar + (var_q + (mu**2))/prior_var - 1.0)
            return kl.sum()
        k = kl_gauss(self.fc1_mu, self.logvar1, self.prior_var) + kl_gauss(self.fc1_bias_mu, self.logvar1, self.prior_var) \
            + kl_gauss(self.fc2_mu, self.logvar2, self.prior_var) + kl_gauss(self.fc2_bias_mu, self.logvar2, self.prior_var)
        return k

# --- è®­ç»ƒä¸æ¯”è¾ƒå‡½æ•° ---
def mse(a, b): return ((a-b)**2).mean()

def train_plain(epochs=200, lr=0.02):
    model = MLP().to(device)
    opt = optim.Adam(model.parameters(), lr=lr)
    for ep in range(epochs):
        pred = model(x_train)
        loss = mse(pred, y_train)
        opt.zero_grad(); loss.backward(); opt.step()
    return model, mse(model(x_val), y_val).item()

def train_wdecay(epochs=200, lr=0.02, weight_decay=1e-3):
    model = MLP().to(device)
    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    for ep in range(epochs):
        pred = model(x_train)
        loss = mse(pred, y_train)
        opt.zero_grad(); loss.backward(); opt.step()
    return model, mse(model(x_val), y_val).item()

def train_variational(epochs=400, lr=0.02, mc_samples=1, beta=1.0):
    # mc_samples ç”¨äºè¿‘ä¼¼æœŸæœ›æ•°æ®è¯¯å·®ï¼ˆé€šå¸¸ 1 æˆ– 2 å°±å¤Ÿ toy ç¤ºèŒƒï¼‰
    model = VariationalMLP().to(device)
    opt = optim.Adam(model.parameters(), lr=lr)
    for ep in range(epochs):
        loss_mse = 0.0
        for _ in range(mc_samples):
            pred = model.forward_with_sample(x_train)
            loss_mse = loss_mse + mse(pred, y_train)
        loss_mse = loss_mse / float(mc_samples)
        kl = model.kl_term() / x_train.shape[0]   # å¹³å‡åˆ°æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä¸Šï¼Œscale æ›´åˆç†
        loss = loss_mse + beta * kl
        opt.zero_grad(); loss.backward(); opt.step()
    val_mse = mse(model.forward_with_sample(x_val), y_val).item()
    std1 = float(torch.exp(0.5*model.logvar1).item())
    std2 = float(torch.exp(0.5*model.logvar2).item())
    return model, val_mse, std1, std2

# --- è¿è¡Œå®éªŒï¼ˆæœ¬åœ°è¿è¡Œï¼‰ ---
if __name__ == "__main__":
    plain_model, plain_val = train_plain()
    wdec_model, wdec_val = train_wdecay(weight_decay=5e-4)
    var_model, var_val, s1, s2 = train_variational(beta=1.0)
    print("Validation MSEs: plain=%.4f, weight-decay=%.4f, variational=%.4f" % (plain_val, wdec_val, var_val))
    print("Variational learned stds: layer1=%.4f, layer2=%.4f" % (s1, s2))

    # ï¼ˆå¯é€‰ï¼‰ç”»ä¸€ä¸‹ä¸‰ç§æ¨¡å‹åœ¨åŒºé—´ä¸Šçš„æ‹Ÿåˆæ›²çº¿ä»¥ç›´è§‚å¯¹æ¯”
    import matplotlib.pyplot as plt
    xs = torch.linspace(-4,4,400).unsqueeze(1)
    with torch.no_grad():
        y_plain = plain_model(xs).cpu().numpy()
        y_wdec = wdec_model(xs).cpu().numpy()
        # å¯¹å˜åˆ†æ¨¡å‹ç”¨å¤šæ¬¡é‡‡æ ·å¹³å‡å¾—åˆ°é¢„æµ‹å¹³å‡
        preds = []
        for _ in range(30):
            preds.append(var_model.forward_with_sample(xs).cpu().numpy())
        y_var = sum(preds)/len(preds)

    plt.scatter(x_val.cpu().numpy(), y_val.cpu().numpy(), s=6, label="val data", alpha=0.6)
    plt.plot(xs.cpu().numpy(), y_plain, label="plain")
    plt.plot(xs.cpu().numpy(), y_wdec, label="weight-decay")
    plt.plot(xs.cpu().numpy(), y_var, label="variational (mean of samples)")
    plt.legend(); plt.show()

```

è¾“å‡ºè§£è¯»ï¼š

- `Validation MSEs`: ä¸‰ç§æ–¹æ³•çš„éªŒè¯è¯¯å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼š
  - plain å¯èƒ½æ‹Ÿåˆè®­ç»ƒæ•°æ®ä½†æ³›åŒ–ä¸å¥½ï¼ˆè§†ç§å­ä¸è¶…å‚è€Œå®šï¼‰ã€‚
  - weight-decay å¾€å¾€é™ä½ overfittingï¼ˆå°¤å…¶åœ¨æ•°æ®å°‘æ—¶ï¼‰ã€‚
  - variational æ–¹æ³•ï¼ˆè®ºæ–‡ä¸»å¼ çš„â€œæœ€å°åŒ–æƒé‡æè¿°é•¿åº¦â€ï¼‰ä¼šå­¦åˆ°æŸäº›å±‚/æŸäº›æƒé‡å…·æœ‰**æ›´å¤§åéªŒæ–¹å·®**ï¼ˆæ›´ç²—ç³™åœ°ç¼–ç ï¼‰ï¼Œä»è€ŒèŠ‚çœç¼–ç ä½å¹¶åœ¨æ³›åŒ–ä¸Šä¼˜äº plainï¼Œæœ‰æ—¶ä¼˜äºå›ºå®š L2ã€‚
- `Variational learned stds`: å¦‚æœæŸä¸€å±‚æˆ–åç½®è¢«å…è®¸æ›´å¤§ stdï¼Œè¯´æ˜è¯¥å±‚çš„å‚æ•°åœ¨ç¼–ç æ—¶å¯ä»¥æ›´ç²—ç³™ï¼ˆå³è¿™äº›å‚æ•°å¯¹è¾“å‡ºä¸æ•æ„Ÿæˆ–å†—ä½™ï¼‰ï¼›åä¹‹ std å¾ˆå°è¯´æ˜å¿…é¡»ç²¾ç¡®ç¼–ç ã€‚

ä¸è®ºæ–‡çš„å¯¹åº”å…³ç³»ï¼ˆæŠŠä»£ç å’Œå…¬å¼å¯¹åº”èµ·æ¥ï¼‰ï¼š

- ä»£ç ä¸­çš„ `loss_mse` å°±æ˜¯æœŸæœ›æ•°æ®è¯¯å·®é¡¹ï¼ˆè®ºæ–‡ä¸­ data-misfitï¼ŒGaussian ç¼–ç å¯¹åº” MSEï¼‰ã€‚
- `kl_term()` å¯¹åº”ç¼–ç æˆæœ¬ï¼ˆè®ºæ–‡ä¸­ç”¨éå¯¹ç§°æ•£åº¦æˆ– KL æ¥è¡¡é‡ä»å…ˆéªŒ P åˆ° posterior Q çš„é¢å¤–ä½æ•°ï¼‰ï¼Œä¹Ÿå°±æ˜¯æƒé‡çš„â€œæè¿°é•¿åº¦â€é¡¹ã€‚è®ºæ–‡åœ¨ 5 èŠ‚é‡Œè¯¦ç»†æ¨å¯¼äº†è¿™äº›å½¢å¼ã€‚
- `beta` å¯ä»¥çœ‹æˆå¯¹ KL çš„æƒé‡ï¼ˆåœ¨ MDL è¯­å¢ƒä¸‹å®ƒå’Œå¦‚ä½•æŠŠæ€»ä½æ•°åˆ†é…åˆ°æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä¸Šæœ‰å…³ï¼‰ï¼Œæ”¹å˜å®ƒå¯ä»¥è°ƒèŠ‚â€œåçˆ±æ›´ç®€å•æ¨¡å‹â€è¿˜æ˜¯â€œåçˆ±æ›´ç²¾ç¡®æ‹Ÿåˆâ€ã€‚



## 2017.01 Pointer Networks

### æ¦‚è¿°

------

æ¦‚è¿°1ï¼š

- Authors: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly
- The paper â€œPointer Networksâ€ introduces **a novel neural architecture** designed to **learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence**. This model, called Pointer Networks (Ptr-Nets), **addresses** the limitation of existing sequence-to-sequence models and Neural Turing Machines, which **struggle with variable-sized output dictionaries**. Ptr-Nets leverage a neural attention mechanism to **select members of the input sequence as the output**, making them particularly effective for problems **such as sorting variable-sized sequences and various combinatorial optimization tasks**.
- **Key Contributions:**
  - The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.
  - Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.
  - The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.
- **Models:**
  - **Sequence-to-Sequence Model:** This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.
  - **Content Based Input Attention:** An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.
  - **Pointer Networks (Ptr-Net):** Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.
- **Empirical Results:**
  - **Convex Hull:** Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.
  - **Delaunay Triangulation:** Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.
  - **Travelling Salesman Problem (TSP):** Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.
- **Conclusion:**
  - The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.

------

æ¦‚è¿°2ï¼š

Pointer Networksï¼ˆPtr-Netï¼‰æ˜¯æŠŠæ³¨æ„åŠ›æœºåˆ¶ï¼ˆattentionï¼‰**ä¸æ˜¯ç”¨æ¥åšåŠ æƒæ±‚å’Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡**ï¼Œè€Œæ˜¯ç›´æ¥æŠŠæ³¨æ„åŠ›çš„ softmax åˆ†å¸ƒå½“ä½œâ€œæŒ‡é’ˆâ€â€”â€”å³æŠŠè¾“å‡ºè¯æ±‡è¡¨æ›¿æ¢ä¸ºâ€œè¾“å…¥åºåˆ—çš„ä½ç½®â€ï¼Œä»è€Œå…è®¸è¾“å‡ºå­—å…¸å¤§å°éšè¾“å…¥é•¿åº¦å˜åŒ–ï¼ˆé€‚åˆæ’åºã€å‡¸åŒ…ã€TSP ç­‰é—®é¢˜ï¼‰ã€‚è®ºæ–‡é‡Œç”¨ LSTM ç¼–ç å™¨/è§£ç å™¨ + attention-as-pointerï¼Œå¹¶åœ¨è‹¥å¹²å‡ ä½•ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸Šåšäº†å®éªŒï¼Œä¸”æŠ¥å‘Šäº†èƒ½åœ¨æœªè§è¿‡çš„æ›´é•¿åºåˆ—ä¸Šæ³›åŒ–çš„ç°è±¡ã€‚



### ç¤ºä¾‹ä»£ç 

è¯¥è„šæœ¬å¹¶ä¸è®­ç»ƒ Pointer-Netï¼Œè€Œæ˜¯ç”¨æç®€ã€å¯è¯»çš„æ–¹å¼æ¨¡æ‹Ÿè®ºæ–‡ä¸­â€œæ³¨æ„åŠ›ä½œä¸ºæŒ‡é’ˆâ€çš„å‰å‘è¿‡ç¨‹ï¼Œå¹¶ç”¨å‡¸åŒ…é—®é¢˜æ¼”ç¤ºè¾“å…¥ä½ç½®å¦‚ä½•è¢«æŒ‡å‘ã€‚è¿è¡Œåä¼šæ˜¾ç¤ºä¸¤å¹…å›¾ï¼šç‚¹ä¸é€‰æ‹©åºåˆ—ï¼›ä»¥åŠæ¯ä¸ªè§£ç æ­¥å¯¹è¾“å…¥çš„ attention åˆ†å¸ƒï¼ˆsoftmaxï¼‰ã€‚

```python
# pointer_demo.py
# è¿è¡Œç¯å¢ƒ: Python 3.x, éœ€è¦ numpy å’Œ matplotlib
import numpy as np
import matplotlib.pyplot as plt

def monotone_chain(points):
    """Andrew's monotone chain convex hull.
    Returns indices of points forming the hull in CCW order starting from leftmost lowest.
    """
    pts = np.asarray(points)
    N = len(pts)
    # sort by x then y
    order = np.lexsort((pts[:,1], pts[:,0]))
    pts_sorted = pts[order]

    def cross(o, a, b):
        return (a[0]-o[0])*(b[1]-o[1]) - (a[1]-o[1])*(b[0]-o[0])

    lower = []
    for i in range(N):
        while len(lower) >= 2 and cross(pts_sorted[lower[-2]], pts_sorted[lower[-1]], pts_sorted[i]) <= 0:
            lower.pop()
        lower.append(i)
    upper = []
    for i in range(N-1, -1, -1):
        while len(upper) >= 2 and cross(pts_sorted[upper[-2]], pts_sorted[upper[-1]], pts_sorted[i]) <= 0:
            upper.pop()
        upper.append(i)
    hull_idx_sorted = lower[:-1] + upper[:-1]
    hull_idxs = order[hull_idx_sorted]
    return list(hull_idxs)

def pointer_step(encoder_outputs, decoder_state, W_enc, W_dec, v):
    """
    encoder_outputs: (N, H)
    decoder_state: (H,)
    score_i = v^T tanh(W_enc * enc_i + W_dec * dec)
    returns softmax probs over N inputs
    """
    enc_term = encoder_outputs @ W_enc.T   # (N, H)
    dec_term = (W_dec @ decoder_state)    # (H,)
    scores = np.tanh(enc_term + dec_term) @ v   # (N,)
    exp = np.exp(scores - np.max(scores))
    probs = exp / np.sum(exp)
    return probs

def demo(N=20, H=64, seed=42):
    np.random.seed(seed)
    points = np.random.rand(N, 2)
    hull = monotone_chain(points)

    # encoder outputs: embed 2D coords to H-dim via linear + tanh
    rng = np.random.RandomState(seed+1)
    W_embed = rng.normal(scale=0.5, size=(H, 2))
    encoder_outputs = np.tanh(points @ W_embed.T)  # (N, H)

    # toy pointer weights (random)
    W_enc = rng.normal(scale=0.1, size=(H, H))
    W_dec = rng.normal(scale=0.1, size=(H, H))
    v = rng.normal(scale=0.1, size=(H,))

    decoder_state = np.mean(encoder_outputs, axis=0)

    steps = max(1, len(hull))
    attention_matrix = np.zeros((steps, N))
    selected_sequence = []

    for t in range(steps):
        probs = pointer_step(encoder_outputs, decoder_state, W_enc, W_dec, v)
        attention_matrix[t] = probs
        pick = int(np.argmax(probs))
        selected_sequence.append(pick)
        decoder_state = encoder_outputs[pick]  # feed chosen encoder vector next step (as paper suggests)

    # Plot
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))
    ax1.scatter(points[:,0], points[:,1])
    for i, (x,y) in enumerate(points):
        ax1.text(x, y, str(i), fontsize=9, va='bottom', ha='right')
    if len(hull) >= 2:
        hull_pts = points[hull + [hull[0]]]
        ax1.plot(hull_pts[:,0], hull_pts[:,1], linestyle='--', linewidth=1, label='True Hull')
    seq_pts = points[selected_sequence]
    ax1.plot(seq_pts[:,0], seq_pts[:,1], marker='o', linewidth=1.5, label='Pointer sequence')
    ax1.set_title('Points, true convex hull (dashed), and toy pointer-selected sequence')
    ax1.set_xlabel('x'); ax1.set_ylabel('y')
    ax1.legend()

    im = ax2.imshow(attention_matrix, aspect='auto', origin='lower')
    ax2.set_xlabel('Input index (position in sequence)')
    ax2.set_ylabel('Decoder step')
    ax2.set_title('Attention (softmax) over input positions â€” "pointer" distribution')
    fig.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)

    plt.tight_layout()
    plt.show()

    print("True hull indices (in sorted/ordered form):", hull)
    print("Toy pointer selected indices:", selected_sequence)

if __name__ == "__main__":
    demo()

```

è¡¥å……ï¼š

<img src="./assets/image-20251128094037197.png" alt="image-20251128094037197" style="zoom:80%;" />



### QA

#### **1. Pointer Network æœ€æ ¸å¿ƒçš„åˆ›æ–°æ˜¯ä»€ä¹ˆï¼Ÿ**

**ç­”ï¼š**
 ä¼ ç»Ÿ seq2seq æ¨¡å‹çš„è¾“å‡ºè¯å…¸æ˜¯å›ºå®šå¤§å°çš„ï¼Œè€Œåƒ**æ’åºã€å‡¸åŒ…ã€TSP**è¿™ç±»ä»»åŠ¡çš„è¾“å‡ºéœ€è¦ä»è¾“å…¥åºåˆ—ã€ŒæŒ‡å‘ã€æŸäº›ä½ç½®ï¼Œè¾“å‡ºè¯å…¸å¤§å°å¿…é¡»éšè¾“å…¥é•¿åº¦å˜åŒ–ã€‚

Ptr-Net çš„åˆ›æ–°æ˜¯ï¼š

> **ä½¿ç”¨æ³¨æ„åŠ›åˆ†å¸ƒï¼ˆattention softmaxï¼‰æœ¬èº«ä½œä¸ºæŒ‡å‘è¾“å…¥åºåˆ—ä¸­æŸä¸€ä¸ªä½ç½®çš„æŒ‡é’ˆã€‚**

ä¹Ÿå°±æ˜¯è¯´ï¼š

- attention = ä¸€ä¸ª N ç»´æ¦‚ç‡åˆ†å¸ƒ
- N = è¾“å…¥åºåˆ—é•¿åº¦
- argmax â†’ è¾“å‡ºä½ç½® index

å› æ­¤è¾“å‡ºå­—å…¸å¤§å° = è¾“å…¥é•¿åº¦ï¼Œå®ç°è¾“å‡ºè¯è¡¨åŠ¨æ€æ‰©å±•ã€‚

è¿™æ˜¯è®ºæ–‡çš„å…³é”®çªç ´ï¼Œä½¿ seq2seq æ¨¡å‹å¯ä»¥è§£å†³ä¸€äº›ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚
 è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®ƒå« Pointer Networkã€‚

------

#### **2. Pointer Net ä¸æ™®é€š Attention seq2seq çš„æ ¹æœ¬åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

| æ¨¡å‹               | Attention çš„ç”¨é€”                                         |
| ------------------ | -------------------------------------------------------- |
| **æ™®é€š attention** | äº§ç”Ÿ context vectorï¼ˆåŠ æƒæ±‚å’Œï¼‰                          |
| **Pointer Net**    | **ç›´æ¥å¯¹è¾“å…¥ä½ç½®åš softmax**ï¼Œä½œä¸ºè¾“å‡ºçš„ predicted index |

æ™®é€š attentionï¼š

> ç”¨æ¥è¯»ä¿¡æ¯ï¼ˆcontextï¼‰

Pointer Netï¼š

> ç”¨æ¥è¾“å‡ºä½ç½®ï¼ˆindexï¼‰ï¼Œæ›¿ä»£å›ºå®šè¯æ±‡è¡¨

å› æ­¤ Pointer Net æ˜¯ *attention as output*ï¼Œä¸æ˜¯ *attention as context*ã€‚

------

#### **3. ä¸ºä»€ä¹ˆ Pointer Net å°¤å…¶é€‚åˆå‡ ä½•/ç»„åˆä¼˜åŒ–ä»»åŠ¡ï¼Ÿ**

å› ä¸ºè¿™äº›ä»»åŠ¡çš„è¾“å‡ºéƒ½æ˜¯ï¼š

- è¾“å…¥åºåˆ—ä¸­ç‚¹çš„**æ¬¡åº**ï¼ˆæ’åºï¼‰
- è¾“å…¥ç‚¹çš„**å­é›†**ï¼ˆå‡¸åŒ…ï¼‰
- è¾“å…¥åŸå¸‚çš„**è·¯å¾„é¡ºåº**ï¼ˆTSPï¼‰

è¿™äº›é—®é¢˜çš„å…±åŒç‚¹æ˜¯ï¼š

> **è¾“å‡ºæ˜¯è¾“å…¥åºåˆ—çš„æ’åˆ—æˆ–å­åºåˆ—ã€‚**

è€Œå›ºå®šè¯å…¸ seq2seq æ— æ³•åšåˆ°äº§å‡ºé•¿åº¦ä¸º N çš„è¯å…¸ã€‚
 Ptr-Net å®Œå…¨åŒ¹é…è¿™ç§è¾“å‡ºç»“æ„ã€‚

------

#### **4. Pointer Net å¦‚ä½•è®­ç»ƒï¼ŸæŸå¤±å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿ**

**å’Œæ™®é€š seq2seq ä¸€æ ·ï¼Œä½¿ç”¨äº¤å‰ç†µã€‚**

æ¯ä¸€ä¸ªè¾“å‡ºæ­¥ tï¼š

- target æ˜¯è¾“å…¥ä½ç½® target_index[t]
- æ¨¡å‹è¾“å‡ºçš„æ˜¯ä¸€ä¸ª softmax attention åˆ†å¸ƒ over N inputs

æ‰€ä»¥ lossï¼š
$$
L = -\sum_{t} \log p(\text{target\_index}_t)
$$
è®­ç»ƒæ—¶ï¼š

- ä½¿ç”¨ teacher forcingï¼ˆä¸Šä¸€æ—¶åˆ»çš„çœŸå®ä½ç½® feedingï¼‰
- decoder æ˜¯ LSTMï¼ˆè®ºæ–‡ä¸­å¦‚æ­¤å®ç°ï¼‰

æŸå¤±éå¸¸ç®€å•ã€‚

------

#### **5. Pointer Net çš„è®¡ç®—å¤æ‚åº¦å¦‚ä½•ï¼Ÿä¸ºä»€ä¹ˆæ¯”ä¹‹å‰æ–¹æ³•æ›´ä¼˜ï¼Ÿ**

åœ¨æ¯ä¸ª decoder ç”Ÿæˆæ­¥ï¼š

- attention over N inputs â†’ O(N)

æ€»å…± T æ­¥ï¼ˆTâ‰ˆNï¼‰ï¼Œå› æ­¤ï¼š

- **æ€»å¤æ‚åº¦ = O(NÂ²)**

è®ºæ–‡æŒ‡å‡ºï¼š

- ä¼ ç»Ÿè®¡ç®—å‡¸åŒ…æˆ–æ’åºç­‰ä»»åŠ¡å¤æ‚åº¦é€šå¸¸ä¸º O(N log N)
- ä½†æ˜¯ç¥ç»æ¨¡å‹ O(NÂ²) åœ¨ GPU ä¸Šå¹¶ä¸æ…¢
- åŒæ—¶å¯ä»¥æ³›åŒ–åˆ°æ–°è§„æ¨¡

å…¶ä¼˜åŠ¿ä¸æ˜¯ç†è®ºå¤æ‚åº¦ï¼Œè€Œæ˜¯ï¼š

> èƒ½è®©æ·±åº¦å­¦ä¹  end-to-end å­¦ä¹ ç»„åˆç»“æ„ã€‚

------

#### **6. Pointer Net èƒ½æ³›åŒ–åˆ°æ›´é•¿åºåˆ—å—ï¼Ÿè®ºæ–‡æœ‰ä»€ä¹ˆå‘ç°ï¼Ÿ**

è®ºæ–‡ä¸­çš„å®éªŒæ˜¾ç¤ºä¸€ä¸ªç‰¹æ€§ï¼š

> æ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ï¼ˆä¾‹å¦‚è®­ç»ƒ n<=50ï¼Œåœ¨ n=100 æˆ– n=300 ä¸Šä»æœ‰ä¸é”™è¡¨ç°ï¼‰ã€‚

å°¤å…¶æ˜¯åœ¨ï¼š

- **å‡¸åŒ…ï¼ˆConvex Hullï¼‰ä»»åŠ¡**ï¼šæ³›åŒ–éå¸¸å¥½
- **Delaunay triangulation**ï¼šä¹Ÿèƒ½æ³›åŒ–
- **TSPï¼ˆè¿‘ä¼¼ï¼‰**ï¼šæ³›åŒ–ä½†è¯¯å·®éšç€ n å¢åŠ 

åŸå› åœ¨äº pointer æœºåˆ¶ä¸è¾“å…¥é•¿åº¦æ— å…³ï¼Œå› æ­¤èƒ½è‡ªç„¶æ‰©å±•åˆ°æ–°çš„ nã€‚

------

#### **7. ä¸ºä»€ä¹ˆ TSP ç²¾åº¦æ¯”å‡¸åŒ…ä½ï¼Ÿ**

å› ä¸ºï¼š

- å‡¸åŒ…ç»“æ„æ›´åŠ **å±€éƒ¨æ€§å¼ºã€ç¡®å®šæ€§å¼º**
- TSP æ˜¯ NP-hardï¼Œå…¨å±€æœ€ä¼˜è·¯å¾„éš¾å¯»
- TSP çš„è¾“å‡ºé¡ºåºä¾èµ–éå±€éƒ¨ä¿¡æ¯ï¼ˆå…¨å±€è·ç¦»ç»“æ„ï¼‰

è®ºæ–‡å‘ç°ï¼š

- Ptr-Net åœ¨å‡¸åŒ…ä¸Šå¯ä»¥è¾¾åˆ°**å‡ ä¹å®Œç¾çš„ F-score**
- ä½†åœ¨ TSP ä¸Šåªèƒ½ç»™å‡º**ä¸é”™çš„è¿‘ä¼¼è§£**ï¼ˆè¯¯å·® 1ï½2%ï¼‰
- æ›´é•¿ n ä¼šé¢å¤–é™ä½å‡†ç¡®åº¦

è¿™ä¸ä»»åŠ¡éš¾åº¦æœ‰å…³ã€‚

------

#### **8. Pointer Net éœ€è¦æ˜¾å¼é˜²æ­¢è¾“å‡ºé‡å¤èŠ‚ç‚¹å—ï¼Ÿ**

è®ºæ–‡ä¸­ä½¿ç”¨ **teacher forcing â‡’ ä¸éœ€è¦æ˜¾å¼çº¦æŸ**ã€‚
 ä½†æ¨ç†æ—¶å¯å‡ºç°é‡å¤èŠ‚ç‚¹ï¼Œå› æ­¤é€šå¸¸åšæ³•æ˜¯ï¼š

- ç¦æ­¢æ¨¡å‹å†æŒ‡å‘å·²ç»é€‰è¿‡çš„ç‚¹ï¼ˆåœ¨ inference æ—¶ mask softmaxï¼‰
- æˆ–è€…è´ªå¿ƒé€‰å®Œè‡ªåŠ¨æ’é™¤å·²é€‰å…ƒç´ 

è®ºæ–‡åœ¨ TSP ä¸­ä¹Ÿé‡‡ç”¨è¿‡ç±»ä¼¼æŠ€å·§ã€‚

------

#### **9. Pointer Net çš„è¾“å…¥é¡ºåºæ˜¯å¦ä¼šå½±å“æ€§èƒ½ï¼Ÿ**

æ˜¯çš„ã€‚
 ç”±äº LSTM ç¼–ç å™¨åºåˆ—åŒ–è¾“å…¥ï¼Œè¾“å…¥é¡ºåºä¼šå½±å“ encoder hidden statesã€‚

ä¾‹å¦‚åœ¨å‡¸åŒ…ä»»åŠ¡ä¸­ï¼š

- éšæœºé¡ºåº â†’ æ¨¡å‹è¡¨ç°å¯èƒ½ä¸‹é™
- å›ºå®šæŒ‰åæ ‡æ’åºè¾“å…¥ â†’ æ¨¡å‹æ›´ç¨³å®š

ä½†è®ºæ–‡å¼ºè°ƒï¼š

> Ptr-Net çš„å­¦ä¹ èƒ½åŠ›è¶³ä»¥é€‚åº”éšæœºè¾“å…¥é¡ºåºã€‚

------

#### **10. Pointer Net æ˜¯å¦å·²è¢«æ›¿ä»£ï¼Ÿç°åœ¨è¿˜å¸¸ç”¨å—ï¼Ÿ**

Pointer Net çš„æ€æƒ³ä»éå¸¸é‡è¦ï¼Œä½†å…¶å…·ä½“æ¨¡å‹å·²è¢«æ›´å¤šç°ä»£æ–¹æ³•æ›¿ä»£ï¼š

##### å¼ºåŠ¿æ›¿ä»£è€…

- **Transformers + Pointer / Copy Mechanism**
- **Graph Neural Networks + Attention**
- **Neural Combinatorial Optimization (2017+)**
- **Reinforcement Learning for TSP / OR-Tools Neural åŸºçº¿**

ç°åœ¨å‡ ä¹ä¸å†ä½¿ç”¨åŸºäº LSTM çš„ Pointer Netï¼Œä½†å®ƒçš„æ€æƒ³ï¼š

##### ä»å¹¿æ³›ä½¿ç”¨ï¼š

- æ–‡æœ¬æ‘˜è¦ / æœºå™¨ç¿»è¯‘ä¸­çš„ copy mechanismï¼ˆCopyNet, Pointer Generatorï¼‰
- ç»“æ„é¢„æµ‹ï¼ˆParsingï¼‰
- å›¾æ¨¡å‹çš„ node selection

Pointer Net æ˜¯æ‰€æœ‰ pointer/copy ç±»æ¨¡å‹çš„å¥ åŸºå·¥ä½œã€‚



## 2012 AlexNet

è®ºæ–‡åœ°å€ï¼š [ImageNet Classification with Deep Convolutional Neural Networksï¼ˆé€šå¸¸ä¹Ÿç§°ä¸º AlexNetï¼‰](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

### æ¦‚è¿°

> å¯ä»¥ç»“åˆæ²ç¥[ã€ŠåŠ¨æ‰‹å­¦ä¹ æ·±åº¦å­¦ä¹ ã€‹](https://zh.d2l.ai/chapter_convolutional-modern/index.html)ä¸­ï¼ŒæŒ‰æ—¶é—´/å‘å±•é¡ºåºè®²è§£LeNetã€AlexNetã€VGGã€NiNã€GoogleNetã€ResNetã€DenseNetï¼Œä¼šæ›´å¥½ç†è§£ä¸€äº›

<img src="./assets/image-20251128095508382.png" alt="image-20251128095508382" style="zoom:80%;" />



### ç¤ºä¾‹ä»£ç 

ä¸‹é¢æ˜¯ä¸€ä¸ªéå¸¸ç®€åŒ– (toy) çš„ â€œAlexNet-likeâ€ ç½‘ç»œç»“æ„ (å¹¶ä¸å®Œå…¨ä¸€æ ·)ï¼Œç”¨ PyTorch å†™ï¼Œå¸®åŠ©ä½ ç†è§£ â€œ5 å±‚å·ç§¯ + å‡ ä¸ªæŠ€å·§â€ æ˜¯å¦‚ä½•ç”¨ä»£ç å®ç°çš„ã€‚

```
# è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆ AlexNet (PyTorch) ç¤ºä¾‹ï¼Œä»…ä¾›å­¦ä¹ ç†è§£
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleAlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(SimpleAlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),  # conv1
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),                  # pool1
            # å¯åŠ  LRNï¼Œä½† PyTorch ä¸­æ²¡æœ‰å†…å»ºï¼›è¿™é‡Œçœç•¥ / ç”¨ BatchNorm ä»£æ›¿
            nn.Conv2d(64, 192, kernel_size=5, padding=2),          # conv2
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),                 # pool2
            nn.Conv2d(192, 384, kernel_size=3, padding=1),         # conv3
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),         # conv4
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),         # conv5
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),                 # pool5
        )
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),
            nn.Linear(256 * 6 * 6, 4096),  # å‡è®¾è¾“å…¥å›¾åƒ crop/pad åˆ° 224x224
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 6 * 6)
        x = self.classifier(x)
        return x

# ä½¿ç”¨æ–¹å¼
if __name__ == "__main__":
    model = SimpleAlexNet(num_classes=1000)
    dummy = torch.randn(1, 3, 224, 224)  # æ¨¡æ‹Ÿä¸€å¼  224x224 RGB å›¾åƒ
    out = model(dummy)
    print(out.shape)  # åº”è¯¥æ˜¯ [1, 1000]

```

è¯´æ˜ / å¦‚ä½•å¯¹åº”è®ºæ–‡ä¸­çš„è®¾è®¡ï¼š

- `Conv2d + ReLU` å¯¹åº”è®ºæ–‡çš„å·ç§¯å±‚ + ReLU æ¿€æ´»ã€‚
- `MaxPool2d` å¯¹åº”æ± åŒ– (pooling) å±‚ (è™½ç„¶æ˜¯éé‡å æ± åŒ–ï¼Œä½†é€»è¾‘ç±»ä¼¼)ï¼›å¦‚æœä½ æƒ³æ›´è´´è¿‘è®ºæ–‡ï¼Œä¹Ÿå¯ä»¥å®ç° overlapping pool (pool size > stride)ï¼›
- `Dropout(p=0.5)` å¯¹åº”è®ºæ–‡ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆçš„ â€œdropoutâ€ æŠ€æœ¯ã€‚
- è¾“å…¥å°ºå¯¸å‡è®¾ä¸º 224Ã—224ï¼Œè¿™æ˜¯ç°ä»£æ ‡å‡† (ç°ä»£å¾ˆå¤šå®ç°æŠŠåŸè®ºæ–‡çš„ 256Ã—256 crop / central-crop + random crop / flip / resize ç­‰ data augmentation åˆå¹¶æˆ â€œ224Ã—224 + augâ€ pipeline)ã€‚
- è¾“å‡ºæ˜¯ `num_classes` (é€šå¸¸ 1000) â€” å¯¹åº” ImageNet 1000 ç±»åˆ†ç±»ä»»åŠ¡ã€‚



### QA

| é—®é¢˜ (Question)                                              | ç­”æ¡ˆ / æç¤º (Answer / Hint)                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **ä¸ºä»€ä¹ˆè¦ç”¨ ReLUï¼Ÿä¸èƒ½ç”¨ tanh / sigmoid å—ï¼Ÿ**              | ReLU æ˜¯éé¥±å’Œ (non-saturating) çš„ï¼šç›¸æ¯” tanh/sigmoidï¼Œå¯ä»¥è®©ç½‘ç»œè®­ç»ƒæ›´å¿«ã€æ›´æ·±ï¼›è®ºæ–‡å®éªŒè¡¨æ˜ï¼Œç”¨ ReLU çš„ CNN æ”¶æ•›é€Ÿåº¦æ˜¯ saturating nonlinearities çš„æ•°å€ã€‚ ([NeurIPS ä¼šè®®è®°å½•](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) |
| **ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®å¢å¼º (data augmentation)ï¼Ÿ**                 | å°½ç®¡æœ‰ç™¾ä¸‡çº§æ•°æ®ï¼Œä½†å¯¹äºä¸€ä¸ªæœ‰åƒä¸‡çº§ /äº¿çº§å‚æ•° (æˆ–è¯´å¤§é‡è‡ªç”±åº¦) çš„æ·±ç½‘ç»œæ¥è¯´ï¼Œä¾ç„¶å¯èƒ½è¿‡æ‹Ÿåˆã€‚æ•°æ®å¢å¼º (å¦‚éšæœºç¿»è½¬ã€éšæœºè£å‰ªã€é¢œè‰²æ‰°åŠ¨) èƒ½äººä¸ºâ€œæ”¾å¤§â€è®­ç»ƒé›†ã€å¢åŠ å˜å¼‚æ€§ï¼Œä»è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡æ­£æ˜¯é€šè¿‡è¿™ç§æ–¹å¼ç¼“è§£è¿‡æ‹Ÿåˆã€‚ ([Alex Hex](https://alexhex7.github.io/2017/09/29/ImageNet classification with deep convolutional neural networks/?utm_source=chatgpt.com)) |
| **Dropout æ˜¯åšä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆåªåœ¨å…¨è¿æ¥å±‚ç”¨ï¼Ÿ**                 | Dropout æ˜¯ä¸€ç§éšæœºæ­£åˆ™åŒ–æ–¹æ³•ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥ä¸€å®šæ¦‚ç‡ (è®ºæ–‡ç”¨ p=0.5) éšæœºâ€œå±è”½ (ç½®é›¶)â€ç¥ç»å…ƒçš„è¾“å‡ºï¼Œä»è€Œé¿å…ç½‘ç»œè¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šç¥ç»å…ƒã€å‡è½»è¿‡æ‹Ÿåˆã€‚ä½œè€…ä¸»è¦åœ¨å…¨è¿æ¥å±‚ä½¿ç”¨ï¼Œå› ä¸ºè¿™äº›å±‚å‚æ•°é‡å¤§ã€æœ€å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ ([NeurIPS ä¼šè®®è®°å½•](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) |
| **ä¸ºä»€ä¹ˆè¦ç”¨å¤š GPU è®­ç»ƒï¼Ÿ**                                  | å½“æ—¶å•å— GPU çš„æ˜¾å­˜ /è®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œè€Œ AlexNet çš„æ¨¡å‹ +å¤§æ•°æ®éå¸¸æ¶ˆè€—èµ„æºã€‚é€šè¿‡æŠŠç½‘ç»œåˆ†å¸ƒåœ¨ä¸¤å— GPU ä¸Š (model-parallel / data-parallel hybrid)ï¼Œå¯ä»¥åŒæ—¶å®¹çº³æ¨¡å‹ä¸æ•°æ®ï¼Œå¹¶ä¸”åŠ é€Ÿè®­ç»ƒã€‚ ([NeurIPS ä¼šè®®è®°å½•](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) |
| **Local Response Normalization (LRN) çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿç°ä»£ç½‘ç»œè¿˜ç”¨å—ï¼Ÿ** | LRN ç±»ä¼¼ç”Ÿç‰©ç¥ç»ç½‘ç»œé‡Œçš„ lateral inhibition â€” è®©æŸäº›å“åº”å¼º (â€œæ¿€æ´»â€) çš„ feature map å¯¹é™„è¿‘ map çš„å“åº”äº§ç”ŸæŠ‘åˆ¶ / ç«äº‰ï¼Œä»è€Œå¢å¼ºå·®å¼‚æ€§ / ç¨€ç–æ€§ï¼Œæœ‰åŠ©äºæ³›åŒ–ã€‚è®ºæ–‡æŒ‡å‡ºåŠ ä¸Š LRN å error rate æœ‰ä¸‹é™ã€‚ ([NeurIPS ä¼šè®®è®°å½•](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) ã€‚ä¸è¿‡åœ¨åæ¥çš„å¾ˆå¤šç°ä»£ç½‘ç»œ (å¦‚ VGG / ResNet / EfficientNet ç­‰) ä¸­ï¼ŒLRN è¢«æ›´æœ‰æ•ˆ (æ›´ç¨³å®š) çš„æœºåˆ¶ (å¦‚ BatchNorm / ç»„å½’ä¸€åŒ– /æ›´æ·±ç»“æ„) æ‰€æ›¿ä»£ã€‚ |
| **ä¸ºä»€ä¹ˆè®ºæ–‡ä¸­ convolutional layer å¾ˆå¤šï¼Œä½†å·ç§¯å±‚çš„å‚æ•°å æ¯” << å…¨è¿æ¥å±‚ï¼Ÿ** | è™½ç„¶ç½‘ç»œæœ‰ 5 å±‚å·ç§¯ + 3 å±‚å…¨è¿æ¥ï¼Œä½†å¾ˆå¤šå‚æ•°éƒ½é›†ä¸­åœ¨å…¨è¿æ¥å±‚ã€‚ä½œè€…åœ¨è®ºæ–‡ä¸­æåˆ°ï¼Œå¦‚æœå»æ‰ä»»ä½•ä¸€ä¸ªå·ç§¯å±‚ (å“ªæ€•å®ƒåªå æ•´ä¸ªå‚æ•°é‡ < 1%)ï¼Œéƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¯´æ˜å·ç§¯å±‚ (ç‰¹å¾æå–å±‚) çš„æ·±åº¦å’Œè¡¨è¾¾èƒ½åŠ›å¯¹æœ€ç»ˆåˆ†ç±»æ•ˆæœæä¸ºå…³é”®ã€‚ ([NeurIPS ä¼šè®®è®°å½•](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) |
| **è¾“å…¥å›¾åƒä¸ºä»€ä¹ˆç»Ÿä¸€åˆ°å›ºå®šå°ºå¯¸ (e.g. 256Ã—256 å†ä¸­å¿ƒ crop)?** | CNN è¦æ±‚å›ºå®šå°ºå¯¸è¾“å…¥ (å› ä¸ºå…¨è¿æ¥å±‚éœ€è¦å›ºå®šç»´åº¦)ï¼›è€ŒåŸ ImageNet çš„å›¾åƒåˆ†è¾¨ç‡ /é•¿å®½æ¯”ä¸ä¸€ã€‚è®ºæ–‡ä¸­ä»–ä»¬å…ˆæŠŠçŸ­è¾¹ç¼©æ”¾åˆ° 256ï¼Œç„¶åä»ä¸­å¿ƒè£å‰ª 256Ã—256ï¼Œå†åœ¨è®­ç»ƒæ—¶åšéšæœº crop / flip / color transform ä½œä¸ºæ•°æ®å¢å¼ºã€‚ ([NeurIPS ä¼šè®®è®°å½•](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) |
| **è¿™ä¸ªç½‘ç»œä¸ºä»€ä¹ˆåœ¨å½“æ—¶å¦‚æ­¤ â€œé©å‘½æ€§â€ï¼Ÿ**                      | åœ¨ 2012 å¹´ä¹‹å‰ï¼Œå›¾åƒåˆ†ç±» /è¯†åˆ«é¢†åŸŸå¤šç”¨æ‰‹å·¥è®¾è®¡ç‰¹å¾ (æ¯”å¦‚ SIFT / HOG / Bag-of-Visual-Words ç­‰) + æµ…å±‚åˆ†ç±»å™¨ (SVM, Softmax, shallow NNâ€¦)ã€‚è¿™ç¯‡è®ºæ–‡è¯æ˜ï¼šè‡ªåŠ¨å­¦ä¹  (æ·± CNN) + å¤§è§„æ¨¡æ•°æ® + GPU + è®­ç»ƒ/æ­£åˆ™æŠ€å·§ â€” èƒ½å–å¾—è¿œè¶…ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½ã€‚ä»æ­¤ â€œæ·±åº¦å­¦ä¹  (deep learning)â€ æˆä¸º computer vision çš„ä¸»æµã€‚ |



## 2016.02 Order Matters: Sequence to Sequence for Sets

è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/pdf/1511.06391



### æ¦‚è¿°

------

- Authors: Oriol Vinyals, Samy Bengio, Manjunath Kudlur
- The paper â€œOrder Matters: Sequence to Sequence for Setsâ€ explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.
- **Key Contributions:**
  - The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.
  - They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.
  - For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the modelâ€™s ability to generalize and perform accurately.
- **Experiments and Results:**
  - **Language Modeling:** The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the modelâ€™s accuracy.
  - **Combinatorial Problems:** The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.
  - **Graphical Models:** The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.
- **Model Architecture:**
  - **Read, Process, Write Model:** The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.
  - **Attention Mechanisms:** The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.
  - **Finding Optimal Orderings:** To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.
- **Conclusion:**
  - The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.

------

è®ºæ–‡æ¦‚è¿°ï¼ˆ3â€“6 å¥ï¼‰

- è¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼Œå½“æˆ‘ä»¬å°†â€œé›†åˆ (set)â€æ•°æ® â€” æ— è®ºæ˜¯è¾“å…¥ (input) è¿˜æ˜¯è¾“å‡º (output) â€” å¼ºè¡Œå½“ä½œåºåˆ— (sequence) æ¥å¤„ç† (å³ç”¨ç»å…¸çš„ seq2seq æ¡†æ¶)ï¼Œæ‰€é€‰çš„æ•°æ®é¡ºåº (ordering) å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚ä½œè€…é€šè¿‡ä¸€ç³»åˆ—ä»»åŠ¡ (sorting, å›¾æ¨¡å‹è”åˆåˆ†å¸ƒå»ºæ¨¡, parsing, è¯­è¨€å»ºæ¨¡ç­‰) å±•ç¤ºäº†â€œä¸æ°å½“é¡ºåºâ€ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œåˆé€‚é¡ºåºèƒ½æ˜¾è‘—æå‡æ•ˆæœã€‚ä¸ºäº†è§£å†³å¯¹â€œé¡ºåº (order)â€çš„ä¾èµ–ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§æ‰©å±• seq2seq çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿä»¥â€œé›†åˆ (set)â€ä¸ºè¾“å…¥ (invariant to permutation)ï¼Œå¹¶å¯¹è¾“å‡ºä¸ºé›†åˆ (set) çš„æƒ…å†µé€šè¿‡è®­ç»ƒæ—¶æœç´¢ (search) è¾“å‡ºé¡ºåºæ¥å­¦ä¹ æœ€ä¼˜åºåˆ—åŒ–æ–¹å¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ä¿®æ”¹åœ¨äººå·¥ä»»åŠ¡ (å¦‚æ’åº) å’ŒçœŸå®ä»»åŠ¡ (å¦‚ parsing) ä¸Šéƒ½æœ‰ç›Šã€‚

å…³é”®è´¡çŒ®è¦ç‚¹

- **æ­ç¤º â€œorder mattersâ€**ï¼šè¯æ˜å³ä¾¿æ•°æ®æœ¬è´¨æ˜¯æ— é¡ºåºçš„ (set)ï¼Œå°†å…¶è§†ä¸ºæœ‰åº (sequence) å¹¶é€‰å–ä¸åŒé¡ºåºï¼Œä¹Ÿä¼šå¯¹å­¦ä¹ ç»“æœ (æ”¶æ•›ã€æ€§èƒ½) äº§ç”Ÿå¾ˆå¤§å½±å“ã€‚[arXiv+1](https://arxiv.org/pdf/1511.06391?utm_source=chatgpt.com)
- **æå‡ºå¤„ç† input set çš„æœºåˆ¶**ï¼šè®¾è®¡äº† â€œRead-Process-Write (RPW)â€ æ¨¡å‹ â€” é€šè¿‡ attention + memory + LSTMï¼Œä½¿å¯¹è¾“å…¥é›†åˆ (unordered) çš„ç¼–ç å¯¹å…ƒç´ é¡ºåº (permutation) ä¸æ•æ„Ÿ (invariant) ã€‚[arXiv](https://arxiv.org/pdf/1511.06391?utm_source=chatgpt.com)
- **æå‡ºå¤„ç† output set çš„è®­ç»ƒæ–¹æ³•**ï¼šå¯¹äºè¾“å‡ºä¸ºé›†åˆ (æ— å›ºå®šé¡ºåº) çš„æƒ…å†µï¼Œå¼•å…¥ä¸€ç§è®­ç»ƒæ—¶å¯¹è¾“å‡ºé¡ºåºè¿›è¡Œæœç´¢ (search over possible orders) çš„æŸå¤± (loss)ï¼Œè®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©â€œå¯¹å®ƒæœ€æœ‰åˆ©â€çš„åºåˆ—åŒ–é¡ºåºã€‚[arXiv](https://arxiv.org/pdf/1511.06391?utm_source=chatgpt.com)
- **å¹¿æ³›å®éªŒè¯æ˜**ï¼šä¸ä»…åœ¨äººå·¥åˆæˆä»»åŠ¡ (ä¾‹å¦‚æ’åºã€ç»„åˆä¼˜åŒ– / å›¾æ¨¡å‹æ¦‚ç‡ä¼°è®¡)ï¼Œä¹Ÿåœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ (language modeling, parsing) ä¸Šï¼Œå±•ç¤ºé¡ºåºé€‰æ‹©ä¸ RPW çš„å®é™…æ•ˆç›Šã€‚[arXiv+1](https://arxiv.org/pdf/1511.06391?utm_source=chatgpt.com)

ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº

- éšç€ RNN / LSTM ç­‰åºåˆ—æ¨¡å‹çš„å‘å±•ï¼Œè®¸å¤šå¤æ‚ä»»åŠ¡ (æœºå™¨ç¿»è¯‘ã€å›¾åƒ captioningã€è§£æ (parsing)ã€ç»„åˆé—®é¢˜æ±‚è§£ç­‰) éƒ½è¢«å»ºæ¨¡ä¸ºåºåˆ—åˆ°åºåˆ— (seq2seq) çš„æ˜ å°„ã€‚[arXiv+1](https://arxiv.org/pdf/1511.06391?utm_source=chatgpt.com)
- ç„¶è€Œï¼Œæœ‰å¾ˆå¤šä»»åŠ¡çš„æ•°æ®å¹¶ä¸æ˜¯å¤©ç„¶æœ‰åº (ä¾‹å¦‚ä¸€ç»„æ•°å­—è¦æ’åºï¼Œä¸€ä¸ªå›¾åƒå¯èƒ½åŒ…å«ä¸€ç»„æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼Œä¸€ç»„éšæœºå˜é‡è¦ä¼°è®¡è”åˆåˆ†å¸ƒç­‰)ã€‚ç›´æ¥æŠŠè¿™äº›é›†åˆ (sets) è½¬æ¢æˆåºåˆ— (sequence) â€” æ¯”å¦‚éšæœºæ’åˆ—ï¼Œæˆ–æŒ‰æŸäººä¸ºè®¾å®šçš„å›ºå®šé¡ºåº â€” æ˜¯ä¸€ç§ä»»æ„ (arbitrary) çš„è®¾è®¡å†³ç­–ã€‚ä½œè€…æ€€ç–‘è¿™ç§éšæ„è®¾è®¡é¡ºåºå¯èƒ½å½±å“å­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆæ•ˆæœã€‚[arXiv+1](https://arxiv.org/pdf/1511.06391?utm_source=chatgpt.com)
- å› æ­¤ï¼Œå¿…è¦æ¢ç´¢ï¼š**æ˜¯å¦å¯ä»¥æ„å»ºä¸€ç§å¯¹é¡ºåºä¸æ•æ„Ÿ (permutation-invariant) çš„æ¶æ„**ï¼Œä½¿å…¶èƒ½â€œæ­£ç¡®â€å¤„ç†é›†åˆ (sets)ï¼› ä»¥åŠåœ¨è¾“å‡ºä¸ºé›†åˆ (sets) çš„æƒ…å¢ƒä¸‹ï¼Œ**èƒ½å¦è‡ªåŠ¨é€‰æ‹© (learn) ä¸€ä¸ªâ€œåˆé€‚â€çš„è¾“å‡ºåºåˆ—åŒ– (ordering)**ï¼Œè€Œä¸éœ€è¦äººå·¥æŒ‡å®šã€‚

æ–¹æ³•ç»†èŠ‚

- ç»å…¸ seq2seq å›é¡¾
- â€œRead-Process-Write (RPW)â€ æ¨¡å‹ for input sets
- å¯¹ output set çš„å¤„ç†

------

å±€é™ä¸æ”¹è¿›å»ºè®®ï¼ˆè‡³å°‘ 4 ç‚¹ï¼‰

1. **è¾“å‡ºé¡ºåºæœç´¢ (search over orders) çš„è®¡ç®—å¤æ‚æ€§** â€” å¯¹äºè¾“å‡ºé›†åˆ (å°¤å…¶å¤§çš„é›†åˆ) æ¥è¯´ï¼Œå¯èƒ½çš„æ’åˆ— (permutations) æ•°é‡æå¤§ ( $n!$ )ï¼Œåœ¨è®­ç»ƒæœŸé—´æœç´¢æœ€ä¼˜é¡ºåºå¯èƒ½éå¸¸è€—æ—¶ / ä¸å¯æ‰©å±•ã€‚è®ºæ–‡è™½ç„¶æå‡ºè¿™ä¸€æ€è·¯ï¼Œä½†å¹¶æ²¡æœ‰ç»™å‡ºé«˜æ•ˆã€å¯æ‰©å±•çš„æœç´¢/è¿‘ä¼¼ç®—æ³•ã€‚
2. **å¯¹å¤§è§„æ¨¡ / é«˜ç»´é›†åˆ (large sets) çš„æ³›åŒ–èƒ½åŠ›æœ‰é™** â€” åœ¨æ’åº (æ•°å­—) çš„å¤§ N æƒ…å†µä¸‹ (æ¯”å¦‚ N = 15)ï¼Œæ¨¡å‹è¡¨ç°å·²ç»å¾ˆå·® (å‡†ç¡®ç‡å¾ˆä½)ã€‚è¯´æ˜ RPW + pointer-net åœ¨å®é™…å¤æ‚é›†åˆä»»åŠ¡ä¸Šæ‰©å±•æ€§å¯èƒ½å—é™ã€‚
3. **è¾“å‡ºä¸ºå›ºå®šå­—å…¸ (vocabulary) çš„é›†åˆ / å¤šæ ‡ç­¾ (multi-label) ä»»åŠ¡æ¬ ç¼ºå®éªŒ** â€” è®ºæ–‡ä¸»è¦å…³æ³¨è¾“å‡ºä¸º pointer â†’ è¾“å…¥é›†åˆå…ƒç´  (å¦‚æ’åº) æˆ–è¾“å‡ºç»“æ„ (å¦‚ parse tree)ï¼Œå¯¹è¾“å‡ºä¸º â€œæ— åºæ ‡ç­¾é›† (multi-label classification)â€ çš„é€šç”¨é›†åˆæƒ…å†µï¼Œç¼ºä¹éªŒè¯ã€‚
4. **å¯¹å®é™…è‡ªç„¶åœºæ™¯ (çœŸå®æ•°æ®é›†) çš„åº”ç”¨è¾ƒå°‘ / æœ‰å±€é™** â€” å°½ç®¡ parsing å’Œè¯­è¨€æ¨¡å‹ä½¿ç”¨äº†çœŸå®æ•°æ®ï¼Œä½†åƒæ£€æµ‹ä¸€ç»„å¯¹è±¡ã€å›¾ç»“æ„é¢„æµ‹ã€å¤šæ ‡ç­¾åˆ†ç±»ç­‰æ›´è´´è¿‘æ—¥å¸¸åº”ç”¨çš„æ•°æ®ç±»å‹å¹¶æœªä½“ç°ã€‚ç»“æœä¸ä¸€å®šèƒ½ç›´æ¥æ¨å¹¿ã€‚
5. **ä¸ç¡®å®šæ€§ä¸ç¨³å®šæ€§** â€” ç”±äºæ¨¡å‹è®­ç»ƒä¸­å¯¹è¾“å‡ºé¡ºåºè¿›è¡Œæœç´¢å’Œé€‰æ‹©ï¼Œä¸åŒéšæœºç§å­ /è®­ç»ƒ run å¯èƒ½å­¦åˆ°ä¸åŒåºåˆ—åŒ–é¡ºåºï¼ŒåŠ ä¸Šè®­ç»ƒå¤æ‚åº¦ã€non-convex optimizationï¼Œå¯èƒ½å¯¼è‡´ä¸ç¨³å®š / ä¸å¯é‡å¤ã€‚
6. **ç¼ºä¹ç†è®ºä¿è¯ (theoretical guarantee)** â€” è®ºæ–‡ä¸»è¦ä»ç»éªŒ (empirical) è§’åº¦è®ºè¯ â€œorder mattersâ€ å’Œ â€œRPW æœ‰ç›Šâ€ï¼Œç¼ºå°‘å¯¹ä½•ç§æ•°æ® / ä»»åŠ¡ / é›†åˆå¤§å° /ç»“æ„ ä¸‹æ¨¡å‹èƒ½æ³›åŒ– /ç¨³å®šå·¥ä½œçš„ç†è®ºåˆ†æã€‚

æ”¹è¿›å»ºè®® (future work)ï¼š

- è®¾è®¡é«˜æ•ˆå¯æ‰©å±•çš„ **è¾“å‡ºé¡ºåºæœç´¢ /ä¼˜åŒ–ç®—æ³•**ï¼Œä¾‹å¦‚ç”¨è¿‘ä¼¼ã€å¯å‘å¼æœç´¢ã€å­¦ä¹ ä¸€ä¸ª order-policyï¼Œè€Œä¸æ˜¯ç©·ä¸¾ /éšæœºæœç´¢ã€‚
- åœ¨æ›´å¤§è§„æ¨¡ã€æ›´å¤æ‚çš„çœŸå®ä»»åŠ¡ (e.g. multi-label classification,å¯¹è±¡æ£€æµ‹ + å¤šç›®æ ‡è¾“å‡º, å›¾ç»“æ„é¢„æµ‹) ä¸Šæµ‹è¯• RPW + è¾“å‡ºé¡ºåºå­¦ä¹ ï¼Œä»¥éªŒè¯å…¶é€šç”¨æ€§ã€‚
- æ¢ç´¢æ›´å¼ºçš„ permutation-invariant / equivariant æ¶æ„ (ä¾‹å¦‚åŸºäºé›†åˆç½‘ç»œ /å›¾ç½‘ç»œ /Transformer)ï¼Œä»¥æ”¹å–„å¯¹å¤§é›†åˆ /å¤æ‚ç»“æ„çš„é€‚åº”æ€§ã€‚
- ä¸ºè®­ç»ƒè¿‡ç¨‹æ·»åŠ æ­£åˆ™åŒ– /ç¨³å®šæœºåˆ¶ï¼Œä»¥å‡å°‘å¯¹éšæœºåˆå§‹åŒ– /è®­ç»ƒé¡ºåº /search randomness çš„æ•æ„Ÿæ€§ã€‚
- ä»ç†è®ºä¸Šåˆ†æä½•ç§ä»»åŠ¡ / æ•°æ®åˆ†å¸ƒ /é›†åˆç‰¹æ€§ (size, cardinality, structure) ä¸‹ â€œorder mattersâ€ çš„å½±å“è¾ƒå¤§ï¼Œä»¥åŠ RPW ç­‰æ–¹æ³•çš„æé™ /ä¿è¯ã€‚



### ç¤ºä¾‹ä»£ç 

ä¸‹é¢æ˜¯ä¸€ä¸ª **æç®€åŒ– (toy)** çš„ Python ä»£ç ç¤ºä¾‹ï¼Œç”¨äºå±•ç¤º â€œRead-Process-Writeâ€ æ€è·¯ â€” å¯¹ä¸€ç»„æ•°å­— (set) ç¼–ç  (permutation-invariant)ï¼Œç„¶åé€šè¿‡ç®€å• decoder (pointer æ¨¡æ‹Ÿ) è¾“å‡ºæ’åº (sort) çš„ç´¢å¼•ã€‚

```
import torch
import torch.nn as nn
import torch.nn.functional as F
import random

class ReadProcessWrite(nn.Module):
    def __init__(self, in_dim=1, hid_dim=32, process_steps=3):
        super().__init__()
        self.encoder = nn.Linear(in_dim, hid_dim)
        self.process_steps = process_steps
        self.proc_lstm = nn.LSTMCell(hid_dim * 2, hid_dim)
        self.decoder = nn.LSTMCell(hid_dim, hid_dim)
        self.output_linear = nn.Linear(hid_dim, hid_dim)
        self.hid_dim = hid_dim

    def forward(self, xs):
        # xs: tensor of shape (batch, N, in_dim)
        batch, N, dim = xs.size()
        # Read
        mem = self.encoder(xs)  # (batch, N, hid_dim)
        # Process: attention over mem for a few steps
        h = torch.zeros(batch, self.hid_dim, device=xs.device)
        c = torch.zeros(batch, self.hid_dim, device=xs.device)
        for _ in range(self.process_steps):
            # compute attention weights
            q = h  # query
            attn = torch.einsum('bd,bnd->bn', q, mem)  # dot-product
            a = F.softmax(attn, dim=1)  # (batch, N)
            r = torch.einsum('bn,bnd->bd', a, mem)  # (batch, hid_dim)
            inp = torch.cat([q, r], dim=1)
            h, c = self.proc_lstm(inp, (h, c))
        context = h  # (batch, hid_dim)

        # For simplicity: we produce a score for each input element, then sort
        scores = torch.einsum('bd,bnd->bn', context, mem)  
        # return indices of sorted (ascending)
        idx = torch.argsort(scores, dim=1)
        return idx

# toy demonstration
if __name__ == "__main__":
    model = ReadProcessWrite(in_dim=1, hid_dim=32, process_steps=5)
    xs = torch.rand(2, 5, 1)  # batch size 2, 5 numbers each
    idx = model(xs)
    print("inputs:", xs)
    print("sorted indices by model:", idx)
    print("ground truth sort:", torch.argsort(xs.squeeze(-1), dim=1))

```



### QA

ä»¥ä¸‹æ˜¯ 8 é“ç»ƒä¹ é¢˜ (ç†è§£ / æ€è€ƒ)ï¼š

------

**Q1.** ä¸ºä»€ä¹ˆå°†é›†åˆ (set) å½“ä½œåºåˆ— (sequence) è¾“å…¥ç»™ç»å…¸ seq2seq æ¨¡å‹ï¼Œå¯èƒ½ä¸åˆç†ï¼Ÿ

**A1.** å› ä¸ºé›†åˆæœ¬è´¨ä¸Šæ— åº (permutation invariant)ï¼Œè€Œåºåˆ— (sequence) æœ‰å›ºå®šé¡ºåº (ordering) â€” å°†é›†åˆå½“æˆåºåˆ—å°±éšå«äº†äººä¸ºåˆ¶å®šçš„é¡ºåºï¼Œè¿™ç§ä»»æ„é¡ºåºå¯èƒ½å½±å“å­¦ä¹  (æ€§èƒ½ã€æ”¶æ•›) å¹¶ç ´åé›†åˆçš„ä¸å˜æ€§ã€‚

------

**Q2.** åœ¨ â€œRead-Process-Writeâ€ æ¨¡å‹ä¸­ï¼Œä¸ºä½• attention + memory çš„æœºåˆ¶æ˜¯ permutation-invariant çš„ï¼Ÿ

**A2.** å› ä¸º attention çš„ soft-attention æƒé‡æ˜¯å¯¹ memory set ä¸­æ‰€æœ‰è®°å¿†å‘é‡ $m_i$ è®¡ç®— (å¹¶åšå½’ä¸€åŒ–)ï¼Œæ‰€ä»¥æ— è®º memory å‘é‡åœ¨å†…éƒ¨åˆ—è¡¨ä¸­é¡ºåºå¦‚ä½• (shuffle)ï¼Œ attention ç»“æœ (è¯»å‡ºçš„å‘é‡) éƒ½ä¸ä¼šæ”¹å˜ â€” ä¿æŒå¯¹é›†åˆå…ƒç´ é¡ºåºä¸æ•æ„Ÿã€‚

------

**Q3.** åœ¨æ’åº (sorting) ä»»åŠ¡å®éªŒä¸­ï¼Œä¸ºä»€ä¹ˆ baseline pointer-net (ç”¨ LSTM é¡ºåºç¼–ç ) éš N å¢å¤§è¡¨ç°æ€¥å‰§ä¸‹é™ï¼Œè€Œ RPW æ¨¡å‹èƒ½æ›´å¥½åº”å¯¹ï¼Ÿ

**A3.** å› ä¸ºå½“ N è¾ƒå¤§æ—¶ï¼Œéšæœºé¡ºåºæˆ–äººä¸ºé¡ºåºå¯¹ LSTM ç¼–ç å¯èƒ½é€ æˆéš¾ä»¥å­¦ä¹  (éç»“æ„åŒ– / æ— è§„å¾‹) çš„è¾“å…¥é¡ºåºï¼Œå¯¼è‡´ encoder éš¾ä»¥æ•æ‰é›†åˆå†…éƒ¨å…³ç³»ã€‚RPW é€šè¿‡ permutation-invariant ç¼–ç  + attention å¤šæ­¥å¤„ç† (process) å¯ä»¥æ›´æœ‰æ•ˆåœ°èšåˆæ•´ä¸ªé›†åˆçš„ä¿¡æ¯ï¼Œå› æ­¤æ›´ç¨³å¥ã€‚

------

**Q4.** å¯¹äºä¸€ä¸ªè¾“å‡ºæ˜¯ä¸€ä¸ªæ— åºé›†åˆ (ä¾‹å¦‚å›¾åƒå¤šç›®æ ‡æ£€æµ‹è¾“å‡ºå¤šä¸ªå¯¹è±¡ bounding-box, æ— å›ºå®šä¹‰åºåˆ—) çš„ä»»åŠ¡ï¼Œç”¨ç»å…¸ seq2seq + chain-rule + å›ºå®šé¡ºåº (ä¾‹å¦‚æŒ‰ x åº§æ ‡æ’åº) ä¼šæœ‰ä»€ä¹ˆæ½œåœ¨é—®é¢˜ï¼Ÿ

**A4.** å›ºå®šæ’åºå¯èƒ½äººä¸ºå¼•å…¥ bias â€” å¹¶ä¸ä¸€å®šä¸ä»»åŠ¡ç»“æ„å¯¹é½ï¼›è€Œè‹¥æ’åºä¸åˆç† (ä¾‹å¦‚æŒ‰ x åº§æ ‡æ’åºå¯¹è¯­ä¹‰æ²¡æœ‰æ„ä¹‰)ï¼Œæ¨¡å‹å¯èƒ½å­¦ä¸åˆ°å¥½çš„è”åˆåˆ†å¸ƒ / ç»“æ„ï¼›æ­¤å¤–ï¼Œå¯¹æŸäº›è¾“å‡ºé›†åˆ (objects) çš„å¤šæ ·æ€§ /å¯¹ç§°æ€§ (permutation invariance) ä¼šè¢«ç ´åã€‚

------

**Q5.** è®ºæ–‡ä¸­å¯¹ output set çš„å¤„ç†æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿç®€è¿°æ€è·¯ã€‚

**A5.** åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ‰€æœ‰ (æˆ–éƒ¨åˆ†) å¯èƒ½çš„è¾“å‡ºåºåˆ—åŒ– (ordering) è¿›è¡Œæœç´¢ (search over orders)ï¼Œé€‰æ‹©å¯¹å½“å‰æ¨¡å‹æœ€æœ‰åˆ© (æœ€å®¹æ˜“å­¦) çš„åºåˆ—åŒ–é¡ºåº â€” ä»è€Œè®©æ¨¡å‹è‡ªåŠ¨å­¦åˆ°ä¸€ä¸ªå¯¹è¯¥ä»»åŠ¡ /æ•°æ®æœ€åˆé€‚çš„è¾“å‡ºé¡ºåºï¼Œè€Œä¸æ˜¯äººå·¥è®¾å®šå›ºå®šé¡ºåºã€‚

------

**Q6.** â€œRead-Process-Writeâ€ æ¨¡å‹åœ¨ç†è®ºä¸Šæ»¡è¶³ä»€ä¹ˆæ€§è´¨ (å…³äºè¾“å…¥é›†åˆ)ï¼Ÿ

**A6.** å®ƒæ»¡è¶³å¯¹è¾“å…¥é›†åˆçš„ **permutation invariance** â€” å³å¯¹è¾“å…¥é›†åˆä¸­å…ƒç´ é¡ºåº (æ’åˆ—) çš„ä»»æ„æ‰“ä¹± (shuffle) ä¸ä¼šæ”¹å˜æ¨¡å‹å¯¹é›†åˆçš„ç¼–ç  /è¡¨ç¤ºã€‚

------

**Q7.** ä½ è®¤ä¸ºä¸ºä»€ä¹ˆåœ¨è¯­è¨€æ¨¡å‹ (sequence æ•°æ®) ä¸Šï¼Œç”¨ â€œä¸è‡ªç„¶ / æ‰°ä¹±é¡ºåº (æ¯”å¦‚ 3-word reversal)â€ è®­ç»ƒå‡ºçš„æ¨¡å‹è™½ç„¶æ€§èƒ½ä¸‹é™ (perplexity â†‘)ï¼Œä½†ä»èƒ½å–å¾—åˆç†æ•ˆæœï¼Ÿ

**A7.** å› ä¸ºå³ä½¿é¡ºåºè¢«æ‰°ä¹±ï¼Œç»å…¸ seq2seq + LSTM ä»èƒ½é€šè¿‡å¤§é‡æ•°æ®å­¦ä¹ åˆ°æŸç§è”åˆåˆ†å¸ƒ (è™½ç„¶ n-gram / è¯­æ³•ç»“æ„è¢«ç ´å)ï¼Œè€Œ LSTM çš„å¼ºå»ºæ¨¡èƒ½åŠ› + chain-rule decomposition ä½¿å®ƒæœ‰ä¸€å®šé²æ£’æ€§ã€‚ä¸è¿‡ï¼Œè¿™æ ·å­¦åˆ°çš„æ¨¡å‹å¯èƒ½æ•æ‰åˆ°çš„æ˜¯ â€œé¡ºåºç»Ÿè®¡ (order-agnostic) + approximate correlationsâ€ è€ŒéçœŸå®è¯­è¨€ç»“æ„ï¼Œå› æ­¤æ€§èƒ½ç•¥å·®ã€‚

------

**Q8.** å‡å¦‚ä½ è¦æŠŠè¿™ç¯‡è®ºæ–‡çš„æ–¹æ³• (RPW + è¾“å‡ºé¡ºåºå­¦ä¹ ) åº”ç”¨äºä¸€ä¸ªç°å®ä»»åŠ¡ â€”â€” æ¯”å¦‚å¯¹å›¾åƒåšå¤šç›®æ ‡æ£€æµ‹å¹¶è¾“å‡ºä¸€ç»„è¾¹ç•Œæ¡† (bounding boxes, æ— ç‰¹å®šé¡ºåº) â€”â€” ä½ ä¼šé¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ

**A8.** æŒ‘æˆ˜åŒ…æ‹¬ï¼šå¤§é‡å¯èƒ½çš„è¾“å‡ºæ’åˆ— (bounding boxes çš„ä¸åŒé¡ºåº)ï¼Œæœç´¢ /å­¦ä¹ åˆé€‚é¡ºåºçš„è®¡ç®—å¤æ‚åº¦æé«˜ï¼›é›†åˆè§„æ¨¡ (ç›®æ ‡æ•°) å’Œå¤šæ ·æ€§é«˜ï¼Œå¯¹ memory å’Œ attention èµ„æºè¦æ±‚å¤§ï¼›è®­ç»ƒæ•°æ®ä¸­æ ‡æ³¨é¡ºåºä¸å›ºå®š (æˆ–å¤šæ ·) â€” éœ€è¦è®¾è®¡å…¼å®¹ label çš„æ–¹å¼ï¼›æ­¤å¤–ï¼Œå¯¹è¾“å‡ºè¾¹ç•Œæ¡†åæ ‡ +ç±»åˆ« +å…¶ä»–å±æ€§ (ä¸ä»…ä»…æ˜¯ä»è¾“å…¥ä¸­ pointer) çš„å¤æ‚æ€§ä¹Ÿé«˜ â€” éœ€è¦æ‰©å±• decoder / loss è®¾è®¡ã€‚

------



## 2018.11 GPipe

è®ºæ–‡åœ°å€ï¼š[GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/1811.06965)



### æ¦‚è¿°

------

æ¦‚è¿°1ï¼š

- Authors: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen
- The paper â€œGPipe: Easy Scaling with Micro-Batch Pipeline Parallelismâ€ introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.
- **Key Contributions:**
  - **GPipe Architecture:** The GPipe library partitions a neural network into smaller sub-sequences of layers, or â€œcells,â€ which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.
  - **Batch-Splitting Pipeline Parallelism:** GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.
  - **Synchronous Gradient Descent:** The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.
- **Experiments and Results:**
  - **Image Classification:** GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.
  - **Multilingual Neural Machine Translation:** GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipeâ€™s ability to handle diverse and large-scale NLP tasks.
- **Performance Optimization:**
  - **Re-materialization:** To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.
  - **Load Balancing:** The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.
- **Design Features and Trade-Offs:**
  - **Flexibility:** GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.
  - **Efficiency:** By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.
  - **Training Stability:** The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.
- **Conclusion:**
  - The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipeâ€™s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The libraryâ€™s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.

------

3â€“6 å¥è®ºæ–‡æ¦‚è¿°

- GPipe æå‡ºä¸€ç§é€šç”¨çš„**æµæ°´çº¿å¹¶è¡Œï¼ˆpipeline parallelismï¼‰**æ¡†æ¶ï¼Œå…è®¸æŠŠä»»æ„å¯è¡¨ç¤ºä¸ºâ€œå±‚åºåˆ—â€çš„ç¥ç»ç½‘ç»œåˆ’åˆ†ä¸ºè‹¥å¹²â€œcellâ€å¹¶æŠŠæ¯ä¸ª cell æ”¾åˆ°ä¸åŒåŠ é€Ÿå™¨ä¸Šï¼ŒåŒæ—¶å°†ä¸€ä¸ª mini-batch åˆ’åˆ†ä¸ºè‹¥å¹² micro-batch æ¥è¿›è¡Œæµæ°´çº¿åŒ–æ‰§è¡Œï¼Œä»è€Œåœ¨ä¸æ”¹å˜åŒæ­¥ SGD è¯­ä¹‰ä¸‹æ‰©å±•æ¨¡å‹å®¹é‡ä¸åŠ é€Ÿè®­ç»ƒã€‚ä¸ºäº†èŠ‚çœæ¿€æ´»ï¼ˆactivationï¼‰å†…å­˜ï¼ŒGPipe ç»“åˆäº†**re-materializationï¼ˆé‡è®¡ç®—ï¼‰**ç­–ç•¥ï¼Œä»…åœ¨åˆ†åŒºè¾¹ç•Œç¼“å­˜æ¿€æ´»ï¼Œåå‘ä¼ æ’­æ—¶æŒ‰éœ€é‡ç®—ã€‚è®ºæ–‡å±•ç¤ºäº† GPipe åœ¨å›¾åƒåˆ†ç±»ï¼ˆè®­ç»ƒ 5.57e8 å‚æ•° AmoebaNet åˆ° ImageNet 84.4% top-1ï¼‰å’Œå¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼ˆè®­ç»ƒ 6Bã€128 å±‚ Transformerï¼‰ä¸Šçš„æˆåŠŸåº”ç”¨ä¸å‡ ä¹çº¿æ€§æ‰©å±•æ€§ã€‚æ€»ä½“ç›®æ ‡æ˜¯æä¾›ä»»åŠ¡ & æ¶æ„æ— å…³ã€å®ç°ç®€å•ã€ä¸”èƒ½æŠŠå•å¡æ— æ³•å®¹çº³çš„å¤§æ¨¡å‹åˆ‡åˆ†åˆ°å¤šå¡ä¸Šè®­ç»ƒçš„é€šç”¨å·¥å…·ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

å…³é”®è´¡çŒ®è¦ç‚¹

1. æå‡º GPipeï¼šä¸€ä¸ªé¢å‘â€œå±‚åºåˆ—â€ç½‘ç»œçš„é€šç”¨æµæ°´çº¿å¹¶è¡Œåº“ä¸æ¥å£ï¼ˆç”¨æˆ·åªéœ€æŒ‡å®š partition æ•°ã€micro-batch æ•°å’Œå±‚åºåˆ—ï¼‰ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)
2. å¼•å…¥ **batch-splitting pipeliningï¼ˆmicro-batch åˆ’åˆ† + åŒæ­¥æ¢¯åº¦ç´¯ç§¯ï¼‰** çš„è®­ç»ƒç®—æ³•ï¼šå°† mini-batch åˆ’æˆ micro-batchesï¼Œæµæ°´çº¿æ‰§è¡Œï¼Œå¹¶åœ¨ mini-batch ç»“æŸæ—¶åŒæ­¥åº”ç”¨æ¢¯åº¦ï¼Œä»è€Œä¿è¯è®­ç»ƒè¯­ä¹‰ä¸å•å¡ä¸€è‡´ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)
3. ç»“åˆ **re-materialization**ï¼ˆåœ¨åå‘æ—¶é‡ç®—ä¸­é—´æ¿€æ´»ï¼‰å¤§å¹…é™ä½å³°å€¼æ¿€æ´»å†…å­˜ï¼Œå…è®¸å•å¡è®­ç»ƒæ›´æ·±/æ›´å®½ç½‘ç»œæˆ–æŠŠæ›´å¤§çš„æ¨¡å‹åˆ‡åˆ†åˆ°æ›´å¤šå¡ä¸Šã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)
4. å¤§è§„æ¨¡å®è¯ï¼šè®­ç»ƒ 557M AmoebaNetï¼ˆImageNet top-1 84.4%ï¼‰å’Œ 6Bã€128-layer Transformerï¼ˆå¤šè¯­ç¿»è¯‘è¶…è¶Šå¤šä¸ªåŒè¯­ baselineï¼‰ï¼Œå¹¶å±•ç¤ºè¿‘çº¿æ€§ååé‡æ‰©å±•ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº

- è¿‘å¹´æ¥æ¨¡å‹å®¹é‡å¢é•¿æ˜¾è‘—ï¼ˆImageNetã€NLP å‡æ˜¾ç¤ºæ›´å¤§æ¨¡å‹æ•ˆæœæ›´å¥½ï¼‰ï¼Œä½†å•ä¸ªåŠ é€Ÿå™¨ï¼ˆGPU/TPUï¼‰å†…å­˜é™åˆ¶é˜»ç¢äº†è¿›ä¸€æ­¥æ‰©å®¹ã€‚å·²æœ‰çš„ model-parallel æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šæ¶æ„ï¼ˆå¦‚æŸäº› Transformer ä¸“ç”¨åˆ‡åˆ†ï¼‰æˆ–éœ€è¦å¤æ‚é€šä¿¡/å®ç°ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚GPipe çš„åŠ¨æœºæ˜¯è®¾è®¡ä¸€ä¸ª**é€šç”¨ã€ç®€å•ã€è·¨æ¶æ„**çš„å¹¶è¡ŒåŒ–æ–¹æ¡ˆï¼Œä½¿â€œä»»æ„æŒ‰å±‚å®šä¹‰çš„ç½‘ç»œâ€éƒ½èƒ½é€šè¿‡æµæ°´çº¿åˆ†åŒºä¸ micro-batch ç­–ç•¥æ‰©å±•åˆ°å·¨å‹æ¨¡å‹ï¼ŒåŒæ—¶å°½é‡ä¿æŒé«˜åˆ©ç”¨ç‡ä¸ä½é€šä¿¡/å†…å­˜å¼€é”€ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

------

> å®éªŒä¸ç»“è®ºæ‘˜è¦ï¼ˆå«å…³é”®æ•°å€¼ï¼‰

è®ºæ–‡åœ¨ä¸¤ç±»ä»»åŠ¡ä¸ŠéªŒè¯ GPipe çš„æœ‰æ•ˆæ€§ï¼š

**å›¾åƒåˆ†ç±»ï¼ˆImageNetï¼‰**ï¼š

- ä½¿ç”¨ AmoebaNetï¼ˆæ‰©å±•åè¾¾ 557M å‚æ•°ï¼Œå³è®ºæ–‡ä¸­çº¦ 5.57e8 å‚æ•°çš„æ¨¡å‹ï¼‰ã€‚è®­ç»ƒç»“æœï¼š**ImageNet-2012 top-1 accuracy = 84.4%**ï¼ˆè®ºæ–‡å£°ç§°è¾¾è¯¥ SOTA æ°´å¹³ï¼‰ã€‚è®­ç»ƒåœ¨ TPU ä¸Šä½¿ç”¨ GPipe æ‰©å±•æ¨¡å‹åˆ°å•å¡æ— æ³•æ‰¿è½½çš„è§„æ¨¡ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼ˆMultilingual NMTï¼‰**ï¼š

- è®­ç»ƒä¸€ä¸ª **128-layer, 6-billion-parameter Transformer**ï¼ˆè¦†ç›– 103 è¯­è¨€å¯¹ï¼Œ102â†’Enï¼‰å¹¶ä¸å¤šä¸ªåŒè¯­ 350M Transformer Big æ¯”è¾ƒã€‚ç»“æœï¼š**å•ä¸€ 6B å¤šè¯­æ¨¡å‹æ•´ä½“ä¼˜äºæ¯å¯¹è®­ç»ƒçš„ bilingual 350M æ¨¡å‹**ï¼ˆå…·ä½“ BLEU æ”¹å–„è§è®ºæ–‡ç›¸å…³è¡¨/å›¾ï¼‰ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**æ‰©å±•æ€§ / ååé‡**ï¼š

- Table 1/2 å±•ç¤ºï¼šå¯¹äº Transformerï¼Œå½“ micro-batch æ•° $M \ge K$ æ—¶ï¼Œè®­ç»ƒååé‡éš partition æ•°æ¥è¿‘çº¿æ€§å¢é•¿ï¼ˆè®ºæ–‡æŠ¥å‘Šåœ¨ TPU ä¸Šå‡ ä¹çº¿æ€§åŠ é€Ÿï¼›Transformer åœ¨åˆ†åŒº 2/4/8 æ—¶ normalized throughput åˆ†åˆ«å¤§å¹…å¢åŠ ï¼Œè§ Table 2ï¼‰ã€‚å¦å¤–ï¼Œä½¿ç”¨ GPipe åœ¨ 8 å¼  8GB GPU ä¸Šèƒ½æŠŠ AmoebaNet æ‰©åˆ° 1.8B å‚æ•°ï¼ˆåœ¨ä¸åŒç¡¬ä»¶é…ç½®ä¸‹è®ºæ–‡ç»™å‡ºå¤šç»„æ•°å€¼ï¼‰ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**ç»“è®ºè¦ç‚¹**ï¼šGPipe å¯åœ¨ä¸æ”¹å˜åŒæ­¥ SGD è¯­ä¹‰çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ micro-batch æµæ°´çº¿ä¸é‡è®¡ç®—æ˜¾è‘—æ‰©å±•æ¨¡å‹å®¹é‡å¹¶åœ¨å¤šç±»æ¶æ„ä¸Šå–å¾—å®é™…æ”¶ç›Šï¼›åœ¨åˆç†é€‰æ‹© micro-batch æ•°é‡ä»¥ amortize bubble åå¯è·å¾—æ¥è¿‘çº¿æ€§çš„é€Ÿåº¦æ‰©å±•

------

å±€é™ä¸æ”¹è¿›å»ºè®®ï¼ˆè‡³å°‘ 4 ç‚¹ï¼‰

1. **åˆ†åŒºè´Ÿè½½ä¸å‡ï¼ˆload imbalanceï¼‰**ï¼šè®ºæ–‡é‡‡ç”¨å¯å‘å¼åˆ‡åˆ†ä»¥å°½é‡å‡è¡¡æ¯ä¸ª cell çš„æˆæœ¬ï¼Œä½†å¯¹äºéå‡åŒ€å±‚ï¼ˆå¦‚ AmoebaNetï¼‰ä»ä¼šå‡ºç°å­çº¿æ€§æ‰©å±•ã€‚
    å»ºè®®ï¼šå¼•å…¥æ›´ç²¾ç»†çš„é™æ€/åŠ¨æ€åˆ†åŒºå™¨ï¼ˆæ¯”å¦‚æ··åˆæ•´æ•°è§„åˆ’æˆ–åŸºäºæµ‹é‡çš„åŠ¨æ€åˆ†åŒºï¼‰ï¼Œæˆ–å…è®¸è·¨è®¾å¤‡é‡åˆ’åˆ†/è¿ç§»ä»¥é€‚åº”è®­ç»ƒé˜¶æ®µçš„å®é™…è´Ÿè½½åˆ†å¸ƒã€‚
2. **é€šä¿¡ä¸å»¶è¿Ÿå¯¹æµæ°´çº¿æ•ˆç‡æ•æ„Ÿ**ï¼šåœ¨ä½å¸¦å®½æˆ–é«˜å»¶è¿Ÿäº’è¿ï¼ˆå¦‚ PCI-E æ—  NVLinkï¼‰åœºæ™¯ï¼Œactivation ä¼ è¾“ä»ä¼šæˆä¸ºç“¶é¢ˆã€‚
    å»ºè®®ï¼šå¢åŠ  activation å‹ç¼©ï¼ˆé‡åŒ– / å‰ªæ /ä½ç§©è¿‘ä¼¼ï¼‰æˆ–å¼‚æ­¥å‹ç¼©é€šä¿¡ï¼Œå¹¶æ¢ç´¢ overlapï¼ˆé€šä¿¡/è®¡ç®—é‡å ï¼‰çš„æ›´æ¿€è¿›ç­–ç•¥ã€‚
3. **å¯¹æ¨¡å‹ç»“æ„çš„é™åˆ¶ï¼ˆå¿…é¡»ä¸ºâ€œå±‚åºåˆ—â€ï¼‰**ï¼šGPipe é€‚ç”¨äºé¡ºåºå±‚æ¨¡å‹ï¼Œä½†å¯¹äºå›¾å½¢åŒ–/æœ‰å¤æ‚ DAG ä¾èµ–ï¼ˆå¦‚å¤šåˆ†æ”¯ã€è·³è·ƒè¿æ¥å¯†é›†çš„ç½‘ç»œï¼‰æ”¯æŒè¾ƒå¼±æˆ–éœ€è¦æ‰‹å·¥æ‹†åˆ†ã€‚
    å»ºè®®ï¼šæ‰©å±•æ¥å£ä»¥æ”¯æŒ DAG å‹æ¨¡å‹è‡ªåŠ¨åˆ’åˆ†ï¼ˆæ„å»ºä¾èµ–å›¾å¹¶æ‰¾åˆ°åˆé€‚çš„ topological partitioningï¼‰ï¼Œå¹¶æ”¯æŒâ€œåˆ†æ”¯åˆå¹¶â€çš„é€šä¿¡è°ƒåº¦ç­–ç•¥ã€‚
4. **å¾®æ‰¹æ¬¡æ•° M é€‰æ‹©æƒè¡¡**ï¼šM å¿…é¡»è¶³å¤Ÿå¤§ä»¥ amortize bubbleï¼Œä½†è¿‡å¤§åˆ™ä½¿æ¯ä¸ª micro-batch æ›´å°ï¼Œå¯èƒ½å½±å“ç»Ÿè®¡æ•ˆæœï¼ˆBNã€ä¼˜åŒ–ç¨³å®šæ€§ï¼‰å’Œç¡¬ä»¶åˆ©ç”¨ã€‚
    å»ºè®®ï¼šè‡ªåŠ¨é€‰æ‹©/è°ƒæ•´ Mï¼ˆè‡ªé€‚åº”å¢/å‡ï¼‰ï¼Œæˆ–ä½¿ç”¨æ··åˆç­–ç•¥ï¼ˆéƒ¨åˆ†æ•°æ®å¹¶è¡Œ + éƒ¨åˆ†æµæ°´çº¿ï¼‰ä»¥åœ¨ç¨³å¥æ€§ä¸æ•ˆç‡ä¹‹é—´å–å¾—æŠ˜ä¸­ã€‚
5. **è®­ç»ƒè¯­ä¹‰ä¸è°ƒå‚å½±å“**ï¼šè™½ç„¶æ¢¯åº¦ç´¯åŠ ä¿æŒäº†åŒæ­¥è¯­ä¹‰ï¼Œä½†å®é™…è®­ç»ƒè¶…å‚ï¼ˆå­¦ä¹ ç‡ã€batch-norm ç»Ÿè®¡ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰åœ¨éå¸¸å¤§æ¨¡å‹/å¤§æ‰¹é‡ä¸‹å¯èƒ½éœ€è¦é‡æ–°è°ƒä¼˜ã€‚
    å»ºè®®ï¼šæä¾›å¼€ç®±å³ç”¨çš„è¶…å‚æ¨èï¼ˆåŸºäºè§„æ¨¡çš„ LR scalingã€warmupã€æ­£åˆ™åŒ–ï¼‰ï¼Œå¹¶åœ¨åº“ä¸­é›†æˆè‡ªåŠ¨å­¦ä¹ ç‡ç¼©æ”¾ä¸ç»Ÿè®¡é‡‡æ ·å·¥å…·ã€‚
6. **å®¹é”™ä¸æ–­ç‚¹æ¢å¤**ï¼šè®ºæ–‡æœªè¯¦è¿°åœ¨å¤šå¡è®­ç»ƒä¸­å‡ºç°å•å¡æ•…éšœæ—¶çš„å®¹é”™æœºåˆ¶ã€‚
    å»ºè®®ï¼šå®ç° checkpointing åœ¨ partition ç²’åº¦ï¼ˆå¯ä¿å­˜æ¯ä¸ª cell çš„æƒé‡ä¸ optimizer çŠ¶æ€ï¼Œå¹¶èƒ½ä»ä»»æ„å•ä¸ª partition æ¢å¤ï¼‰ï¼Œå¹¶æ”¯æŒéƒ¨åˆ†é‡å¯/æ›¿æ¢ deviceã€‚

------



### ç¤ºä¾‹ä»£ç 

ä¸‹é¢ç»™å‡ºä¸€ä¸ª**å¯è¿è¡Œçš„ PyTorch ç¤ºèŒƒè„šæœ¬**ï¼ˆå¯åœ¨ CPU ä¸Šè¿è¡Œï¼Œä¹Ÿå¯åœ¨å•æœºå¤š GPU ä¸Šè¿è¡Œï¼‰æ¥**æ¨¡æ‹Ÿ GPipe çš„ micro-batch æµæ°´çº¿åˆ†åŒº**ã€‚æ­¤ç¤ºä¾‹æŠŠä¸€ä¸ª `nn.Sequential` æ¨¡å‹åˆ†ä¸ºä¸¤æ®µï¼ˆä¸¤ä¸ª partitionï¼‰ï¼Œå°†ä¸€ä¸ª mini-batch åˆ‡æˆè‹¥å¹² micro-batchesï¼Œå¹¶æŒ‰æµæ°´çº¿æ—¶åºåœ¨ä¸åŒè®¾å¤‡ä¸Šé¡ºåºæ‰§è¡Œï¼ˆè‹¥æ— å¤š GPUï¼Œä¼šæŠŠ device è®¾ä¸º CPUï¼Œä½†é€»è¾‘ä¸€è‡´ï¼‰ã€‚è¯¥è„šæœ¬æ¼”ç¤ºå‰å‘/åå‘çš„ micro-step è°ƒåº¦ä»¥åŠæœ€åçš„æ¢¯åº¦ç´¯åŠ ä¸ä¸€æ¬¡ optimizer.step()ã€‚

```
# gpipe_sim.py -- ç®€å• GPipe é£æ ¼æµæ°´çº¿æ¨¡æ‹Ÿ (PyTorch)
import torch
import torch.nn as nn
import torch.optim as optim

# é…ç½®
B = 16              # mini-batch å¤§å°
M = 4               # micro-batches æ•° (M <= B ä¸” B % M == 0)
b = B // M
in_dim = 32
hidden = 64
out_dim = 10
device0 = torch.device('cpu')    # è‹¥æœ‰å¤š GPU å¯è®¾ 'cuda:0'
device1 = torch.device('cpu')    # è‹¥æœ‰å¤š GPU å¯è®¾ 'cuda:1'

# æ„å»ºä¸€ä¸ªç®€å•çš„ Sequential ç½‘ç»œå¹¶åˆ†æˆ 2 ä¸ª cell
cell1 = nn.Sequential(
    nn.Linear(in_dim, hidden),
    nn.ReLU(),
    nn.Linear(hidden, hidden),
).to(device0)

cell2 = nn.Sequential(
    nn.ReLU(),
    nn.Linear(hidden, hidden),
    nn.ReLU(),
    nn.Linear(hidden, out_dim)
).to(device1)

# åˆå¹¶å‚æ•°ç”¨äº optimizerï¼ˆæ³¨æ„ï¼šçœŸå®åˆ†å¸ƒå¼éœ€æ¯è®¾å¤‡å„è‡ª optimizer æˆ–åˆ†å¸ƒå¼åŒæ­¥ï¼‰
params = list(cell1.parameters()) + list(cell2.parameters())
opt = optim.SGD(params, lr=0.01)

# æŸå¤±
criterion = nn.CrossEntropyLoss()

# äººé€ æ•°æ®
X = torch.randn(B, in_dim)
y = torch.randint(0, out_dim, (B,))

# åˆ‡åˆ†ä¸º micro-batches
micro_X = torch.split(X, b)
micro_y = torch.split(y, b)

# ä¸ºäº†æ¨¡æ‹Ÿ re-materializationï¼Œæˆ‘ä»¬ä¸ç¼“å­˜ä¸­é—´æ¿€æ´»ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºèŒƒï¼‰
# è®°å½•æ¯ä¸ª micro-batch åœ¨ cell ä¸Šçš„ forward/activationsï¼ˆåœ¨çœŸå®å®ç°ä¸­ä¼šä¼ è¾“ tensorï¼‰
# è¿™é‡Œä½¿ç”¨ç®€å•çš„ time-step è°ƒåº¦ï¼šæ€»æ­¥æ•° = M + K - 1 (K=2)
K = 2
total_steps = M + K - 1

# ç”¨äºç´¯ç§¯æ¢¯åº¦ï¼ˆPyTorch é»˜è®¤æ˜¯ç´¯åŠ æ¢¯åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ optimizer.step å‰ä¸æ¸… gradï¼‰
opt.zero_grad()

# æ¨¡æ‹Ÿæµæ°´çº¿ï¼šæ¯ä¸ª stepï¼Œæ£€æŸ¥å“ªäº› (micro_index, cell_index) åº”è¯¥æ‰§è¡Œ
# micro_index ä» 0..M-1, cell_index ä» 0..K-1
for t in range(total_steps):
    for k in range(K):
        micro_index = t - k
        if 0 <= micro_index < M:
            # æ‰§è¡Œå‰å‘æˆ–åå‘çš„ç®€åŒ–è°ƒåº¦ï¼šå…ˆå…¨éƒ¨forwardï¼Œå†åœ¨åç»­ step åš backward
            # ä¸ºæ¸…æ™°èµ·è§ï¼Œè¿™é‡Œå…ˆåš forward å¹¶ä¿å­˜ activation åˆ°å°å˜é‡ï¼Œç„¶ååœ¨
            # å½“å¯¹åº” micro åœ¨æœ€æœ« cell å®Œæˆå‰å‘åï¼Œæˆ‘ä»¬ç«‹åˆ»åš backwardï¼ˆç®€åŒ–ï¼‰
            x_mb = micro_X[micro_index]
            y_mb = micro_y[micro_index]
            # å‰å‘è£å‰ªä¸è®¾å¤‡ä¼ è¾“ï¼ˆæ˜¾å¼ç§»åŠ¨ï¼‰
            if k == 0:
                x = x_mb.to(device0)
                a = cell1(x)            # activation è¾“å‡ºåœ¨ device0
                a_to_next = a.detach().to(device1)  # æ¨¡æ‹Ÿé€šä¿¡ï¼ˆdetach è¡¨ç¤ºä¸ä¿ gradientï¼‰
            else: # k==1
                a = a_to_next.to(device1)
                out = cell2(a)
                loss = criterion(out, y_mb.to(device1))
                # åå‘ï¼šè®¡ç®—å½“å‰ micro çš„æ¢¯åº¦ï¼ˆå°†æ¢¯åº¦ç´¯åŠ åˆ°å‚æ•°ï¼‰
                loss.backward()
                # æ³¨æ„ï¼šåœ¨çœŸå® GPipe ä¸­ï¼Œè¿˜è¦è€ƒè™‘ re-materializationï¼ˆè¿™é‡Œç•¥ï¼‰
    # end inner for
# åœ¨æ‰€æœ‰ micro-batches å®Œæˆååº”ç”¨ä¸€æ¬¡å‚æ•°æ›´æ–°
opt.step()
opt.zero_grad()
print("Done. Simulated GPipe step applied optimizer.step() once for mini-batch.")

```



### QA

ä¸‹é¢ç»™ä¸€ç»„ç»ƒä¹ é¢˜ï¼ˆå¸¦å‚è€ƒç­”æ¡ˆï¼‰ï¼Œé€‚åˆç”¨äºç†è§£è®ºæ–‡è¦ç‚¹ä¸å®ç°ç»†èŠ‚ã€‚

------

**é¢˜ 1.** GPipe çš„æ ¸å¿ƒç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•ä¿è¯ä¸å•å¡åŒæ­¥ SGD çš„ç­‰ä»·æ€§ï¼Ÿ
 **ç­” 1.** æ ¸å¿ƒç­–ç•¥æ˜¯æŠŠ mini-batch æ‹†æˆå¤šä¸ª micro-batches å¹¶æµæ°´çº¿åœ°åœ¨ä¸åŒ partition ä¸Šå¹¶è¡Œæ‰§è¡Œï¼ŒåŒæ—¶åœ¨æ¯ä¸ª micro-batch çš„åå‘è¿‡ç¨‹ä¸­ç´¯åŠ æ¢¯åº¦ï¼Œå¹¶åœ¨å¤„ç†å®Œæ•´ä¸ª mini-batch åä¸€æ¬¡æ€§åº”ç”¨æ¢¯åº¦ï¼ˆoptimizer.stepï¼‰ã€‚å› æ­¤æ›´æ–°é¢‘ç‡ä¸å•å¡åŒæ­¥ SGD ä¸€è‡´ï¼Œè®­ç»ƒè¯­ä¹‰ç­‰ä»·ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 2.** ä»€ä¹ˆæ˜¯ re-materializationï¼ˆé‡è®¡ç®—ï¼‰ï¼Œå®ƒåœ¨ GPipe ä¸­èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ
 **ç­” 2.** re-materialization æŒ‡ä¸åœ¨å‰å‘æ—¶ç¼“å­˜å…¨éƒ¨ä¸­é—´æ¿€æ´»ï¼Œè€Œæ˜¯åœ¨åå‘æ—¶é‡æ–°è®¡ç®—éœ€è¦çš„å‰å‘ä¸­é—´ç»“æœä»¥èŠ‚çœå†…å­˜ã€‚åœ¨ GPipe ä¸­ä¸åˆ†åŒºç»“åˆåå¯æ˜¾è‘—é™ä½å³°å€¼æ¿€æ´»å†…å­˜ï¼Œä»è€Œå…è®¸æ›´å¤§æ¨¡å‹ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 3.** â€œbubbleâ€ åœ¨æµæ°´çº¿ä¸Šä¸‹æ–‡æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•å‡å°å®ƒçš„å½±å“ï¼Ÿ
 **ç­” 3.** bubble æŒ‡åŠ é€Ÿå™¨ç©ºé—²ç­‰å¾…æ—¶é—´ï¼ˆpipeline fill/drain é˜¶æ®µäº§ç”Ÿçš„ç©ºé—²æ§½ï¼‰ï¼Œä¼šé™ä½å¹¶è¡Œæ•ˆç‡ã€‚å°† micro-batch æ•° $M$ å¢å¤§ï¼ˆé€šå¸¸ $M \ge K$ï¼‰å¯ amortize bubble å¼€é”€ï¼Œä»è€Œé™ä½å…¶ç›¸å¯¹å½±å“ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 4.** å½“ç½‘ç»œå±‚è®¡ç®—/å‚æ•°åˆ†å¸ƒä¸å‡åŒ€æ—¶ï¼ŒGPipe ä¼šé¢ä¸´ä»€ä¹ˆé—®é¢˜ï¼Ÿè§£å†³æ–¹æ¡ˆæœ‰å“ªäº›ï¼Ÿ
 **ç­” 4.** ä¼šå‡ºç° load imbalanceï¼ˆæŸäº› partition è®¡ç®—è¿œå¤šäºå…¶ä»– partitionï¼‰ï¼Œå¯¼è‡´ååé‡é™ä½ï¼›å¯ç”¨æ›´æ™ºèƒ½çš„åˆ†åŒºå™¨ï¼ˆåŸºäºè®¡é‡çš„åˆ’åˆ†ã€ä¼˜åŒ–æ±‚è§£å™¨ï¼‰æˆ–é‡åˆ†é… layer-granularity çš„åˆ‡åˆ†æ¥ç¼“è§£ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 5.** åœ¨æ²¡æœ‰é«˜é€Ÿäº’è¿ï¼ˆå¦‚ NVLinkï¼‰çš„æœºå™¨ä¸Šï¼ŒGPipe çš„é€šä¿¡å¼€é”€å¦‚ä½•ï¼Ÿè®ºæ–‡ç»™å‡ºçš„å®éªŒç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ
 **ç­” 5.** é€šä¿¡éœ€è¦é€šè¿‡ä¸»æœºå†…å­˜ï¼ˆPCI-Eï¼‰ä¼ è¾“ activation å¼ é‡ï¼Œä¼šå¢åŠ å¼€é”€ï¼Œä½†è®ºæ–‡åœ¨ GPU æ—  NVLink çš„è®¾ç½®ä¸­ä»è§‚å¯Ÿåˆ°ä¸€å®š speedupï¼ˆè§† micro-batch æ•°ä¸ partition è®¾ç½®è€Œå®šï¼‰ï¼Œè¡¨æ˜é€šä¿¡å¼€é”€åœ¨åˆç†è®¾ç½®ä¸‹ä¸æ˜¯ä¸å¯æ§çš„ç“¶é¢ˆã€‚è¯¦è§ Table 3ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 6.** GPipe å¯å¦ç›´æ¥ç”¨äºä»»æ„ DAGï¼ˆæœ‰åˆ†æ”¯/åˆå¹¶ï¼‰çš„æ¨¡å‹ï¼Ÿä¸ºä»€ä¹ˆæˆ–ä¸ºä»€ä¹ˆä¸ï¼Ÿ
 **ç­” 6.** è®ºæ–‡ä¸»è¦é’ˆå¯¹çº¿æ€§å±‚åºåˆ—ï¼ˆsequence-of-layersï¼‰æ¨¡å‹ï¼›å¤æ‚ DAG éœ€å…ˆå°†å›¾è½¬ä¸ºçº¿æ€§åºåˆ—æˆ–æ‰‹å·¥æ‹†åˆ†ï¼Œå› å…¶è‡ªåŠ¨åˆ’åˆ†/è°ƒåº¦æ›´å¤æ‚ï¼ŒGPipe åŸå§‹å®ç°å¯¹ DAG æ”¯æŒæœ‰é™ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 7.** åœ¨ GPipe ä¸­é€‰æ‹© micro-batch æ•° M çš„ä¸»è¦è€ƒè™‘å› ç´ æœ‰å“ªäº›ï¼Ÿ
 **ç­” 7.** ä¸»è¦è€ƒè™‘ amortize bubbleï¼ˆM åº”è‡³å°‘ä¸ partition æ•° K ç›¸å½“æˆ–æ›´å¤§ï¼‰ã€æ¯ micro-batch çš„ batch-norm ç»Ÿè®¡ç¨³å®šæ€§ã€ä»¥åŠæ•´ä½“å†…å­˜å¯å®¹çº³æ€§ï¼›è¿‡å° M ä¼šå¯¼è‡´ä½åˆ©ç”¨ç‡ï¼Œè¿‡å¤§ M å¯èƒ½å½±å“ç»Ÿè®¡/æ”¶æ•›ä¸é€šä¿¡é¢‘ç‡ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 8.** åˆ—ä¸¾è®ºæ–‡ä¸­ä¸¤é¡¹å…³é”®å®è¯æ€§èƒ½æŒ‡æ ‡ï¼ˆä»»åŠ¡ + æ•°å€¼ï¼‰ã€‚
 **ç­” 8.** (i) AmoebaNet æ‰©å±•è‡³ ~557M å‚æ•°å¹¶åœ¨ ImageNet-2012 ä¸Šè¾¾åˆ° **84.4% top-1**ï¼›(ii) è®­ç»ƒå•ä¸€ 128-layerã€6B å‚æ•° Transformer è¦†ç›– 103 è¯­è¨€ï¼Œå¯¹æ¯”å¤šå¯¹åŒè¯­ 350M æ¨¡å‹æœ‰æ›´å¥½è¡¨ç°ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 9.** è‹¥åœ¨ GPipe ä¸­ä½¿ç”¨ BatchNormï¼Œåº”å¦‚ä½•å¤„ç†ç»Ÿè®¡é‡ä»¥ä¿æŒä¸€è‡´æ€§ï¼Ÿ
 **ç­” 9.** åœ¨è®ºæ–‡ä¸­è¯´æ˜ï¼ŒBatchNorm çš„ç»Ÿè®¡åº”åœ¨ micro-batch çº§åˆ«è®¡ç®—ï¼Œå¹¶åœ¨éœ€è¦æ—¶è·¨ replicas æ±‡æ€»ï¼›åŒæ—¶è®°å½• mini-batch å±‚é¢çš„ moving average ä»¥ç”¨äºè¯„ä¼°ï¼ˆpaper Â§2.2ï¼‰ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

**é¢˜ 10.** GPipe èƒ½å¦ä¸æ•°æ®å¹¶è¡Œç»“åˆï¼Ÿæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ
 **ç­” 10.** å¯ä»¥ï¼šGPipe è´Ÿè´£æ¨¡å‹å¹¶è¡Œï¼ˆè·¨å±‚åˆ‡åˆ†ï¼‰ï¼Œæ•°æ®å¹¶è¡Œè´Ÿè´£è·¨ replica çš„æ•°æ®åˆ†å¸ƒï¼Œä¸¤è€…ç»“åˆå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•è®­ç»ƒåˆ°æ›´å¤šè®¾å¤‡å¹¶å¢åŠ ååé‡/ç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚è®ºæ–‡å»ºè®®å¯ç»„åˆä½¿ç”¨ä»¥æ‰©å±•è®­ç»ƒè§„æ¨¡ã€‚[ar5iv](https://ar5iv.org/pdf/1811.06965)

------



## 2015.12 ResNet

è®ºæ–‡åŸæ–‡ï¼š[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

ç¥ä¸­ç¥ï¼



### æ¦‚è¿°

------

æ¦‚è¿°ï¼š

- **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
- **Affiliation**: Microsoft Research
- This seminal paper introduces the concept of deep residual networks (ResNets), which significantly ease the training of networks that are substantially deeper than those used previously. By utilizing residual blocks that allow layers to fit a residual mapping instead of directly attempting to fit a desired underlying mapping, ResNets facilitate the training process and improve the accuracy from increased depth.

Key innovations and findings from the paper include:

1. **Residual Learning Framework**: The layers in ResNet learn residual functions with reference to the layer inputs, which simplifies the learning process because the network learns to modify the identity mapping rather than having to estimate the full output.
2. **Ease of Optimization**: The residual blocks make deeper networks easier to optimize because they mitigate the problem of vanishing gradients by using shortcut connections that perform identity mapping.
3. **Superior Performance on Deep Networks**: Extensive experiments demonstrate that ResNets, with their deeper architectures, outperform traditional networks on major datasets like ImageNet and CIFAR-10. For instance, ResNets with a depth of up to 152 layers show better performance and lower complexity compared to VGG nets.
4. **Broad Applicability**: The paper also highlights the effectiveness of ResNets across various tasks beyond image classification, such as object detection and localization, through adaptations like bottleneck designs that enhance computational efficiency.

- These contributions have had a profound impact on the field of deep learning, influencing a wide range of subsequent research and applications in both academia and industry.

------

### QA

ä¸‹é¢ç»™å‡º 10 é“ç»ƒä¹ é¢˜ï¼ˆè¦†ç›–æ¦‚å¿µã€å…¬å¼ã€æ¶æ„ã€å®éªŒä¸æ‰¹åˆ¤æ€§æ€è€ƒï¼‰ï¼Œæ¯é¢˜åç»™å‡ºå‚è€ƒç­”æ¡ˆã€‚

1. **é—®ï¼šæ®‹å·®å—çš„åŸºæœ¬æ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒèƒ½ç¼“è§£â€œé€€åŒ–é—®é¢˜â€ï¼Ÿ**
    ç­”ï¼šåŸºæœ¬æ€æƒ³æ˜¯è®©å­ç½‘ç»œå­¦ä¹ æ®‹å·®å‡½æ•° $\mathcal{F}(x)=\mathcal{H}(x)-x$ï¼Œå¹¶é€šè¿‡æ·å¾„æŠŠ $x$ ç›´æ¥åŠ å›å»ï¼ˆ$\mathbf{y}=\mathcal{F}(x)+x$ï¼‰ã€‚å¦‚æœæœ€ä¼˜æ˜ å°„æ¥è¿‘æ’ç­‰æ˜ å°„ï¼Œåˆ™æ®‹å·®æ¥è¿‘ 0ï¼Œè¿™æ ·å­¦ä¹ æ›´å®¹æ˜“ï¼Œä»è€Œå‡è½»æ·±å±‚ç½‘ç»œè®­ç»ƒæ—¶å‡ºç°çš„è®­ç»ƒè¯¯å·®ä¸Šå‡ï¼ˆé€€åŒ–ï¼‰ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
2. **é—®ï¼šå†™å‡ºè®ºæ–‡ä¸­æ®‹å·®å—åœ¨ç»´åº¦ä¸åŒ¹é…æ—¶çš„æ•°å­¦å½¢å¼ã€‚**
    ç­”ï¼šå½“è¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒ¹é…æ—¶ï¼Œä½¿ç”¨çº¿æ€§æŠ•å½± $W_s$ æ¥åŒ¹é…ï¼š$\mathbf{y}=\mathcal{F}(\mathbf{x},\{W_i\})+W_s\mathbf{x}$ã€‚è¿™æ˜¯è®ºæ–‡ Eqn.(2)ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
3. **é—®ï¼šè®ºæ–‡ä¸­æåˆ°â€œidentity shortcut ä¸å¼•å…¥é¢å¤–å‚æ•°æˆ–è®¡ç®—å¤æ‚åº¦â€ï¼Œåœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹è¿™ä¸€è¯´æ³•æˆç«‹ï¼Ÿ**
    ç­”ï¼šå½“è¾“å…¥ä¸è¾“å‡ºé€šé“æ•°åŒ¹é…ã€ä¸”ä¸éœ€è¦ä¸‹é‡‡æ ·æ—¶ï¼Œä½¿ç”¨æ’ç­‰æ·å¾„ï¼ˆç›´æ¥ç›¸åŠ ï¼‰ä¸ä¼šå¢åŠ å‚æ•°æˆ–æ˜¾è‘—çš„è®¡ç®—ï¼›ä»…ç›¸åŠ æ“ä½œæˆæœ¬å¾ˆå°ã€‚è‹¥ç»´åº¦ä¸åŒ¹é…åˆ™éœ€æŠ•å½±ï¼ˆä¾‹å¦‚ $1\times1$ å·ç§¯ï¼‰ï¼Œæ­¤æ—¶ä¼šå¢åŠ å‚æ•°ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
4. **é—®ï¼šResNet-50 ä¸ ResNet-34 åœ¨ç»“æ„ä¸Šçš„ä¸»è¦å·®åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**
    ç­”ï¼šResNet-34 ä½¿ç”¨ BasicBlockï¼ˆä¸¤å±‚ 3Ã—3ï¼‰ï¼Œè€Œ ResNet-50 ä½¿ç”¨ Bottleneck blockï¼ˆ1Ã—1 é™ç»´ â†’ 3Ã—3 â†’ 1Ã—1 æ¢å¤ç»´åº¦ï¼‰ï¼Œå› æ­¤ ResNet-50 èƒ½åœ¨æ›´æ·±çš„å±‚æ•°ä¸‹ä¿æŒè®¡ç®—æ•ˆç‡ã€‚è¯¦è§ Table 1ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
5. **é—®ï¼šè®ºæ–‡åœ¨ ImageNet ä¸ŠæŠ¥å‘Šçš„ ResNet-152 å•æ¨¡å‹ top-5 éªŒè¯è¯¯å·®å¤§çº¦æ˜¯å¤šå°‘ï¼Ÿé›†æˆæµ‹è¯•è¯¯å·®æ˜¯å¤šå°‘ï¼Ÿ**
    ç­”ï¼šè®ºæ–‡æŠ¥å‘Š ResNet-152 å•æ¨¡å‹ top-5 éªŒè¯è¯¯å·®çº¦ **4.49%**ï¼ˆæˆ–åœ¨å¦ä¸€è¡¨ä¸­ 5.71%ï¼Œå–å†³äºè¯„æµ‹ç»†èŠ‚/å˜ä½“ï¼‰ï¼›é›†æˆï¼ˆensembleï¼‰åœ¨æµ‹è¯•é›†ä¸Šçš„ top-5 é”™è¯¯ä¸º **3.57%**ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
6. **é—®ï¼šä¸ºä»€ä¹ˆè®ºæ–‡å¼ºè°ƒæ®‹å·®å‡½æ•°çš„è¾“å‡ºé€šå¸¸è¾ƒå°ï¼Ÿè¿™è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ**
    ç­”ï¼šä½œè€…åœ¨å®éªŒè¯æ˜æ®‹å·®å“åº”æ™®éåå°ï¼Œè¯´æ˜åœ¨è®¸å¤šæƒ…å†µä¸‹æœ€ä¼˜æ˜ å°„æ¥è¿‘æ’ç­‰æ˜ å°„ï¼Œæ®‹å·®å­¦ä¹ å°†é—®é¢˜é¢„è°ƒèŠ‚ï¼ˆpreconditionï¼‰ä¸ºâ€œå­¦ä¹ å°çš„æ‰°åŠ¨â€æ›´å®¹æ˜“ä¼˜åŒ–ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
7. **é—®ï¼šåˆ—ä¸¾è‡³å°‘ä¸¤ç§å¯ä»¥æ›¿ä»£æˆ–æ‰©å±•æ’ç­‰æ·å¾„çš„æ–¹æ³•ï¼Œå¹¶ç®€çŸ­è¯´æ˜ç”¨é€”ã€‚**
    ç­”ï¼šå¯å­¦ä¹ çš„é—¨æ§ï¼ˆå¦‚ highway networksï¼‰å¯ä»¥æ§åˆ¶ä¿¡æ¯æµï¼›æŠ•å½±æ·å¾„ï¼ˆ1Ã—1 convï¼‰ç”¨äºåŒ¹é…ç»´åº¦ï¼›æ³¨æ„åŠ›æ¨¡å—ï¼ˆSE, CBAMï¼‰å¯ä»¥å¯¹é€šé“/ç©ºé—´åŠ æƒä»¥å¢å¼ºè¡¨å¾ã€‚æ¯ç§æ–¹æ³•å¯æ”¹è¿›ä¿¡æ¯é€‰æ‹©æˆ–å»ºæ¨¡èƒ½åŠ›ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
8. **é—®ï¼šåœ¨å®é™…è®­ç»ƒè¶…æ·± ResNetï¼ˆä¾‹å¦‚ 152 å±‚ï¼‰æ—¶ï¼Œå“ªäº›è®­ç»ƒæŠ€å·§æ˜¯å¿…éœ€æˆ–éå¸¸é‡è¦çš„ï¼Ÿ**
    ç­”ï¼šBatch Normalizationã€åˆé€‚çš„å‚æ•°åˆå§‹åŒ–ã€é€‚å½“çš„å­¦ä¹ ç‡è®¡åˆ’ï¼ˆlearning-rate scheduleï¼‰ã€æ•°æ®å¢å¼ºã€å¤§æ‰¹é‡æˆ–æ¢¯åº¦ç´¯ç§¯ä»¥åŠæƒé‡è¡°å‡å¸¸è¢«ç”¨äºç¨³å®šè®­ç»ƒã€‚è®ºæ–‡ä½¿ç”¨ BN å¹¶ç»“åˆè®­ç»ƒç»†èŠ‚å–å¾—ç¨³å®šè®­ç»ƒã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
9. **é—®ï¼ˆæ€è€ƒé¢˜ï¼‰ï¼šæ®‹å·®ç½‘ç»œæ˜¯å¦å®Œå…¨è§£å†³äº†æ·±å±‚ç½‘ç»œçš„æ‰€æœ‰ä¼˜åŒ–é—®é¢˜ï¼Ÿä¸¾ä¾‹è¯´æ˜ä»å¯èƒ½é‡åˆ°çš„é—®é¢˜ã€‚**
    ç­”ï¼šä¸æ˜¯ã€‚æ®‹å·®ç»“æ„æ˜¾è‘—ç¼“è§£ä½†ä¸èƒ½å®Œå…¨æ¶ˆé™¤æ‰€æœ‰é—®é¢˜ï¼šä¾‹å¦‚åœ¨æç«¯æ·±åº¦ä¸‹ä»éœ€è‰¯å¥½å½’ä¸€åŒ–/åˆå§‹åŒ–æŠ€å·§ï¼ŒBN åœ¨å°‘æ‰¹æ¬¡åœºæ™¯è¡¨ç°å·®ï¼›æ­¤å¤–ï¼Œè®¡ç®—ä¸å†…å­˜å¼€é”€ã€æ¨ç†å»¶è¿Ÿã€æ¨¡å‹è§£é‡Šæ€§ä»æ˜¯å®é™…é—®é¢˜ã€‚æ”¹è¿›å¯åŒ…æ‹¬å½’ä¸€åŒ–æ›¿ä»£ã€æ¨¡å‹å‹ç¼©å’Œç†è®ºåˆ†æã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)
10. **é—®ï¼ˆå·¥ç¨‹å®è·µï¼‰ï¼šä½ å¦‚ä½•æŠŠ ResNet è¿ç§»åˆ°ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Ÿ**
     ç­”ï¼šæŠŠ ResNet å½“ä½œ backbone æå–å¤šå±‚ç‰¹å¾ï¼ˆé€šå¸¸å»æ‰æœ€åçš„ FCï¼Œä¿ç•™ conv ç‰¹å¾ï¼‰ï¼Œå†ä¸ RPN + Fast/Faster R-CNN æˆ– FPN ç­‰æ£€æµ‹å¤´è¿æ¥ã€‚è®ºæ–‡åœ¨ Faster R-CNN / RPN æ¡†æ¶ä¸Šå±•ç¤ºäº†è¿ç§»æ•ˆæœå¹¶åœ¨ COCO/ILSVRC detection ä¸Šå–å¾—æå‡ã€‚[ar5iv](https://ar5iv.org/pdf/1512.03385)











