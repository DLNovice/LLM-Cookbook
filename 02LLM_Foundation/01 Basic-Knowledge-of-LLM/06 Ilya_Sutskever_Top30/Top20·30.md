## 2015.12 Deep Speech 2 

ËÆ∫ÊñáÂú∞ÂùÄÔºö[Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/pdf/1512.02595)

------

Ê¶ÇËø∞Ôºö

- **Authors:** Baidu Research ‚Äì Silicon Valley AI Lab
- **Abstract:** The paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.
- **Introduction:** Traditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.
- **Model Architecture:** The model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.
- **Training Data:** Training leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.
- **Results:**
  - **English:** Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.
  - **Mandarin:** The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.
- **Deployment:** The system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.
- **Conclusion:** Deep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.
- This summary covers the main findings and contributions of the Deep Speech 2 paper, highlighting its end-to-end deep learning approach, architectural innovations, and significant performance improvements in both English and Mandarin speech recognition.

------



## 2020.01 Scaling Laws for Neural Language Models

ËÆ∫ÊñáÂú∞ÂùÄÔºöhttps://arxiv.org/abs/2001.08361

Ëß£ËØªËßÜÈ¢ëÔºöhttps://www.bilibili.com/video/BV1rRWXzfEej

------

Áî±2020Âπ¥OpenAIÂèëÂ∏ÉÔºåÂΩìÂâç‰∏ªË¶ÅËÆ≤Ëß£‰∏§ÁØáËÆ∫ÊñáÔºö

- 2020.01 Scaling Laws (OpenAI)
- 2022.03 Chinchilla (Google)ÔºöGoogle ÁöÑ Scaling Laws

Âú®Ê≠§‰πãÂâçÔºåË°•ÂÖÖÂá†‰∏™Ê¶ÇÂøµÔºö

- 1993Âπ¥ÊúâÁØá„Ää Learning Curves „ÄãÔºåÊèêÂá∫‰∫Ü xxx„ÄÇ
- ÂÖ∂Ê¨°ÔºåÊàë‰ª¨Ë¶Å‰∫ÜËß£‰∏Ä‰∏ã Power-law RegionÔºåËøôÊòØ xxx„ÄÇ

------

ÂÖ≥‰∫é 2020.01 Scaling Laws (OpenAI)Ôºå‰ªé‰∏â‰∏™Áª¥Â∫¶ÁúãÈóÆÈ¢òÔºåËÄÅ‰∏âÊ†∑‰∫Ü„ÄÇ

- ÁÆóÂäõÔºöxxx
- Ê®°ÂûãÂ§ßÂ∞èÔºöxxx
- Êï∞ÊçÆÔºöxxx

‰∏Ä‰∫õÊé®ËÆ∫Ôºö

- Smooth power laws.
- Transfer improvers with test performance. ‰∏ÄÁßçËøÅÁßªÂ≠¶‰π†ÁöÑËÉΩÂäõ

------

ÂÖ≥‰∫é 2022.03 Chinchilla (Google)ÔºåÂá†Âº†ÂõæÂéªÊ¶ÇËø∞ËøôÁØáÊñáÁ´†Ôºö

xxx

------

TakeawaysÔºàÊ†∏ÂøÉ‰ø°ÊÅØ/ÂêØÁ§∫ÔºâÔºö

- Early: abundance D,bound on C, increase N
- Now: bound on D, aboundounce C (?)
- Others:
  - Mamba  / MoE
  - Minimize inference cost
  - Scale in post training: 2025Âπ¥RLÂæàÁÅ´
  - Test time scale

2025Âπ¥ÂàùÊó∂ÔºållyaËØ¥Êàë‰ª¨Âç≥Â∞ÜËÄóÂ∞Ω‰∫íËÅîÁΩë‰∏äÁöÑÊâÄÊúâÈ´òË¥®ÈáèÊï∞ÊçÆÔºåScaling LawÂèàÂ∞ÜËµ∞Âêë‰ΩïÊñπÔºü





## 2004.06 MDL

ËÆ∫ÊñáÂú∞ÂùÄÔºö[A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/pdf/math/0406077)

------

Ê¶ÇËø∞Ôºö

- Authors: Peter Gr√ºnwald
- This paper provides an extensive introduction and technical exposition on Rissanen‚Äôs Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.
- Key Technical Details:
  1. **MDL and Data Compression**: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.
  2. **Kolmogorov Complexity and MDL**: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.
  3. **Practical MDL**: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.
  4. **Refined and Crude MDL**: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.
  5. **MDL for Model Selection**: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.
  6. **Statistical and Information Theoretic Underpinnings**: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.
  7. **Applications and Extensions**: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.
- The document serves as a comprehensive introduction to MDL, providing essential insights into both the theoretical and practical aspects of the principle. It emphasizes the importance of MDL in selecting models that are not just good at fitting the data, but also in providing meaningful insights in a parsimonious way .

------





## 2008.06 Machine Super Intelligence

ËÆ∫ÊñáÂú∞ÂùÄÔºö[Machine Super Intelligence](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf)

------

Ê¶ÇËø∞ÔºöShane Legg‚Äôs dissertation, ‚ÄúMachine Super Intelligence,‚Äù presents an extensive analysis of the challenges and theoretical foundations underlying the development of superintelligent machines. Key technical discussions in the thesis include:

1. **Framework for Intelligence Measures:** Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.
2. **Superintelligence Pathways:** The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.
3. **Algorithmic Insights into Intelligence:** Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.
4. **Theoretical Models of Machine Learning:** Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.
5. **Safety and Control:** A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.

- These components of Legg‚Äôs dissertation provide a deep theoretical foundation for understanding and advancing toward the development of superintelligent AI systems, while also addressing the critical issues of control and safety in such developments.

------



## 20xx.xx Kolmogorov Complexity and Algorithmic Randomness

ËÆ∫ÊñáÂú∞ÂùÄÔºö[Kolmogorov Complexity and Algorithmic Randomness](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf)

------

Ê¶ÇËø∞Ôºö

- The book ‚ÄúKolmogorov Complexity and Algorithmic Randomness‚Äù by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:
- **Definition and Significance**: Kolmogorov complexity is defined as the shortest binary program (in the sense of Turing machine code) that can generate a given string and then halt. The complexity measures the amount of information contained in the string, essentially quantifying its randomness.
- **Unpredictability and Random Sequences**: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.
- Theoretical Foundations
  - **Formalisms and Proofs**: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.
  - **Incompressibility Method**: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.
- Practical Applications
  - **Data Compression**: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.
  - **Psychological Models**: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.
- Advanced Topics
  - **Mutual Information**: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.
  - **Conditional Complexity**: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.
- Mathematical Rigor
  - **Deep Mathematical Analysis**: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.
- **Future Directions**: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.
- This book is a valuable resource for researchers, scholars, and students interested in the deep mathematical structures that underlie information theory, computer science, and related disciplines. It not only provides a rigorous introduction to Kolmogorov complexity and algorithmic randomness but also explores their implications in practical and theoretical domains.

------



## CS231n

ËØæÁ®ãÂú∞ÂùÄÔºö[Stanford‚Äôs CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/)



## Meta

### [Better & Faster Large Language Models Via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)

- Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi√®re, David Lopez-Paz, and Gabriel Synnaeve
- The recent advancements in large language models (LLMs) have primarily revolved around the next-token prediction methodology. However, a novel approach introduced in the paper titled ‚ÄúBetter & Faster Large Language Models via Multi-token Prediction‚Äù suggests a significant shift towards predicting multiple tokens simultaneously. This method not only enhances the efficiency and speed of LLMs but also demonstrates considerable improvements in model performance across various tasks, especially in coding benchmarks.
- The multi-token prediction architecture redefines how LLMs process and generate text by allowing the model to predict several future tokens at once. Unlike traditional architectures that predict the next single token sequentially, this approach utilizes multiple independent output heads that work in parallel, significantly speeding up the training and inference processes.
- At the core of the multi-token prediction architecture is the shared trunk, a common feature extractor that processes the input data. This trunk is responsible for producing a rich, contextualized representation of the input, which is then fed into multiple output heads. Each head is tasked with predicting a different future token based on the shared representation, ensuring that all predicted tokens are contextually coherent and relevant.
- The introduction of multi-token prediction architecture has several profound implications. Firstly, it enhances sample efficiency, meaning the model requires fewer data iterations to achieve high performance. Secondly, it significantly speeds up the inference process, as multiple tokens can be generated in parallel, reducing the time needed to produce outputs. This architecture also shows great scalability with increased model size, making it particularly effective for larger models that traditionally face bottlenecks in speed and efficiency.
- Empirical results from the study highlight the effectiveness of the multi-token prediction model. On coding benchmarks like HumanEval and MBPP, models equipped with this new architecture outperform traditional next-token prediction models by a considerable margin. For instance, models trained with multi-token prediction solve up to 17% more problems on MBPP and demonstrate similar improvements on HumanEval.
- Moreover, these models are up to three times faster at inference compared to their traditional counterparts. This speed increase is crucial for real-time applications and services that rely on quick responses from LLMs. The architecture‚Äôs benefits are also more pronounced as the model size increases, which confirms its suitability for large-scale implementations where efficiency and speed are critical.
- Thus, the multi-token prediction architecture presents a viable and promising alternative to the conventional methodologies used in training large language models, pushing the boundaries of what is possible in natural language processing and machine learning.

Key Takeaways:

- üîπ The model consists of a shared trunk and several independent output heads. It processes incoming data to generate a contextualized representation, which is then utilized simultaneously by all output heads for predicting multiple future tokens.
- üîπ Departing from traditional single-token prediction, this model enables simultaneous prediction of multiple tokens, significantly accelerating both training and inference processes.
- üîπ The shared trunk, built on transformer technology, extracts a latent representation from the input data. This unified representation is shared across all output heads, ensuring consistent and coherent predictions.
- üîπ Each output head functions independently to predict a distinct future token. This design reduces the sequential dependencies typical in conventional language models, enhancing the model‚Äôs efficiency.
- üîπ The model‚Äôs ability to make multiple predictions concurrently not only speeds up learning but also improves sample efficiency. This results in quicker model convergence and less data required for effective training.
- üîπ At the inference stage, the model can leverage all output heads simultaneously, leading to swift generation of text sequences. This is particularly advantageous for real-time application scenarios.

### [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf)

- Authors: Vladimir Karpukhin, Barlas Oguz,Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih
- In open-domain question answering (system‚Äôs capability to answer questions on any topic rather than being restricted on a specific domain), it‚Äôs vital to efficiently identify the right passages from vast information sources (retrieval). Traditional methods, like TF-IDF and BM25, utilize sparse vector models to pick these passages. However, Karpukhin and colleagues in their 2020 EMNLP paper demonstrate a novel approach: using dense vector representations. They employ a dual-encoder framework to generate embeddings from a select set of questions and passages.
- Their objective is metric learning: crafting a vector space where relevant question-passage pairs are closer together than unrelated ones. They optimize this by focusing on the likelihood of selecting the correct (positive) passage amidst a sea of irrelevant (negative) ones.
- Collecting negative examples for training from such a vast pool is challenging. Their solution? Utilizing random passages, ones that match the most question tokens without the actual answer (via BM25), and relevant passages paired with other questions. The most effective model they produced uses these ‚Äúgold‚Äù passages from the same training batch as negative instances, combined with one BM25 negative passage.
- Results were promising. When tested on diverse open-domain QA datasets, their model greatly outperformed the established Lucene-BM25 system, enhancing top-20 passage retrieval accuracy by 9%-19%. This led to their model setting new performance benchmarks in open-domain QA.

Dense Passage Retriever (DPR):

1. **Purpose**: The goal of the DPR is to improve the retrieval component in open-domain QA. This involves efficiently retrieving relevant text passages from a vast collection when given a question.
2. **Key Task**: Given a large number M of text passages, the DPR aims to index all of these passages in a low-dimensional continuous space, making it efficient to retrieve the top k most relevant passages for a given input question. M can be very large, like 21 million passages, but k (the number of passages we want to retrieve for a given question) is relatively small, often between 20 and 100.
3. **DPR‚Äôs Mechanism**:
   - **Dense Encoder for Passages EP(‚ãÖ)**: It converts any text passage to a d-dimensional real-valued vector. This encoder processes and indexes all M passages for retrieval.
   - **Encoder for Questions EQ(‚ãÖ)**: At runtime, when a question is posed, this encoder turns the question into a d-dimensional vector.
   - **Similarity Measurement**: The similarity between a question and a passage is calculated using the dot product of their respective vectors: sim(q,p)=EQ(q)‚ãÖEP(p).
4. **Passage Size and Boundaries**: The passage‚Äôs size and the decision of where a passage begins and ends affect the retriever and reader. Fixed-length passages have been found to be more effective in retrieval and QA accuracy.
5. **Encoders Implementation**: The encoders for both questions and passages are based on BERT networks, a popular deep learning model for NLP. They use the representation at the [CLS] token as the output, meaning the output vector has 768 dimensions.
6. **Inference**: During the process of answering a question, the system uses the passage encoder to process all passages and then indexes them using FAISS, an efficient library for similarity search. For any given question, its embedding is computed, and the top k passages with the closest embeddings are retrieved.
7. **Training**:
   - The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.
   - The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.
   - For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.
8. **In-batch Negatives**: A training optimization method is discussed where they use relevant passages from the same batch of questions as negatives, which makes computation more efficient. This technique leverages the similarities between passages in the same batch to boost the number of training examples, effectively reusing computation.

### [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401v4)

- The paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.
- Addressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.
- The RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.
- The retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.
- RAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.
- In open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn‚Äôt in any retrieved document.
- RAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.
- On the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.
- This study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.

## HuggingFace

### [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/pdf/2310.16944.pdf)

- Authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf
- The paper introduces a technique termed ‚Äúdistilled direct preference optimization‚Äù (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:
  1. Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.
  2. AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the ‚Äúchosen‚Äù response and one random lower scoring response as the ‚Äúrejected‚Äù response. This provides training pairs of good vs bad responses.
  3. Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the ‚Äúchosen‚Äù responses higher than ‚Äúrejected‚Äù responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.
- They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.
- Results:
  - Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.
  - It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.
  - Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.
- The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.
- The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.
- Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.
- The image below [(source)](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) gives a graphical sense of Zephyr‚Äôs performance on tasks as compared with our LLMs.



## Stanford

### [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2302.12345)

- This paper by Liu et al. from Stanford University, University of California Berkeley, and Samaya AI, focuses on analyzing language models‚Äô performance in tasks that require identifying relevant information in long input contexts. The research particularly highlights issues in multi-document question answering and key-value retrieval tasks, revealing a significant degradation in performance when relevant information is situated in the middle of lengthy contexts.
- The study involved an experimental setup for multi-document question answering. Models were tasked with identifying relevant information from a set of documents to answer questions. The researchers manipulated both the length of the input context and the position of the relevant information to observe changes in task performance.
- Several state-of-the-art open and closed language models were evaluated. Among the open models were MPT-30B-Instruct, capable of handling up to 8192 tokens, and LongChat-13B (16K), which extends the context window to 16384 tokens. Closed models included GPT-3.5-Turbo and its variant with an expanded context length of 16K tokens, as well as Claude-1.3 and Claude-1.3 (100K).
- The results revealed a distinct U-shaped performance curve across these models. They performed best when relevant information appeared at the beginning or end of the input context. However, the performance significantly declined when accessing information in the middle of long contexts, challenging the efficacy of extended-context models in utilizing their input effectively.
- A synthetic key-value retrieval task was also used to assess models‚Äô ability to retrieve exact matches from an input context. The task‚Äôs simplicity varied across models, with some achieving near-perfect performance, while others struggled with larger contexts.
- The study also explored the impact of model architecture on context usage, comparing decoder-only and encoder-decoder models. Encoder-decoder models like Flan-T5-XXL and Flan-UL2 exhibited more stable performance across various contexts. However, they also began to show performance degradation with sequences longer than their training-time context windows.
- The impact of query-aware contextualization was examined. While this dramatically improved performance in the key-value retrieval task, it had only a minimal effect on the multi-document question answering task.
- Instruction fine-tuning‚Äôs effect was analyzed by comparing models like MPT-30B and MPT-30B-Instruct, both fine-tuned for instructions. Both models showed similar U-shaped performance curves, indicating that instruction fine-tuning alone is not responsible for these trends.
- In a case study on open-domain question answering, the research found that model performance does not always improve with an increase in the amount of context provided. The study observed that performance saturates before retriever recall, suggesting that providing too much context may not be beneficial and could potentially reduce accuracy.

## Misc

### [Precise Zero-Shot Dense Retrieval Without Relevance Labels](https://arxiv.org/abs/2212.10496)

- The paper by Gao, Ma, Lin, and Callan from Carnegie Mellon University and University of Waterloo introduces Hypothetical Document Embeddings (HyDE), a novel approach for fully zero-shot dense retrieval in the absence of relevance labels. HyDE utilizes instruction-following language models (like InstructGPT) to generate a hypothetical document capturing relevance patterns, although these documents may contain inaccuracies or fictional details.
- Dense retrieval has been effective across various tasks and languages but creating an effective fully zero-shot dense retrieval system without relevance labels remains challenging. Traditional methods like negative mining, distillation, and task-specific pre-training have been proposed to enhance supervised dense retrieval models, yet zero-shot dense retrieval still presents difficulties.
- HyDE‚Äôs methodology involves two main steps: generating a hypothetical document that answers the query, and then encoding this document into an embedding vector using an unsupervised contrastively learned encoder like Contriever. This process pivots away from traditional dense retrieval‚Äôs reliance on relevance judgments, instead utilizing a language model‚Äôs ability to generate relevant content.
- Experiments conducted with HyDE used InstructGPT and Contriever models, along with datasets such as TREC DL19, DL20 (based on MS-MARCO), and a collection from the BEIR dataset for web search, question answering, fact verification, and non-English retrieval tasks. The results showed that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and is comparable to fine-tuned retrievers across these tasks and languages.
- The paper concludes by reflecting on HyDE‚Äôs novel approach to relevance modeling, which shifts from traditional numerical relevance scores to leveraging natural language generation models. This paradigm suggests a future where the need for relevance labels might be eliminated, and relevance modeling and instruction understanding can be delegated to more powerful and flexible language models. HyDE is practical in the initial stages of a search system‚Äôs life, providing performance comparable to fine-tuned models without reliance on relevance labels.

### [ALCUNA: Large Language Models Meet New Knowledge](https://arxiv.org/pdf/2310.14820.pdf)

- Authors: Xunjian Yin, Baizhou Huang, and Xiaojun Wan
- The paper proposes a new method called KnowGen to generate artificial entities with new knowledge by making changes to the attributes and relationships of existing entities. This simulates the natural process of new knowledge emerging in the real world.
- KnowGen is applied to structured biological taxonomic data from the EOL database to create artificial organisms. This results in a benchmark dataset called ALCUNA for evaluating large language models (LLMs) on their ability to handle new knowledge.
- ALCUNA contains questions testing the model‚Äôs knowledge understanding, differentiation, and association abilities when faced with new entities.
- Several popular LLMs like ChatGPT, Alpaca, Vicuna, and ChatGLM are evaluated on ALCUNA in zero-shot and few-shot settings. The results show these models still struggle with reasoning between new and existing knowledge.
- Analysis reveals factors impacting model performance on new knowledge like entity similarity, contextual knowledge, and input representation format.
- The paper argues benchmarks with truly new knowledge like ALCUNA are important to drive progress in LLMs‚Äô ability to understand and reason with new information, as opposed to existing knowledge already seen during training.
- The artificial nature of the knowledge in ALCUNA makes it reusable as a standard benchmark to assess different models on new knowledge without having to collect new data repeatedly.
- This paper proposes a novel method to automatically generate new structured knowledge for evaluating LLMs‚Äô capabilities in more realistic and challenging settings involving unfamiliar information. The ALCUNA benchmark constructed using this approach provides insights into current model limitations and opportunities for improvement.

### [The Perils & Promises of Fact-checking with Large Language Models](https://arxiv.org/pdf/2310.13549.pdf)

- Authors: Dorian Quelle & Alexandre Bovet
- The paper evaluates using large language models (LLMs) like GPT-3.5 and GPT-4 for automated fact-checking of claims. This is important as LLMs are being used more in high stakes domains like research and journalism.
- They test the models on two datasets: PolitFact (US political claims) and a multilingual dataset from Data Commons. The models are evaluated with and without providing contextual information from web searches.
- Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.
- Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.
- Key Results:
  - GPT-4 outperformed GPT-3.5 overall.
  - Providing context significantly improved accuracy, highlighting the importance of evidence gathering.
  - Models struggled with ambiguous ‚Äúhalf-true‚Äù type verdicts.
  - Performance varied across languages - non-English claims saw a boost when translated to English first.
  - No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.
- Limitations:
  - Biased evaluation due to use of GPT-4 as a scorer.
  - Did not explore model scaling or curating better training data.
  - Safety/ethics of potential misinformation not addressed.
- Implications:
  - LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.
  - Critical examination of LLM reasoning is important before deployment.
  - Understanding model limitations and language-specific differences is key.
  - Continued learning after initial training needs more investigation.
- The paper provides a comprehensive evaluation of GPT-3.5 and GPT-4 on fact-checking, using novel context retrieval and multilingual data. Key findings highlight the models‚Äô strengths as well as areas needing improvement before responsible LLM-assisted fact-checking.

