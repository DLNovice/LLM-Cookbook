æ‚¨å¥½ï¼Œæˆ‘ç›®å‰åœ¨åŸºäºè´µå¹³å°æµ‹è¯•åœ¨å„ä¸ªæ˜¾å¡é…ç½®ä¸‹å¤§æ¨¡å‹æ€§èƒ½ï¼Œéœ€è¦è¯·æ•™ä¸€ä¸‹è´µå¹³å°GPUç›¸å…³çš„å‡ ä¸ªé—®é¢˜ï¼š

- è¯·é—®è´µå¹³å°çš„4090ç¡®å®šæ˜¯æ»¡è¡€4090å—ï¼Ÿå…¶ä½™ç±»å‹æ˜¾å¡çš„ç®—åŠ›æ˜¯å¦æœ‰å‡ºå…¥ï¼Ÿ
  - NVIDIA RTX 4090 å¹¶æœªåšä»»æ„é˜‰å‰²ï¼Œä¸º ada æ¶æ„ï¼Œæœ¬èº«æ²¡æœ‰ nvlink

- è¯·é—®å¤šå¡A6000æˆ–è€…3090ç­‰æ”¯æŒnvlinkçš„æ˜¾å¡ï¼Œæ˜¯é‡‡ç”¨nvlinkè¿æ¥çš„ï¼Œè¿˜æ˜¯é‡‡ç”¨pcieï¼Ÿè¯·é—®å¹³å°ä¸Šçš„A6000æ˜¯å®‰åŸ¹æ¶æ„çš„å—ï¼Ÿ
  - RTX 3090 ä¸º PCIeï¼ŒA6000 æ˜¯ ampere æ¶æ„ï¼Œä¸¤ä¸¤æœ‰ nvlinkï¼Œæ”¯æŒ PCIeï¼Œä¸æ”¯æŒæ›´å¤šå¡çš„äº’è”ï¼›8 å¡ 4090 å’Œ A6000 è®­ç»ƒå¤šå¡æˆ–è€…è·¨èŠ‚ç‚¹äº’è”éœ€è¦å·¥ç¨‹è°ƒä¼˜ï¼Œæ‚¨å¯ä»¥å…ˆæµ‹è¯•çœ‹çœ‹ã€‚




æ˜¾å¡æ€§èƒ½è®¤çŸ¥ï¼š

- Ollamaï¼š
  - å•å¼ 4090ï¼šdeepseek-r1:32b-qwen-distill-q4_K_Mã€qwen2.5:32b-instruct-q4_K_M
  - ä¸‰å¼ 4090ï¼šæ»¡è¡€DeepSeek-R1-32Bï¼Œä½†æ˜¯é‡åŒ–æ­¤æ¨¡å‹ä¼šæ˜¾å­˜çˆ†ç‚¸
  - å››å¼ 4090ï¼šqwen2.5:72b-instruct-q4_K_Mã€deepseek-r1:70b-llama-distill-q4_K_M
  - ä¸€å¼ A6000ï¼šqwen2.5:72b-instruct-q4_K_Mã€deepseek-r1:70b-llama-distill-q4_K_M
  - ä¸¤å¼ A6000ï¼šdeepseek-r1:32b-qwen-distill-fp16

- vLLMï¼š
  - ä¸€å¼ 4090å·²ç»ä¸å¤Ÿè·‘qwen2.5:32b-instruct-q4_K_Mï¼Œæ˜¾å­˜çˆ†ç‚¸







# 01 æµ‹è¯•æ–¹æ¡ˆ

## ä¸€ã€æµ‹è¯•tokenè¾“å‡ºé€Ÿåº¦

> éœ€è¦æµ‹è¯•é¦–Tokenå»¶è¿Ÿ(TTFT)å’Œå…¶ä½™Tokenå»¶è¿Ÿ (TPOT)

åˆç†çš„tokenè¾“å‡ºé€Ÿåº¦ï¼š10-15 tokens/ç§’ï¼Œå‚è€ƒhttps://www.zhihu.com/question/591112394/answer/3164341451

```
æˆå¹´äººçš„é˜…è¯»é€Ÿåº¦ï¼šè‹±æ–‡æ–‡æœ¬å¹³å‡é˜…è¯»é€Ÿåº¦å¤§çº¦æ˜¯200-300è¯/åˆ†é’Ÿï¼Œå³3-5è¯/ç§’ã€‚ä¸­æ–‡æ–‡æœ¬å¹³å‡é˜…è¯»é€Ÿåº¦å¤§çº¦æ˜¯300-500å­—ç¬¦/åˆ†é’Ÿï¼Œå³5-8å­—ç¬¦/ç§’ã€‚

è‹±æ–‡ä¸­çš„ä¸€ä¸ªtokenå¹¶ä¸æ€»æ˜¯å¯¹åº”ä¸€ä¸ªå®Œæ•´çš„å•è¯ã€‚æ‰€ä»¥æ¨¡å‹è¾“å‡ºé€Ÿåº¦å¯èƒ½éœ€è¦æé«˜åˆ°**5-10 tokens/ç§’**ã€‚å¯¹äºä¸­æ–‡ï¼Œç”±äºå…¶å­—ç¬¦æ€§è´¨ï¼Œå¸¸è§çš„åˆ†è¯å™¨å¯èƒ½ä¼šå°†æ¯ä¸ªå­—ç¬¦è§†ä¸ºä¸€ä¸ªtokenï¼Œä½†ä¹Ÿå¯èƒ½ä¼šå°†å¸¸è§çš„è¯æ±‡æˆ–çŸ­è¯­åˆå¹¶ä¸ºä¸€ä¸ªtokenã€‚è¾“å‡ºé€Ÿåº¦åº”è¾¾åˆ°**5-8 tokens/ç§’ã€‚**

å®é™…ä¸Šï¼Œä¸ºäº†æä¾›æµç•…çš„ç”¨æˆ·ä½“éªŒï¼Œæ¨¡å‹çš„è¾“å‡ºé€Ÿåº¦å¯èƒ½éœ€è¦è¿œè¿œè¶…è¿‡è¿™ä¸ªé€Ÿåº¦ï¼Œå°¤å…¶æ˜¯åœ¨äº¤äº’å¼åº”ç”¨ä¸­ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿå³æ—¶å“åº”ã€‚åˆç†çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹çš„è¾“å‡ºé€Ÿåº¦**è‡³å°‘è¾¾åˆ°10-15 tokens/ç§’ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå•tokençš„å»¶è¿Ÿåœ¨100msä»¥å†…ï¼Œå¯ç¡®ä¿æµç•…çš„ç”¨æˆ·ä½“éªŒã€‚**
```

ä½†ä¸æ˜¯ç”¨æˆ·å¯èƒ½æ›´å–œæ¬¢éæµå¼ï¼Œç­‰æ¨¡å‹è¾“å‡ºå®Œæ¯•åå†æŸ¥çœ‹ï¼Œæ‰€ä»¥ä¸€æ–¹é¢tokené€Ÿåº¦è¦è¶³å¤Ÿå¿«ï¼Œå…¶æ¬¡ï¼Œæ ¹æ®å®é™…éœ€æ±‚ï¼Œè€ƒè™‘æ˜¯å¦ä¸€å®šè¦ç”¨æ¨ç†æ¨¡å‹ã€‚



### 1ã€åŸºäºollama

å®‰è£…ollama

```
curl -fsSL https://ollama.com/install.sh | sh

ollama run qwen2.5:0.5b
```

æµ‹è¯•tokenè¾“å‡ºé€Ÿåº¦ï¼š

- åŸºäºverboseå‚æ•°ï¼š`ollama run ä½ è¦è·‘çš„æ¨¡å‹ - -verbose`
  ![image-20250115152959172](./assets/image-20250115152959172.png)
- åŸºäºOllama APIæµ‹è¯• Tokené€Ÿåº¦
- åŸºäºå¼€æºé¡¹ç›®ï¼š
  - é¡¹ç›®1ï¼ˆæ— äººå¼•ç”¨ï¼Œæµ‹è¯•æ•ˆæœä¸ä½³ï¼‰ï¼šhttps://github.com/robbiemu/ollama_token_bench
    ![image-20250115155706241](./assets/image-20250115155706241.png)



### 2ã€åŸºäºDify

ç›´æ¥æŸ¥çœ‹èµ„æºç›‘æ§å³å¯ã€‚





### 3ã€Transformers

ç›´æ¥æºç è°ƒç”¨æ¨¡å‹å¹¶æµ‹è¯•ï¼Œå¦‚qwen2.5ï¼šhttps://github.com/QwenLM/Qwen2.5

å®˜æ–¹æä¾›äº†ç¤ºä¾‹ä½¿ç”¨ä»£ç ï¼Œæ³¨æ„æŸ¥é˜…æœ€æ–°çš„æ–‡æ¡£ï¼Œéƒ¨åˆ†æ—§çš„apiå·²æ— æ³•ä½¿ç”¨



### 4ã€åŸºäºvLLM

ç•¥



## äºŒã€æµ‹è¯•æ˜¾å­˜ç­‰å ç”¨

åŸºäºnvidia-smi



åŸºäºç¬¬ä¸‰æ–¹å·¥å…·ï¼š

- https://techdiylife.github.io/blog/topic.html?category2=t05&blogid=0031



## ä¸‰ã€æµ‹è¯•ååé‡

**ååé‡**æ˜¯è¡¡é‡ç³»ç»Ÿå¤„ç†èƒ½åŠ›çš„æ ¸å¿ƒæŒ‡æ ‡ï¼Œè¡¨ç¤ºå•ä½æ—¶é—´å†…ç³»ç»Ÿå®Œæˆçš„æœ‰æ•ˆå·¥ä½œé‡ã€‚åœ¨AIæ¨ç†åœºæ™¯ä¸­ï¼š

1. **è¯·æ±‚ååé‡ (Requests/s)**
   - å®šä¹‰ï¼šæ¯ç§’æˆåŠŸå¤„ç†çš„å®Œæ•´è¯·æ±‚æ•°é‡
   - æ„ä¹‰ï¼šåæ˜ ç³»ç»Ÿæ•´ä½“æœåŠ¡èƒ½åŠ›ï¼Œæ•°å€¼è¶Šå¤§è¯´æ˜ç³»ç»Ÿèƒ½åŒæ—¶å“åº”æ›´å¤šç”¨æˆ·è¯·æ±‚
   - ç¤ºä¾‹ï¼šè‹¥å€¼ä¸º10 Requests/sï¼Œè¡¨ç¤ºæ¯ç§’å¯å¤„ç†10ä¸ªç‹¬ç«‹é—®é¢˜
2. **Tokenååé‡ (Tokens/s)**
   - å®šä¹‰ï¼šæ¯ç§’ç”Ÿæˆçš„æœ‰æ•ˆæ–‡æœ¬å•å…ƒï¼ˆTokenï¼‰æ€»æ•°
   - æ„ä¹‰ï¼šä½“ç°æ–‡æœ¬ç”Ÿæˆæ•ˆç‡ï¼Œæ•°å€¼è¶Šå¤§è¡¨ç¤ºç”Ÿæˆå†…å®¹çš„é€Ÿåº¦è¶Šå¿«
   - ç¤ºä¾‹ï¼šè‹¥å€¼ä¸º500 Tokens/sï¼Œè¡¨ç¤ºæ¯ç§’å¯ç”Ÿæˆç›¸å½“äº500ä¸ªæ±‰å­—/å•è¯çš„å†…å®¹

ä¸¤è€…å…³ç³»ï¼š

1. Tokenååé‡ = è¯·æ±‚ååé‡ Ã— å¹³å‡æ¯è¯·æ±‚ç”ŸæˆTokenæ•°
2. é«˜è¯·æ±‚ååé‡é€‚åˆçŸ­é—®ç­”åœºæ™¯ï¼Œé«˜Tokenååé‡é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ã€‚



å¸¸è§é—®é¢˜ï¼š

- Q1ï¼šååé‡è¶Šå¤§ï¼Œç”¨æˆ·çš„æ¨ç†é€Ÿåº¦å°±è¶Šå¿«å—ï¼Ÿ

  - **ä¸ä¸€å®šã€‚** ååé‡çš„æé«˜å¹¶ä¸æ€»æ˜¯æ„å‘³ç€å•ä¸ªç”¨æˆ·çš„æ¨ç†é€Ÿåº¦ï¼ˆå³**æ¨ç†å»¶è¿Ÿ**ï¼‰ä¼šå˜å¿«ï¼Œè¿™ä¸»è¦å–å†³äºç³»ç»Ÿçš„è°ƒåº¦æ–¹å¼å’Œèµ„æºåˆ†é…ç­–ç•¥ï¼š
    - **ååé‡ï¼ˆThroughputï¼‰** å…³æ³¨çš„æ˜¯å•ä½æ—¶é—´å†…æ¨¡å‹èƒ½å¤„ç†å¤šå°‘è¯·æ±‚ï¼ˆQPSï¼‰æˆ– Tokenï¼ˆToken/sï¼‰ã€‚ï¼ˆå…³é”®å› ç´ ï¼šæ‰¹å¤„ç†Batchingã€å¹¶å‘è°ƒåº¦Concurrency Schedulingã€ç¡¬ä»¶èµ„æºçš„é™åˆ¶ï¼‰
    - **æ¨ç†é€Ÿåº¦ï¼ˆLatencyï¼‰** å…³æ³¨çš„æ˜¯**å•ä¸ªè¯·æ±‚**çš„å¤„ç†æ—¶é—´ï¼Œå³ä»ç”¨æˆ·æäº¤è¯·æ±‚åˆ°æ”¶åˆ°å“åº”æ‰€èŠ±çš„æ—¶é—´ã€‚

- Q2ï¼šå¦‚ä½•æµ‹è¯•ååé‡ï¼Ÿ

  ååé‡æµ‹è¯•é€šå¸¸é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

  1. **å›ºå®š QPS æ–¹å¼**ï¼š
     - è®¾ç½®å›ºå®šçš„è¯·æ±‚é€Ÿç‡ï¼ˆå¦‚ 1000 QPSï¼‰ï¼Œè§‚å¯Ÿç³»ç»Ÿæ˜¯å¦èƒ½ç¨³å®šå¤„ç†å¹¶è®¡ç®—ååé‡ã€‚
     - ä¸»è¦è¡¡é‡æˆåŠŸå¤„ç†çš„è¯·æ±‚æ•°ã€å¤±è´¥ç‡ã€å»¶è¿Ÿå˜åŒ–ã€‚
  2. **å¹¶å‘è´Ÿè½½é€’å¢ï¼ˆConcurrency Scalingï¼‰**ï¼š
     - é€æ­¥å¢åŠ å¹¶å‘ç”¨æˆ·æ•°ï¼ˆä¾‹å¦‚ä» 1 æé«˜åˆ° 100ã€500ã€1000ï¼‰ï¼Œè§‚å¯Ÿç³»ç»Ÿåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„ååé‡å˜åŒ–ã€‚
     - ç¡®å®šååé‡è¾¾åˆ°ç“¶é¢ˆçš„ä½ç½®ã€‚
  3. **æ‰¹å¤„ç†ååæµ‹è¯•ï¼ˆBatch Throughput Testingï¼‰**ï¼š
     - æµ‹è¯•ä¸åŒæ‰¹å¤§å°ï¼ˆBatch Size = 1, 8, 16, 32, 64ï¼‰å¯¹ååé‡çš„å½±å“ï¼Œæ‰¾å‡ºæœ€ä½³ Batch Sizeã€‚
  4. **Token é€Ÿç‡æµ‹è¯•ï¼ˆToken Per Second, TPSï¼‰**ï¼ˆé€‚ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼‰ï¼š
     - è®¡ç®—æ¯ç§’ç”Ÿæˆçš„ Token æ•°é‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„å¤„ç†èƒ½åŠ›ã€‚

- Q3ï¼šGPU åˆ©ç”¨ç‡æ˜¯å¦å¿…é¡»å¾ˆé«˜ï¼Ÿ

  **ä¸ä¸€å®šã€‚** è™½ç„¶é«˜ååé‡é€šå¸¸ä¼šå¯¼è‡´ GPU åˆ©ç”¨ç‡å‡é«˜ï¼Œä½† GPU èµ„æºåˆ©ç”¨ç‡å¹¶ä¸æ˜¯å”¯ä¸€è¡¡é‡ååé‡çš„æŒ‡æ ‡ï¼š

  - **GPU åˆ©ç”¨ç‡é«˜ â‰  é«˜ååé‡**
    - å¦‚æœ GPU åˆ©ç”¨ç‡é«˜ä½†ååé‡ä½ï¼Œå¯èƒ½æ˜¯å› ä¸ºè®¡ç®—æ•ˆç‡ä½ï¼Œä¾‹å¦‚ï¼š
      - Batch Size è¿‡å°ï¼Œå¯¼è‡´ GPU è®¡ç®—èµ„æºæœªå……åˆ†åˆ©ç”¨ã€‚
      - æ•°æ®ä¼ è¾“ï¼ˆå¦‚ I/O è¯»å–ã€ç½‘ç»œä¼ è¾“ï¼‰æˆä¸ºç“¶é¢ˆï¼ŒGPU éœ€è¦ç­‰å¾…æ•°æ®ã€‚
      - å†…å­˜ç®¡ç†æˆ–è®¡ç®—å›¾ä¼˜åŒ–ä¸å¤Ÿé«˜æ•ˆï¼Œå¯¼è‡´ GPU è®¡ç®—è´Ÿè½½ä¸å‡è¡¡ã€‚
  - **ååé‡æµ‹è¯•çš„ç›®æ ‡** æ˜¯**åœ¨å°½å¯èƒ½ä½çš„å»¶è¿Ÿä¸‹ï¼Œæé«˜ååé‡**ï¼Œè€Œä¸ä»…ä»…æ˜¯è®© GPU åˆ©ç”¨ç‡è¾¾åˆ° 100%ã€‚
    - åœ¨ä¼˜åŒ–ååé‡æ—¶ï¼Œé€šå¸¸ä¼šè°ƒæ•´ **Batch Sizeã€æ•°æ®é¢„åŠ è½½ã€è®¡ç®—ä¼˜åŒ–ï¼ˆå¦‚ FP16 é‡åŒ–ã€å¼ é‡å¹¶è¡Œï¼‰** ç­‰ç­–ç•¥ï¼Œä»¥æé«˜å¤„ç†æ•ˆç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯è¿½æ±‚ GPU åˆ©ç”¨ç‡çš„æœ€å¤§åŒ–ã€‚



## å››ã€ç§Ÿç”¨æœåŠ¡å™¨

> å…¶ä¸­ï¼ŒOpenBayeså’Œæ™ºæ˜Ÿäº‘æ”¯æŒå¤šå¡A6000ï¼ŒAutoDLä¸æ”¯æŒï¼Œä½†æ˜¯å£ç¢‘è¾ƒå¥½

### 1ã€OpenBayes

#### 01 å¹³å°ä»‹ç»

å…³äºOpenBayeséƒ¨ç½²Difyå¦‚ä½•æ˜ å°„åˆ°æœ¬åœ°ï¼šå¯é‡‡ç”¨å…è´¹çš„é¢åº¦å…ˆæµ‹è¯•ä¸€ä¸‹

- æ–¹æ¡ˆ1ï¼šè‡ªåŠ¨ä½¿ç”¨å…¬ç½‘IP
  - https://openbayes.com/docs/tutorials/use-label-studio-ml-backend-with-uie#machine-learning-%E9%9B%86%E6%88%90
  - https://blog.csdn.net/OpenBayes/article/details/137153227
- æ–¹æ¡ˆ2ï¼šé‡‡ç”¨åŸŸå
  - https://blog.csdn.net/OpenBayes/article/details/136906762
- æ–¹æ¡ˆ3ï¼šç¼–å†™serveræœåŠ¡
  - https://openbayes.com/docs/serving/predictor
- æ–¹æ¡ˆ4ï¼šé‡‡ç”¨sshç›´æ¥æ˜ å°„



å®‰è£…difyæ—¶ï¼š

- å¹³å°libcurl` å’Œ `curlå­˜åœ¨é—®é¢˜ï¼Œéœ€è¦é‡æ–°å®‰è£…ä¸€ä¸‹

  ```
  apt-get update
  apt-get install --reinstall libcurl4 curl
  ```

- å®‰è£…docker-compose:

  ```
  # github: https://github.com/docker/compose/releases/tag/v2.20.2 
  # å›½å†…ä¸‹è½½åœ°å€ï¼šhttps://gitee.com/smilezgy/compose/releases/tag/v2.20.2
  curl -SL \
  https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-linux-x86_64 \
  -o /usr/local/bin/docker-compose
  ```

- å®‰è£…dockerï¼šå¯ä»¥å®‰è£…ï¼Œæ­£å¸¸è¾“å‡ºdockerç‰ˆæœ¬ï¼Œä½†æ˜¯æ— æ³•å¯åŠ¨dockerï¼ˆsystemctl start dockerã€service docker startã€dockerd &ï¼‰ï¼Œå®˜æ–¹äººå‘˜å›åº”OpenBayes å¹³å°å®¹å™¨æœ¬èº«æ˜¯åŸºäº Docker çš„ï¼Œå› æ­¤ä¸æ”¯æŒ Docker å¥— Docker

  ```
  apt-get update
  apt-get install --reinstall libcurl4 curl
  
  curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
  
  echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
  
  apt update
  apt-get update
  apt install docker-ce docker-ce-cli containerd.io
  ```

åå°è¯•é¿å¼€dockerï¼Œæºç å®‰è£…difyï¼Œå­˜åœ¨å…¶ä»–å¤šä¸ªé—®é¢˜éš¾è§£å†³ï¼Œç›®å‰æ‰“ç®—ç›´æ¥æµ‹LLMçš„tokenè¾“å‡ºé€Ÿåº¦ï¼Œå¹¶å°†å¤§æ¨¡å‹æœåŠ¡é€šè¿‡å¼€æ”¾APIæ¥å£æ˜ å°„åˆ°æœ¬åœ°DifyæœåŠ¡ï¼ˆæ¶‰åŠç½‘ç»œå»¶è¿Ÿï¼Œæ¯”æœ¬åœ°éƒ¨ç½²ç•¥æ…¢ï¼‰



æœåŠ¡å™¨ä¸Šéƒ¨ç½²çš„æ¨¡å‹æ˜ å°„åˆ°æœ¬åœ°Difyï¼š

- åŸºäºOllama
- ç›´æ¥åŸºäºpythonä»£ç 



#### 02ã€å¹³å°ç¯å¢ƒé—®é¢˜

##### Q1ï¼šdockerå¯å®‰è£…ï¼Œä½†æ— æ³•ä½¿ç”¨

å®˜æ–¹å›åº”ï¼Œå¹³å°æ˜¯ç»§ç»­dockerè™šæ‹ŸåŒ–çš„ï¼Œä¸èƒ½dockeré‡Œé¢åµŒå…¥docker



##### Q2ï¼špipå®‰è£…éƒ¨åˆ†åº“æŠ¥é”™

pipå®‰è£…transformersæˆ–è€…vllmä¼šå‡ºç°å¦‚ä¸‹é”™è¯¯ï¼š

```
Collecting tokenizers>=0.19.1 (from vllm)
  Using cached https://repo.huaweicloud.com/repository/pypi/packages/20/41/c2be10975ca37f6ec40d7abd7e98a5213bb04f284b869c1a24e6504fd94d/tokenizers-0.21.0.tar.gz (343 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  Ã— Preparing metadata (pyproject.toml) did not run successfully.
  â”‚ exit code: 1
  â•°â”€> [6 lines of output]
      
      Cargo, the Rust package manager, is not installed or is not on PATH.
      This package requires Rust and Cargo to compile extensions. Install it through
      the system's package manager or via https://rustup.rs/
      
      Checking for Rust toolchain....
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

Ã— Encountered error while generating package metadata.
â•°â”€> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

åŸå› ï¼š**ç¼ºå°‘Rustç¼–è¯‘ç¯å¢ƒ**ã€‚`tokenizers`åº“åº•å±‚ä¾èµ–Rustç¼–å†™çš„æ‰©å±•ï¼Œè€Œæ‚¨çš„ç³»ç»Ÿä¸­æœªå®‰è£…Rustå·¥å…·é“¾ï¼ˆCargoå’ŒRustcï¼‰ã€‚

è§£å†³æ–¹æ¡ˆæ­¥éª¤ï¼š

1. **å®‰è£…Rustå·¥å…·é“¾**

   ```
   # ä½¿ç”¨å®˜æ–¹æ¨èçš„rustupå®‰è£…ï¼ˆæ¨èï¼‰
   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
   ```

   å®‰è£…è¿‡ç¨‹ä¸­é€‰æ‹©é»˜è®¤é€‰é¡¹ï¼ˆæŒ‰å›è½¦ç¡®è®¤ï¼‰ï¼Œå®‰è£…å®Œæˆåæ‰§è¡Œï¼š

   ```
   source "$HOME/.cargo/env"
   ```

   *å¦‚æœæ— æ³•ä½¿ç”¨curlï¼Œä¹Ÿå¯ä»¥é€šè¿‡aptå®‰è£…ï¼ˆç‰ˆæœ¬å¯èƒ½è¾ƒæ—§ï¼‰ï¼š*

   ```
   sudo apt update && sudo apt install -y rustc cargo
   ```

2. **éªŒè¯Rustæ˜¯å¦å®‰è£…æˆåŠŸ**

   ```
   rustc --version  # åº”è¾“å‡ºç±»ä¼¼ "rustc 1.76.0 (07dca48 2024-02-04)"
   cargo --version  # åº”è¾“å‡ºç±»ä¼¼ "cargo 1.76.0 (1ec8f49 2024-02-01)"
   ```

3. **å®‰è£…ç¼–è¯‘ä¾èµ–ï¼ˆå¯é€‰ä½†å»ºè®®ï¼‰**

   ```
   sudo apt install -y python3-dev python3-pip build-essential
   ```

4. **é‡æ–°å®‰è£…vllm**

   ```
   pip install vllm
   ```



##### Q3ï¼šé‡å¯åèµ„æºæ¶ˆå¤±é—®é¢˜

å‚è€ƒï¼šhttps://openbayes.com/docs/runtimes/faq

é‡å¯åï¼ŒåŒæ ·çš„å·¥ä½œç©ºé—´ï¼Œæ— æ³•å¯åŠ¨æˆ–è€…é‡æ–°å®‰è£…ollamaï¼ŒæŠ¥é”™å¦‚ä¸‹ï¼š

```
curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to ollama.com:443
```

å¹¶ä¸”ollmaæ¨¡å‹å¹¶éä¸‹è½½åœ¨homeç›®å½•ä¸‹ï¼ˆéœ€è¦ä¿®æ”¹ï¼‰ï¼Œéœ€è¦é‡æ–°ä¸‹è½½ã€‚

æ‰€ä»¥æ€»çš„æ¥è¯´ï¼Œä¸ºäº†é¿å…ollamaæœåŠ¡é—®é¢˜ï¼Œéœ€è¦è§£å†³ï¼š

- é‡å¯æœåŠ¡å™¨åçš„SSLé—®é¢˜ï¼šæš‚æœªè§£å†³
- ä¿®æ”¹Ollamaä¿å­˜æ¨¡å‹ä½ç½®è‡³homeç›®å½•ä¸‹ï¼šæš‚æœªè§£å†³
- ä»£ç è°ƒç”¨homeè·¯å¾„ä¸‹æ¨¡å‹ï¼šæš‚æœªè§£å†³



##### Q4ï¼štransformerå’Œvllmæ— æ³•ä½¿ç”¨é—®é¢˜

æŠ¥é”™ï¼š

```
RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):
/usr/local/lib/python3.8/site-packages/tokenizers/tokenizers.abi3.so: undefined symbol: PyInterpreterState_Get
```

è§£å†³æ–¹æ³•ä¸ºï¼Œé‡æ–°å¼€ä¸€ä¸ªæœåŠ¡å™¨ï¼Œæ ¼å¼ç›´æ¥é€‰vllm-gpuï¼Œé‡Œé¢å·²ç»é…å¥½äº†torch-gpuã€cudaã€transformersã€vllmçš„ç¯å¢ƒ



##### Q5ï¼šOllamaä¸‹è½½é—®é¢˜

ä¸‹è½½ollamaå¯èƒ½å‡ºç°SSLé—®é¢˜

```
curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to objects.githubusercontent.com:443
```

å¦‚æœæ˜¯å¯åŠ¨æ—§çš„å®¹å™¨ï¼Œé‡æ–°ä¸‹è½½Ollamaï¼Œæš‚æ—¶æ²¡æœ‰è§£å†³æ–¹æ¡ˆ

å¦‚æœæ˜¯æ–°çš„å®¹å™¨å‡ºç°äº†è¿™ä¸ªé—®é¢˜ï¼Œå¤šè¯•å‡ ä¸‹åº”è¯¥å¯ä»¥ï¼Œå®åœ¨ä¸è¡Œå°±é—®å®¢æœ



### 2ã€æ™ºæ˜Ÿäº‘

å®˜ç½‘é“¾æ¥ï¼šhttps://gpu.ai-galaxy.cn/store



### 3ã€AutoDL

å®˜ç½‘é“¾æ¥ï¼šhttps://www.autodl.com/



# 02 æµ‹è¯•ç»“æœ

## 202502

> ä»¥ä¸‹æµ‹è¯•ï¼ŒåŸºäºOpenBayesæœåŠ¡å™¨ç§Ÿç”¨å¹³å°ï¼Œæ³¨æ„ï¼Œå¹³å°å¼€å‘ç¥¨å¯èƒ½æœ‰ä½æ¶ˆï¼Œå»ºè®®å†²ä¹‹å‰å…ˆé‚®ä»¶å’¨è¯¢å®¢æœ

### å‰è¦

#### 1ã€æ¨¡å‹é€‰æ‹©ä¸æµ‹è¯•ç¯å¢ƒ

æ¨¡å‹é€‰æ‹©ï¼š

- è¾ƒå¤§ï¼š
  - qwen2.5:72b-instruct-q4_K_M
  - deepseek-r1:70b-llama-distill-q4_K_M
  - deepseek-r1:32b-qwen-distill-fp16
- è¾ƒå°ï¼š
  - qwen2.5:32b-instruct-q4_K_M
  - deepseek-r1:32b-qwen-distill-q4_K_M





æµ‹è¯•ç»“æœï¼šå‚è€ƒexcelæ–‡ä»¶

æµ‹è¯•æ–¹æ³•æ€»ç»“ï¼š

- transformers
- vLLM
- Ollama + Dify + ä¸CloudPSSäº¤äº’

å…¶ä»–æµ‹è¯•æ–¹æ³•ï¼š

- å‘å¸ƒç®—æ³•APIï¼šä¸LLMç¦»çº¿æ¨¡å‹ç±»ä¼¼ï¼Œæµ‹è¯•éƒ¨åˆ†æ¨¡å‹å³å¯ï¼Œæ— éœ€å…¨éƒ¨å®éªŒéƒ½ä¸€ä¸€æµ‹è¯•
- Xinferenceï¼ˆå’¨è¯¢äº‘ç¦å“¥ï¼‰ï¼šç”±äºæ¶‰åŠåˆ°ç•Œé¢ï¼Œä¸”æŒ‡ä»¤å¯¼å…¥å¤±è´¥ï¼Œæš‚æ—¶ä¸ä½¿ç”¨



ç¡®è®¤cudaå’Œcudnnæ˜¯å¦å®‰è£…ï¼š

```
nvcc -V

cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
```



#### 2ã€åŠ é€Ÿæ¨¡å‹ä¸‹è½½

åŸºäºmodelscopeä¸‹è½½æ¨¡å‹ï¼š

```
echo 'export VLLM_USE_MODELSCOPE=True' >> ~/.bashrc
source ~/.bashrc
```



åŸºäºhuggingfaceçš„é•œåƒç½‘ç«™`hf-mirror.com`ä¸‹è½½æ•´ä¸ªä»“åº“çš„æ¨¡å‹ï¼š

```
pip install -U huggingface_hub
echo 'export HF_ENDPOINT=https://hf-mirror.com' >> ~/.bashrc
source ~/.bashrc
```

ç¤ºä¾‹ä»£ç ï¼š

```
from huggingface_hub import snapshot_download
snapshot_download(repo_id="lysandre/arxiv-nlp")
```

æˆ–è€…ä½¿ç”¨`huggingface-cli`ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼šï¼ˆæ„Ÿè§‰æ¯”huggingfaceå’Œmodelscopeéƒ½ç•¥å¿«ï¼Œä½†æ˜¯ä¹Ÿä¼šå‡ºç°ç½‘ç»œé—®é¢˜ä¸­æ–­çš„æƒ…å†µï¼Œéœ€è¦é‡å¤æ‰§è¡Œå¤šæ¬¡æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå¯ä»¥åŒæ—¶ä¸‹è½½å¤šä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªæƒé‡ä¸‹è½½çš„é€Ÿåº¦æœ‰ä¸Šé™ï¼ŒåŒæ—¶ä»…ä¸‹è½½ä¸€ä¸ªæ¨¡å‹ä¹Ÿä¸ä¼šæé€Ÿï¼‰

```
huggingface-cli download --resume-download Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --local-dir Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4
```

Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4

Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4

Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4



é—®é¢˜ï¼š

- åŸºäºmodelscopeä¸‹è½½çš„æ¨¡å‹ï¼Œåœ¨ç§Ÿç”¨å¹³å°
  - å¦‚æœä¸­æ–­ä¸‹è½½ï¼Œå†æ¬¡ä¸‹è½½ä¼šå‡ºç°ä»£ç å¡ä½ä¸ä¸‹è½½çš„æƒ…å†µ
  - ä¸‹è½½å®Œæ¯•ï¼Œåˆ†æå®Œæ¯•ï¼Œé‡æ–°æ‰§è¡Œä»£ç ï¼Œä»£ç ä¼šé‡æ–°ä¸‹è½½æ¨¡å‹ï¼Ÿï¼



#### 3ã€æ¨¡å‹é‡åŒ–é—®é¢˜

é—®é¢˜ï¼š

- Q1ï¼šé‡åŒ–æ‰€éœ€æ˜¾å­˜é—®é¢˜
  - å¦‚æœhuggingfaceä¸Šå¼€æºäº†é‡åŒ–æ¨¡å‹å°±ç”¨ï¼Œå¦‚æœæ²¡æœ‰å¼€æºä¸”æ˜¾å­˜ä¸å¤Ÿï¼Œé‚£å°±ä¸æµ‹äº†ï¼Œç”¨ollamaæµ‹è¯•ä¸€ä¸‹ï¼Œé¢„ä¼°ä¸€ä¸‹å³å¯
- Q2ï¼šåŸºäºollamaä¸‹è½½çš„æ¨¡å‹ï¼Œæ˜¯å¦å¯ä»¥ç”¨äºtransformers/vllmæ¨ç†æ¡†æ¶
  - é¢„è®¡ä¸è¡Œï¼Œollamaå¯ä»¥å¯¼å‡ºæˆ–è€…å¯¼å…¥ggufæ ¼å¼çš„æ¨¡å‹ï¼Œä½†æ˜¯transformers/vllmæ¨ç†æ¡†æ¶æœªæåŠå¯æ”¯æŒggufï¼Œä¸€èˆ¬é‡‡ç”¨GPTQæ ¼å¼
  - æœ‰åšå®¢è¡¨ç¤ºvLLMå¯¹ GGUF æ–‡ä»¶çš„æ”¯æŒä»å¤„äºé«˜åº¦å®éªŒé˜¶æ®µï¼ˆhttp://www.hubwiz.com/blog/ollama-vs-llama-cpp-performance-comparison/ï¼‰
  - GGUF é‡åŒ–ä»å¤„äºå®éªŒé˜¶æ®µ



å¦‚æœæ¶‰åŠåˆ°é‡åŒ–æ¨¡å‹ï¼š

**æ–¹æ³•ä¸€ï¼šåŸºäºauto-gptq**

```
pip install auto-gptq
```

ç¤ºä¾‹ä»£ç ï¼š

```

```



**æ–¹æ³•äºŒï¼šåŸºäºswiftæ¡†æ¶**

é­”æ­ç¤¾åŒºæ¨å‡ºçš„æ¡†æ¶ï¼šhttps://github.com/modelscope/ms-swift

é€šè¿‡ä½¿ç”¨web-uié‡åŒ–æ¨¡å‹ï¼š

![image-20250213103149699](./assets/image-20250213103149699.png)

æˆ–è€…ç›´æ¥ç»ˆç«¯æŒ‡ä»¤ï¼Œå¦‚ï¼š

```
CUDA_VISIBLE_DEVICES=0 swift export --adapters 'output/some-model/vx-xxx/checkpoint-xxx' --quant_bits 4 --load_data_args true --quant_method gptq
```

```
# OMP_NUM_THREADS=14 please Check issue: https://github.com/AutoGPTQ/AutoGPTQ/issues/439
OMP_NUM_THREADS=14 \
CUDA_VISIBLE_DEVICES=0 \
swift export \
    --model Qwen/Qwen2.5-1.5B-Instruct \
    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \
              'AI-ModelScope/alpaca-gpt4-data-en#500' \
    --quant_n_samples 256 \
    --quant_batch_size 1 \
    --max_length 2048 \
    --quant_method gptq \
    --quant_bits 4 \
    --output_dir Qwen2.5-1.5B-Instruct-GPTQ-Int4
```

```
swift export --model E:\work_zhaoyulong\Project\LLMProject\TokensTest\deepseek-ai\DeepSeek-R1-Distill-Llama-70B --quant_method gptq --quant_bits 4 --output_dir E:\work_zhaoyulong\Project\LLMProject\TokensTest\deepseek-ai\DeepSeek-R1-Distill-Llama-70B-GPTQ-Int4
```



å¸¸è§é—®é¢˜ï¼šä¸èƒ½çœ‹ç»ˆç«¯çš„æ—¥å¿—ï¼Œè¦çœ‹web-uiçš„æ—¥å¿—æˆ–è€…ç›´æ¥çœ‹logæ–‡ä»¶

1. è·¯å¾„å·²å­˜åœ¨é—®é¢˜ï¼šç›´æ¥ç”¨ç©ºè·¯å¾„ï¼Œç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„éƒ½ä¼šæŠ¥é”™
2. æ ¡å‡†æ•°æ®é›†ä¸ºç©ºé—®é¢˜ï¼šæ ¡å‡†å¯ä»¥æŒ‰ç…§â€œalpaca-gpt4-data-zhâ€è¿™å‡ ä¸ªæ•°æ®é›†åšï¼Œä¹Ÿå¯ä»¥æŒ‰ç…§Qwençš„å®˜æ–¹æ•™ç¨‹åšï¼ˆhttps://qwen.readthedocs.io/zh-cn/latest/quantization/gptq.htmlï¼‰
3. `ModuleNotFoundError: No module named 'awq' `ï¼špip install autoawq
4. `module 'torch.library' has no attribute 'register_fake'`ï¼šè²Œä¼¼æ˜¯torchç‰ˆæœ¬å’Œtorchvisionç‰ˆæœ¬åŒ¹é…ï¼ˆå®‰è£…autoawqæ—¶è‡ªåŠ¨é‡è£…äº†torchï¼‰ï¼Œè¿™é‡Œå¸è½½äº†å†æ¬¡é‡è£…ä¸€ä¸‹å³å¯ï¼ˆautoawqçš„ç‰ˆæœ¬å†²çªåˆ«ç®¡ä»–ï¼‰
5. `ImportError: cannot import name 'shard_checkpoint' from 'transformers.modeling_utils' `ï¼šåº”è¯¥æ˜¯ 4.47 ä»¥åçš„ Transformers æŠŠ shard_checkpoint å»æ‰äº†å¯¼è‡´é—®é¢˜ã€‚è¿™é‡Œå°è¯•å°† Transformersä»4.48.3é™ä¸º4.46.3å³å¯
6. æ˜¾å­˜çˆ†ç‚¸é—®é¢˜ï¼šæ³¨æ„å¯¼å‡ºæ—¶è¦é€‰æ‹©æ‰€æœ‰çš„GPUï¼Œä»…é é»˜è®¤çš„ä¸€å¼ å¡ä¸å¤Ÿç”¨ï¼Œç›®å‰é‡åŒ–32Bçš„deepseek-r1ä¸ºint4ï¼Œä¸‰å¼ 4090æ˜¾å­˜æš‚æ—¶ä¸å¤Ÿ

![image-20250213165936554](./assets/image-20250213165936554.png)



#### 4ã€æ¨ç†æ—¶æœªå……åˆ†åˆ©ç”¨å¤šå¡è®¡ç®—èƒ½åŠ›

å¤šå¡ä¸‹ï¼Œè™½ç„¶GPUæ˜¾å­˜å æ»¡ï¼Œä½†æ¨ç†æ—¶GPU-utilå¾ˆå°ï¼Œå³GPUåˆ©ç”¨ç‡å¹¶ä¸é«˜

![image-20250213100424814](./assets/image-20250213100424814.png)





### ä¸€ã€transformers

#### 1ã€tokenè¾“å‡ºé€Ÿåº¦ + cpuå ç”¨ + å†…å­˜å ç”¨

##### Q1ï¼šç¯å¢ƒé…ç½®

åŸºç¡€ç¯å¢ƒï¼š

```
transformers>=4.32.0,<4.38.0
accelerate
tiktoken
einops
transformers_stream_generator==0.0.4
scipy
```

åç»­å‘ç°ä½ç‰ˆæœ¬çš„transformerså¯èƒ½ä¼šå‡ºç°:

```
`rope_scaling` must be a dictionary with with two fields, `type` and `factor`, got {'factor': 8.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
```

å³`rope_scaling`å‚æ•°é”™è¯¯ï¼Œè¿˜æ˜¯éœ€è¦æ›´æ–°ä¸€ä¸‹ï¼ˆtransformerså‚è€ƒ4.46.3ï¼‰

```
pip install --upgrade transformers
```

æ·±åº¦å­¦ä¹ ç¯å¢ƒé…ç½®ï¼šç›´æ¥å®‰è£…torchä¼šå¯¼è‡´æ¨¡å‹æ¨ç†æ—¶åªç”¨cpuï¼Œéœ€è¦å®‰è£…ä¸ç³»ç»ŸCUDAåŒ¹é…çš„pytorchï¼ˆç‰ˆæœ¬ä½ä¸€ç‚¹ä¹Ÿæ²¡å…³ç³»ï¼Œå¯èƒ½å­˜åœ¨cudaç‰ˆæœ¬ä¸å­˜åœ¨çš„æƒ…å†µï¼Œå¦‚cu122ï¼Œä¸è¿‡æ³¨æ„ï¼Œè¿™é‡Œå°è¯•äº†ä¸€æ¬¡è¿˜ä¼šæŠ¥é”™ï¼Œå¤šé‡è£…å‡ æ¬¡æ‰æˆåŠŸçš„ï¼‰ï¼Œå³ä¾¿è™šæ‹Ÿç¯å¢ƒä¸­æ²¡æœ‰å®‰è£…cudaå’Œcudnnï¼Œä¹Ÿä¼šè‡ªåŠ¨è°ƒç”¨ç³»ç»Ÿçš„cuda

```
# å¸è½½å½“å‰PyTorch
pip uninstall torch torchvision torchaudio

# å®‰è£…ä¸ç³»ç»ŸCUDAç‰ˆæœ¬åŒ¹é…çš„PyTorchï¼ˆä»¥CUDA 11.8ä¸ºä¾‹ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```



##### Q2ï¼šé‡åŒ–æ¨¡å‹åŠ è½½é—®é¢˜

ç¯å¢ƒé…ç½®ï¼š

```
pip install bitsandbytes
```

å¹¶é‡‡ç”¨transformersä¸‹BitsAndBytesConfigæ–¹æ³•åŠ è½½ï¼Œä½†æ˜¯åç»­å‘ç°ï¼Œä»ç„¶å‡ºç°å¦‚` 'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'`ç­‰é—®é¢˜ï¼Œä½†æ˜¯BitsAndBytesConfigä¸‹æ˜¯æœ‰get_loading_attributesæ–¹æ³•çš„ã€‚

æœç´¢å‘ç°å¤§æ¦‚ç‡æ˜¯`transformers`åº“å’Œ`bitsandbytes`åº“çš„ç‰ˆæœ¬é—®é¢˜ï¼Œæ¢äº†å‡ ç‰ˆä¹Ÿä¸è¡Œï¼Œæœ€åè§£å†³æ–¹æ³•ä¸ºç›´æ¥ç”¨GPTQåŠ è½½

```
if "GPTQ" in model_name:
    from transformers import GPTQConfig
    quant_config = GPTQConfig(bits=4, dataset="c4")
```

åç»­æŸ¥é˜…å®˜ç½‘å‘ç°ï¼Œæœ€å¥½çš„æŒ‡å®šç²¾åº¦çš„æ–¹å¼å°±æ˜¯ä¸æŒ‡å®šï¼ç›´æ¥æ³¨é‡Šæ‰quantization_configï¼



##### Q3ï¼šç¤ºä¾‹æµ‹è¯•è„šæœ¬

ä½¿ç”¨å‰ï¼Œæ ¹æ®æŠ¥é”™æç¤ºï¼Œå®‰è£…å„ä¸ªåº“

```
pip install bitsandbytes
pip install --upgrade accelerate
pip install optimum
pip install auto-gptq
```

```
import os
import time
import threading
import torch
import psutil
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# å°è¯•å¯¼å…¥ModelScopeåº“ï¼ˆéœ€æå‰å®‰è£… modelscopeï¼‰
try:
    from modelscope import snapshot_download
except ImportError:
    snapshot_download = None


class ResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self, interval=1):
        self.interval = interval  # é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        self.cpu_percentages = []  # CPUä½¿ç”¨ç‡è®°å½•
        self.memory_usages = []  # å†…å­˜å ç”¨è®°å½•ï¼ˆMBï¼‰
        self.is_monitoring = False
        self.process = psutil.Process(os.getpid())  # ç›‘æ§å½“å‰è¿›ç¨‹

    def _monitor(self):
        """ç›‘æ§çº¿ç¨‹ä¸»å‡½æ•°"""
        while self.is_monitoring:
            # è·å–CPUä½¿ç”¨ç‡ï¼ˆç³»ç»Ÿçº§ï¼‰
            cpu_percent = psutil.cpu_percent(interval=None)
            # è·å–è¿›ç¨‹å†…å­˜å ç”¨ï¼ˆRSSï¼‰
            mem_usage = self.process.memory_info().rss / 1024 / 1024  # è½¬æ¢ä¸ºMB

            self.cpu_percentages.append(cpu_percent)
            self.memory_usages.append(mem_usage)
            time.sleep(self.interval)

    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        self.is_monitoring = True
        self.thread = threading.Thread(target=self._monitor)
        self.thread.start()

    def stop(self):
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        self.is_monitoring = False
        if self.thread.is_alive():
            self.thread.join()

        stats = {
            "cpu_avg": np.mean(self.cpu_percentages) if self.cpu_percentages else 0,
            "cpu_peak": np.max(self.cpu_percentages) if self.cpu_percentages else 0,
            "mem_avg": np.mean(self.memory_usages) if self.memory_usages else 0,
            "mem_peak": np.max(self.memory_usages) if self.memory_usages else 0
        }
        return stats


def benchmark_model(
        model_name="Qwen/Qwen2.5-0.5B",
        download_source="modelscope",  # ä¸‹è½½æºé€‰æ‹©ï¼šmodelscope/huggingface/hf_mirror
        max_new_tokens=512,
        num_runs=3,
        device_type="auto",
        model_cache_dir=None,
):
    """
    å¢å¼ºç‰ˆæ€§èƒ½æµ‹è¯•å‡½æ•°
    æ–°å¢åŠŸèƒ½ï¼š
    1. å¤šä¸‹è½½æºæ”¯æŒ
    2. CPU/å†…å­˜èµ„æºç›‘æ§
    """

    # é…ç½®ä¸‹è½½æº-----------------------------------------------------------
    if download_source == "hf_mirror":
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    elif download_source == "huggingface":
        os.environ.pop('HF_ENDPOINT', None)  # æ¢å¤é»˜è®¤

    # æ¨¡å‹ä¸‹è½½é€»è¾‘---------------------------------------------------------
    if download_source == "modelscope" and snapshot_download:
        if not os.path.exists(os.path.join(model_cache_dir or "", model_name)):
            print(f"\n=== ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ ===")
            model_name = snapshot_download(
                model_name,
                revision="master",
                cache_dir=model_cache_dir
            )

    # è®¾å¤‡æ£€æµ‹-------------------------------------------------------------
    device = device_type.lower()
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\n=== è¿è¡Œç¯å¢ƒé…ç½® ===")
    print(f"ä¸‹è½½æº: {download_source.upper()}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device.upper()}")

    # æ¨¡å‹åŠ è½½-------------------------------------------------------------
    print("\n=== æ­£åœ¨åŠ è½½æ¨¡å‹ ===")
    load_start = time.time()

    try:
        # è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
        torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

        # é‡åŒ–é…ç½®æ£€æµ‹
        quant_config = None
        # if "Int4" in model_name or "4bit" in model_name.lower():
        #     quant_config = BitsAndBytesConfig(load_in_4bit=True)
        # elif "Int8" in model_name or "8bit" in model_name.lower():
        #     quant_config = BitsAndBytesConfig(load_in_8bit=True)
        if "Int4" in model_name or "4bit" in model_name.lower():
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",  # å¿…éœ€å‚æ•°
                bnb_4bit_compute_dtype=torch_dtype,
                bnb_4bit_use_double_quant=True  # å¯é€‰åŒé‡åŒ–
            )
        elif "Int8" in model_name or "8bit" in model_name.lower():
            quant_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0  # è°ƒèŠ‚é‡åŒ–é˜ˆå€¼
            )

        # å¦‚æœä»é‡åˆ°é—®é¢˜ï¼Œå¯å°è¯•ç›´æ¥åŠ è½½GPTQé‡åŒ–æ¨¡å‹
        if "GPTQ" in model_name:
            from transformers import GPTQConfig
            quant_config = GPTQConfig(bits=4, dataset="c4")

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch_dtype,
            # quantization_config=quant_config,  # æ›¿æ¢æ—§çš„load_in_8bitå‚æ•°ï¼Œåç»­å‘ç°ï¼Œæ­¤å‚æ•°ä¼šè‡ªåŠ¨é…ç½®ï¼Œæ³¨é‡Šæ‰æ›´çœäº‹
            device_map="auto",  # å¾ˆå¥‡æ€ªï¼Œé‡‡ç”¨deviceè¿™ä¸ªå˜é‡ï¼Œå°±æ— æ³•ä½¿ç”¨å¤šå¡ï¼Œä½†æœ¬è´¨éƒ½æ˜¯â€œautoâ€
            trust_remote_code=True
        )
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
    except ImportError as e:
        if "bitsandbytes" in str(e):
            print("é‡åŒ–éœ€è¦å®‰è£…bitsandbytes: pip install bitsandbytes")
        elif "accelerate" in str(e):
            print("éœ€è¦æ›´æ–°accelerate: pip install --upgrade accelerate")
        else:
            print(f"ç¼ºå°‘ä¾èµ–åº“: {str(e)}")
        return
    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            print("æ˜¾å­˜ä¸è¶³ï¼å°è¯•ï¼š1.å‡å°max_new_tokens 2.ä½¿ç”¨æ›´ä½ç²¾åº¦")
        else:
            print(f"è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
        return

    print(f"æ¨¡å‹åŠ è½½è€—æ—¶: {time.time() - load_start:.2f}s")

    # è¾“å…¥å‡†å¤‡-------------------------------------------------------------
    messages = [
        {"role": "user", "content": "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»å¤§è¯­è¨€æ¨¡å‹"}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(device)

    # é¢„çƒ­é˜¶æ®µ-------------------------------------------------------------
    print("\n=== é¢„çƒ­é˜¶æ®µ ===")
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=1)
    if device == "cuda":
        torch.cuda.synchronize()

    # æ€§èƒ½æµ‹è¯•-------------------------------------------------------------
    print(f"\n=== å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼ˆ{num_runs}æ¬¡å¹³å‡ï¼‰===")
    total_time = 0
    total_tokens = 0

    # å¯åŠ¨èµ„æºç›‘æ§
    monitor = ResourceMonitor(interval=0.5)
    monitor.start()

    for run_idx in range(num_runs):
        if device == "cuda":
            torch.cuda.synchronize()

        start_time = time.time()

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False
            )

        if device == "cuda":
            torch.cuda.synchronize()

        elapsed = time.time() - start_time
        new_tokens = outputs[0].shape[-1] - inputs.input_ids.shape[-1]

        total_time += elapsed
        total_tokens += new_tokens

        print(
            f"[{run_idx + 1}/{num_runs}] Tokens: {new_tokens} | Time: {elapsed:.2f}s | Speed: {new_tokens / elapsed:.2f}tok/s")

    # åœæ­¢ç›‘æ§å¹¶è·å–æ•°æ®
    resource_stats = monitor.stop()

    # ç»“æœç»Ÿè®¡-------------------------------------------------------------
    avg_tps = total_tokens / total_time
    avg_latency = total_time / num_runs

    print("\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    print(f"å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_tps:.2f} tokens/ç§’")
    print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ç§’/è¯·æ±‚")
    print(f"CPUå ç”¨ç‡ï¼ˆå‡å€¼/å³°å€¼ï¼‰: {resource_stats['cpu_avg']:.1f}% / {resource_stats['cpu_peak']:.1f}%")
    print(f"å†…å­˜å ç”¨ï¼ˆå‡å€¼/å³°å€¼ï¼‰: {resource_stats['mem_avg']:.1f}MB / {resource_stats['mem_peak']:.1f}MB")

    # if device == "cuda":
    #     print(f"æ˜¾å­˜å³°å€¼å ç”¨: {torch.cuda.max_memory_allocated() / 1024 ** 3:.2f}GB")

    # è·å–æ‰€æœ‰GPUçš„æ˜¾å­˜å ç”¨
    if device == "cuda" and torch.cuda.is_available():
        max_memory_per_gpu = []
        for i in range(torch.cuda.device_count()):
            max_memory_per_gpu.append(torch.cuda.max_memory_allocated(i) / 1024 ** 3)  # æ˜¾å­˜ä»¥GBä¸ºå•ä½
        # max_memory_all_gpus = max(max_memory_per_gpu) if max_memory_per_gpu else 0
        # print(f"æ‰€æœ‰GPUçš„æ˜¾å­˜å³°å€¼å ç”¨: {max_memory_all_gpus:.2f} GB")
        max_memory_all_gpus = 0
        for i, mem in enumerate(max_memory_per_gpu):
            print(f"GPU-{i} æ˜¾å­˜å³°å€¼å ç”¨: {mem:.2f} GB")
            max_memory_all_gpus += mem
        print(f"æ‰€æœ‰GPUçš„æ˜¾å­˜å³°å€¼å ç”¨ä¹‹å’Œ: {max_memory_all_gpus:.2f} GB")


if __name__ == "__main__":
    # llm_model_name = "Qwen/Qwen2.5-0.5B",
    # llm_model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
    # llm_model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
    # llm_model_name = "Qwen/Qwen2.5-72B-Instruct"
    # llm_model_name = "Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8"
    # llm_model_name = "Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4"
    llm_model_name = "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4"

    benchmark_model(
        model_name=llm_model_name,
        download_source="modelscope",  # æµ‹è¯•é•œåƒç«™ä¸‹è½½ï¼š"hf_mirror"ã€"modelscope"
        model_cache_dir=r"E:\work_zhaoyulong\Project\LLMProject\TokensTest",
        max_new_tokens=128,
        num_runs=3
    )

```

ç¤ºä¾‹è¾“å‡ºï¼š

```
=== å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼ˆ3æ¬¡å¹³å‡ï¼‰===
[1/3] Tokens: 128 | Time: 105.26s | Speed: 1.22tok/s
[2/3] Tokens: 128 | Time: 105.47s | Speed: 1.21tok/s
[3/3] Tokens: 128 | Time: 105.84s | Speed: 1.21tok/s

=== æµ‹è¯•ç»“æœæ±‡æ€» ===
å¹³å‡ç”Ÿæˆé€Ÿåº¦: 1.21 tokens/ç§’
å¹³å‡å»¶è¿Ÿ: 105.52 ç§’/è¯·æ±‚
CPUå ç”¨ç‡ï¼ˆå‡å€¼/å³°å€¼ï¼‰: 33.6% / 98.3%
å†…å­˜å ç”¨ï¼ˆå‡å€¼/å³°å€¼ï¼‰: 1416.3MB / 1436.5MB
```





#### 2ã€ååé‡

æŒ‡æ ‡ï¼š

- è¯·æ±‚ååé‡: Requests/s

- Tokenååé‡: Tokens/s

ç¤ºä¾‹ä»£ç 1ï¼šå•å¡

```
import time
import threading
import logging
from concurrent.futures import ThreadPoolExecutor
from transformers import AutoModelForCausalLM, AutoTokenizer

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(threadName)s - %(message)s",
)
logger = logging.getLogger(__name__)

class ThroughputTester:
    def __init__(self):
        self._init_components()
        self._setup_test_config()
        
    def _init_components(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        self.model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4"
        ).cuda()
        self.tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4"
        )
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆ›å»ºçº¿ç¨‹å®‰å…¨é”
        self.model_lock = threading.Lock()

    def _setup_test_config(self):
        """é…ç½®æµ‹è¯•å‚æ•°"""
        self.input_text = "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»å¤§è¯­è¨€æ¨¡å‹ã€‚"
        self.num_requests = 40
        self.concurrency = 8
        self.max_new_tokens = 100
        
        # ç»Ÿè®¡å˜é‡
        self.completed_requests = 0
        self.failed_requests = 0
        self.total_tokens = 0
        self.start_time = None

    def _process_single_request(self, request_id):
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        try:
            start_time = time.time()
            logger.debug(f"è¯·æ±‚ {request_id} å¼€å§‹å¤„ç†")
            
            # ä½¿ç”¨çº¿ç¨‹é”ä¿è¯æ¨¡å‹è°ƒç”¨å®‰å…¨
            with self.model_lock:
                inputs = self.tokenizer(
                    self.input_text, 
                    return_tensors="pt"
                ).to("cuda")
                
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.max_new_tokens,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è®¡ç®—ç”Ÿæˆtokenæ•°
            generated_tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
            elapsed = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            with threading.Lock():
                self.completed_requests += 1
                self.total_tokens += generated_tokens
                logger.info(
                    f"è¯·æ±‚ {request_id} å®Œæˆ | "
                    f"Tokens: {generated_tokens} | "
                    f"è€—æ—¶: {elapsed:.2f}s | "
                    f"è¿›åº¦: {self.completed_requests}/{self.num_requests}"
                )
                
        except Exception as e:
            with threading.Lock():
                self.failed_requests += 1
            logger.error(f"è¯·æ±‚ {request_id} å¤±è´¥: {str(e)}", exc_info=True)

    def run_test(self):
        """æ‰§è¡Œå‹åŠ›æµ‹è¯•"""
        logger.info("="*50)
        logger.info(f"å¼€å§‹æ€§èƒ½æµ‹è¯• | æ€»è¯·æ±‚æ•°: {self.num_requests} | å¹¶å‘æ•°: {self.concurrency}")
        self.start_time = time.time()
        
        with ThreadPoolExecutor(
            max_workers=self.concurrency,
            thread_name_prefix="InferenceThread"
        ) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [
                executor.submit(self._process_single_request, i)
                for i in range(self.num_requests)
            ]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in futures:
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}")

        # è®¡ç®—ç»“æœ
        total_time = time.time() - self.start_time
        success_rate = self.completed_requests / self.num_requests * 100
        
        logger.info("="*50)
        logger.info("æµ‹è¯•ç»“æœ:")
        logger.info(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"æˆåŠŸè¯·æ±‚: {self.completed_requests}")
        logger.info(f"å¤±è´¥è¯·æ±‚: {self.failed_requests}")
        logger.info(f"æˆåŠŸç‡: {success_rate:.2f}%")
        logger.info(f"è¯·æ±‚ååé‡: {self.completed_requests / total_time:.2f} Requests/s")
        logger.info(f"Tokenååé‡: {self.total_tokens / total_time:.2f} Tokens/s")
        logger.info("="*50)

if __name__ == "__main__":
    tester = ThroughputTester()
    tester.run_test()
```



ç¤ºä¾‹ä»£ç 2ï¼š

```
import time
import threading
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4",
    device_map="auto",
    torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4")
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è®¾å¤‡åˆ†å¸ƒï¼š{model.hf_device_map}")

# å®šä¹‰æµ‹è¯•å‚æ•°
input_text = "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»å¤§è¯­è¨€æ¨¡å‹ã€‚"
num_requests = 40
concurrency = 8
max_new_tokens = 100

# å…¨å±€çŠ¶æ€å˜é‡
completed_requests = 0
start_time = None
print_lock = threading.Lock()
model_lock = threading.Lock()

def get_gpu_status():
    """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
    status = []
    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1024**2
        reserved = torch.cuda.memory_reserved(i) / 1024**2
        status.append(f"GPU{i}: {allocated:.1f}MB (é¢„ç•™:{reserved:.1f}MB)")
    return " | ".join(status)

def calculate_throughput():
    """è®¡ç®—å®æ—¶ååé‡"""
    elapsed = time.time() - start_time
    if elapsed < 1e-6:  # é˜²æ­¢é™¤é›¶é”™è¯¯
        return 0.0, 0.0
    req_throughput = completed_requests / elapsed
    token_throughput = (completed_requests * max_new_tokens) / elapsed
    return req_throughput, token_throughput

# è¯·æ±‚å¤„ç†å‡½æ•°
def process_request():
    global completed_requests
    thread_id = threading.get_ident()
    
    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        request_start = time.time()
        
        # å¤„ç†è¯·æ±‚
        with model_lock:
            inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        with print_lock:
            completed_requests += 1
            current_time = time.time()
            
            # è®¡ç®—æŒ‡æ ‡
            req_tps, token_tps = calculate_throughput()
            latency = current_time - request_start
            elapsed_total = current_time - start_time
            gpu_status = get_gpu_status()
            
            # æ ¼å¼åŒ–è¾“å‡º
            print(f"\nğŸ“Š è¯·æ±‚å®Œæˆ [{completed_requests}/{num_requests}]")
            print(f"â”œâ”€ çº¿ç¨‹ID: {thread_id}")
            print(f"â”œâ”€ å•æ¬¡å»¶è¿Ÿ: {latency:.2f}s")
            print(f"â”œâ”€ ç´¯è®¡è€—æ—¶: {elapsed_total:.2f}s")
            print(f"â”œâ”€ è¯·æ±‚ååé‡: {req_tps:.2f} Requests/s")
            print(f"â”œâ”€ Tokenååé‡: {token_tps:.2f} Tokens/s")
            print(f"â””â”€ GPUçŠ¶æ€: {gpu_status}")
            
    except Exception as e:
        with print_lock:
            print(f"âŒ è¯·æ±‚å¤„ç†å¤±è´¥ | çº¿ç¨‹{thread_id} | é”™è¯¯: {str(e)}")

# å¯åŠ¨æµ‹è¯•
print("\nğŸ”¥ å¼€å§‹å‹åŠ›æµ‹è¯•...")
start_time = time.time()
print(f"ğŸ“Œ æµ‹è¯•å‚æ•°ï¼š{num_requests}è¯·æ±‚ | {concurrency}å¹¶å‘ | {max_new_tokens} tokens/è¯·æ±‚")
print(f"ğŸ–¥ï¸ ç¡¬ä»¶ä¿¡æ¯ï¼š{torch.cuda.device_count()}GPU | {torch.cuda.get_device_name(0)}")

# åˆ›å»ºçº¿ç¨‹æ± 
threads = []
for i in range(num_requests):
    # æ§åˆ¶å¹¶å‘åº¦
    while True:
        alive = sum(t.is_alive() for t in threads)
        if alive < concurrency:
            break
        time.sleep(0.1)
    
    t = threading.Thread(target=process_request)
    t.start()
    threads.append(t)
    
    # æ‰“å°å¯åŠ¨çŠ¶æ€
    if (i+1) % 10 == 0:
        print(f"ğŸš€ å·²å¯åŠ¨{i+1}/{num_requests}è¯·æ±‚ | å½“å‰å¹¶å‘: {alive+1}")

# ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
print("\nâ³ ç­‰å¾…è¯·æ±‚å®Œæˆ...")
[t.join() for t in threads]

# æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š
total_time = time.time() - start_time
print(f"\nğŸ¯ æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š:")
print(f"- æ€»è€—æ—¶: {total_time:.2f}s")
print(f"- å¹³å‡è¯·æ±‚ååé‡: {num_requests/total_time:.2f} Requests/s")
print(f"- å¹³å‡Tokenååé‡: {(num_requests*max_new_tokens)/total_time:.2f} Tokens/s")
print(f"- å³°å€¼æ˜¾å­˜ä½¿ç”¨:")
for i in range(torch.cuda.device_count()):
    print(f"  GPU{i}: {torch.cuda.max_memory_allocated(i)/1024**2:.1f}MB")
```





ç¤ºä¾‹ä»£ç 3ï¼šå¤šå¡ï¼Œå­˜åœ¨é—®é¢˜ï¼Œä»£ç æ˜¯å°†è¯·æ±‚åˆ†åˆ«æ”¾åœ¨ä¸åŒæ˜¾å¡ä¸Šå»è¿è¡Œï¼Œæ ¹æœ¬é€»è¾‘ä¸Šå°±å­˜åœ¨é—®é¢˜ï¼Œè€Œä¸”ä¼šé‡åˆ°æ˜¾å­˜çˆ†ç‚¸

```
CUDA_VISIBLE_DEVICES=0,1 python benchmark.py --log=DEBUG
```

```
ç•¥
```





#### 3ã€GPUç›¸å…³

æµ‹è¯•ï¼šGPUåˆ©ç”¨ç‡ (%)ã€æ˜¾å­˜å ç”¨ (MiB)ã€åŠŸè€— (W)ã€æ¸©åº¦ (Â°C)ï¼Œæµ‹è¯•ç»“æŸæ—¶`ctrl + c`ç»ˆæ­¢æŒ‡ä»¤å³å¯ï¼Œæ³¨æ„ï¼Œæ¯æ¬¡ä½¿ç”¨å‰ï¼Œåˆ é™¤æ—§çš„logæ–‡ä»¶

```
nvidia-smi --query-gpu=index,timestamp,utilization.gpu,memory.used,power.draw,temperature.gpu --format=csv,noheader,nounits -l 1 -f gpu_stats.log
```

å¤‡æ³¨ï¼šåŠ¨æ€æŸ¥çœ‹nvidia-smiçš„è¾“å‡ºç»“æœ

- windowsä¸‹ï¼š`nvidia-smi -l x`
- linuxä¸‹ï¼š`watch -n 1 nvidia-smi`



å¤„ç†æ—¥å¿—æ–‡ä»¶ï¼š

```python
import pandas as pd

# è¯»å–æ—¥å¿—æ–‡ä»¶
df = pd.read_csv("gpu_stats.log", header=None,
                 names=["index", "timestamp", "utilization.gpu [%]", "memory.used [MiB]", "power.draw [W]",
                        "temperature.gpu [Â°C]"])

# ç¡®ä¿æ‰€æœ‰åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹å†è¿›è¡Œæå–
df['utilization.gpu [%]'] = df['utilization.gpu [%]'].astype(str).str.extract('(\d+)').astype(float)
df['memory.used [MiB]'] = df['memory.used [MiB]'].astype(str).str.extract('(\d+)').astype(float)
df['power.draw [W]'] = df['power.draw [W]'].astype(str).str.extract('(\d+\.?\d*)').astype(float)
df['temperature.gpu [Â°C]'] = df['temperature.gpu [Â°C]'].astype(str).str.extract('(\d+)').astype(float)

# è®¡ç®—æ¯ä¸ª GPU çš„å³°å€¼
gpu_stats = {}

# è·å–æ‰€æœ‰ GPU ç´¢å¼•
gpu_indexes = df['index'].unique()

for gpu in gpu_indexes:
    gpu_data = df[df['index'] == gpu]

    gpu_stats[gpu] = {
        "GPUåˆ©ç”¨ç‡ (%)": gpu_data['utilization.gpu [%]'].max(),
        "æ˜¾å­˜å ç”¨å³°å€¼ (MiB)": gpu_data['memory.used [MiB]'].max(),
        "åŠŸè€—å³°å€¼ (W)": gpu_data['power.draw [W]'].max(),
        "æ¸©åº¦å³°å€¼ (Â°C)": gpu_data['temperature.gpu [Â°C]'].max()
    }

# è®¡ç®—æ‰€æœ‰ GPU æ˜¾å­˜çš„å³°å€¼æ€»å’Œ
total_memory_peak = df.groupby('timestamp')['memory.used [MiB]'].max().sum()

# è¾“å‡ºæ¯ä¸ª GPU çš„å³°å€¼
print("æ¯ä¸ª GPU çš„å³°å€¼ï¼š")
for gpu, stats in gpu_stats.items():
    print(f"GPU-{gpu}:")
    for k, v in stats.items():
        print(f"  {k}: {v:.2f}")

# è¾“å‡ºæ‰€æœ‰ GPU æ˜¾å­˜çš„å³°å€¼æ€»å’Œ
print(f"\næ‰€æœ‰ GPU æ˜¾å­˜å ç”¨çš„å³°å€¼æ€»å’Œ (MiB): {total_memory_peak:.2f}")

```





### äºŒã€vLLM

#### 001 ç¯å¢ƒé…ç½®

##### 1ã€é’ˆå¯¹ç§Ÿç”¨å¹³å°

`torch.cuda.is_available()`å‡ºç°cudaä¸å¯ç”¨ï¼Œæ¨æµ‹ä¸ºCUDA é©±åŠ¨æˆ–ç¡¬ä»¶å…¼å®¹æ€§é—®é¢˜ï¼Œå³vllmç‰ˆæœ¬ä¸torchã€cudaç‰ˆæœ¬ä¸åŒ¹é…

å½“å‰ç¯å¢ƒï¼š

- vllm 0.7.2ï¼ˆæ˜¯20250219å½“å‰æœ€æ–°ç‰ˆï¼‰
- torch 2.5.1+cu124
- ç³»ç»ŸCUDAï¼š12.4

å°è¯•é‡‡ç”¨å…¶ä»–åˆç†çš„ç‰ˆæœ¬ï¼Œå¦‚ï¼š

```
pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118
```

```
pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 torchaudio==2.3.0 torchtext torchdata --index-url https://download.pytorch.org/whl/cu121
pip install vllm==0.5.1
```

ä¸å¯ç”¨ï¼Œå°è¯•åˆ«äººå·²ç»éªŒè¯çš„ç¯å¢ƒï¼š

```
pip uninstall torch
pip uninstall vllm

pip install torch==2.3.0+cu118 xformers -f https://download.pytorch.org/whl/torch_stable.html
pip install vllm==0.5.1
```

è¿˜æ˜¯ä¸å¯å“Ÿã€‚

ä¸è¿‡å‘ç°ï¼ŒVLLMå®˜æ–¹æä¾›çš„CUDAå¯¹åº”çš„ç‰ˆæœ¬æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯11.8å’Œ12.1ã€‚



æœ€åè§£å†³æ–¹æ³•ï¼šåˆšå¼€å§‹ç§ŸæœåŠ¡å™¨ã€é€‰æ‹©é•œåƒçš„æ—¶å€™ï¼Œå°±é€‰æ‹©æ—§çš„vllmç‰ˆæœ¬ï¼Œç›´æ¥èƒ½ç”¨



##### 2ã€ä¸€èˆ¬æ–¹æ³•

ç¯å¢ƒé…ç½®ï¼š

```bash
pip install vllm transformers psutil numpy
```

å®‰è£…cudaç­‰æ·±åº¦å­¦ä¹ ç¯å¢ƒï¼š

```bash
conda install pytorch torchvision torchaudio cudatoolkit -c pytorch
```

é»˜è®¤æƒ…å†µä¸‹ï¼ŒvLLMä»HuggingFaceä¸‹è½½æ¨¡å‹ã€‚å¦‚æœæ‚¨æƒ³åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ä½¿ç”¨ModelScopeä¸­çš„æ¨¡å‹ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
echo 'export VLLM_USE_MODELSCOPE=True' >> ~/.bashrc
source ~/.bashrc
```

vllmæµ‹è¯•ä»£ç ï¼š

```python
from vllm import LLM
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]

llm = LLM(model="Qwen/Qwen2-0.5B",trust_remote_code=True,gpu_memory_utilization=0.9) 

outputs = llm.generate(prompts)
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

```



#### 002 æ–¹æ¡ˆä¸€ã€ç”¨vLLMåŸç”Ÿçš„åŸºå‡†æµ‹è¯•å·¥å…·

æºç å‚è€ƒï¼š

- https://github.com/vllm-project/vllm/tree/main/benchmarks
- https://github.com/xorbitsai/inference/blob/main/xinference/model/llm/vllm/core.py

é¡¹ç›®å‚è€ƒï¼š

- https://blog.csdn.net/arkohut/article/details/135167762
- https://blog.csdn.net/qq_36221788/article/details/142982659
- https://zhuanlan.zhihu.com/p/694568911
- https://blog.csdn.net/qq_36221788/article/details/142980919?spm=1001.2014.3001.5501



##### 1ã€åŸºå‡†æµ‹è¯•-ååé‡ã€Tokenè¾“å‡ºé€Ÿåº¦

æ€»çš„æ¥è¯´ï¼Œå®˜æ–¹çš„å‡ ä¸ªé€‚ç”¨äºå½“å‰åœºæ™¯çš„æµ‹è¯•å·¥å…·ï¼š

- [benchmark_serving.py](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving.py)ï¼šåŸºå‡†æµ‹è¯•åœ¨çº¿æœåŠ¡çš„ååé‡
- [benchmark_throughput.py](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_throughput.py)ï¼šåŸºå‡†æµ‹è¯•ç¦»çº¿æ¨ç†çš„ååé‡
- [benchmark_long_document_qa_throughput.py](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_long_document_qa_throughput.py)ï¼šåŸºå‡†æµ‹è¯•ç”¨äºé•¿æ–‡æ¡£QAååé‡

å…¶ä»–è„šæœ¬å„æœ‰ä½œç”¨



ç¯å¢ƒé…ç½®ï¼š

```
git clone -b v0.6.4.post1 --single-branch https://github.com/vllm-project/vllm.git
cd vllm
git describe --tags
```

æ•°æ®å‡†å¤‡ï¼šæ•°æ®é›†é‡‡ç”¨é»˜è®¤æ•°æ®é›†ï¼ˆsharegptï¼‰ï¼Œå®˜æ–¹æä¾›æ–¹æ³•å¦‚ä¸‹

```
wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json
```

é”™è¯¯æ–¹æ³•ï¼šä¸‹è½½çš„å†…å®¹æœ‰é—®é¢˜ï¼ŒåŸºå‡†æµ‹è¯•æ—¶å‡ºç°json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

```
git lfs install ; git clone https://www.modelscope.cn/datasets/otavia/ShareGPT_Vicuna_unfiltered.git
```



æ¨¡å‹å‡†å¤‡ï¼š

```
pip install -U huggingface_hub
huggingface-cli download --resume-download Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --local-dir Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4
```

Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4

Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4

Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4



å¯åŠ¨æ¨¡å‹ï¼šå¦‚æœæŒ‡å®šCUDA_VISIBLE_DEVICES=1 åè€Œä¼šæŠ¥é”™

```
python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8007 --max-model-len 8000 --model Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --disable-log-requests --swap-space 16 --tensor-parallel-size 2 --gpu-memory-utilization 0.9
```

å…¶ä»–å‚æ•°ï¼š

- é™åˆ¶å¹¶å‘è¯·æ±‚æ•°ï¼š

  ```
  --max-num-seqs 64 \        # é™åˆ¶å¹¶å‘å¤„ç†åºåˆ—æ•°ï¼ˆï¼‰
  --max-num-batched-tokens 2048 \  # é™åˆ¶æ‰¹å¤„ç†tokenæ•°ï¼ˆï¼‰
  --gpu-memory-utilization 0.8  # é¢„ç•™20%æ˜¾å­˜ä½™é‡ï¼ˆï¼‰
  ```

- ä¼˜åŒ–KVç¼“å­˜ç®¡ç†ï¼š

  ```
  --block-size 16 \          # å‡å°KVç¼“å­˜å—å¤§å°ï¼ˆï¼‰
  --enable-chunked-prefill \ # å¯ç”¨åˆ†å—é¢„å¡«å……ï¼ˆï¼‰
  --max-model-len 4096       # æ ¹æ®å®é™…éœ€æ±‚ç¼©çŸ­æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆï¼‰
  ```

- å†…å­˜å›æ”¶ç­–ç•¥ï¼š

  ```
  --swap-space 32 \          # å¢å¤§Swapç©ºé—´è‡³32GBï¼ˆåº”å¯¹çªå‘å³°å€¼ï¼‰
  --enforce-eager \          # ç¦ç”¨CUDAå›¾æ¨¡å¼ï¼ˆå‡å°‘å›¾å†…å­˜å ç”¨ï¼Œï¼‰
  ```
  
-  --save-result --result-dir=/home/temp 



æ³¨æ„ï¼š

- è¦ä½¿ç”¨è¯¥ç±»è¿è¡Œå¤š GPU æ¨ç†`LLM`ï¼Œè¯·å°†`tensor_parallel_size`å‚æ•°è®¾ç½®ä¸ºè¦ä½¿ç”¨çš„ GPU æ•°é‡

- æ‚¨è¿˜å¯ä»¥å¦å¤–æŒ‡å®š`--pipeline-parallel-size`å¯ç”¨ç®¡é“å¹¶è¡Œæ€§ã€‚ä¾‹å¦‚ï¼Œè¦åœ¨å…·æœ‰ç®¡é“å¹¶è¡Œæ€§å’Œå¼ é‡å¹¶è¡Œæ€§çš„ 8 ä¸ª GPU ä¸Šè¿è¡Œ API æœåŠ¡å™¨ï¼š`--tensor-parallel-size 4 --pipeline-parallel-size 2`

- VLLM_USE_PROFILER=1 VLLM_TORCH_PROFILER_DIR=/temp/ ç”¨äºæŒ‡å®šåˆ†ææ—¥å¿—è¾“å‡ºç›®å½•ï¼Œ**åœæ­¢æœåŠ¡æ—¶ï¼ˆæ­£å¸¸é€€å‡ºæœåŠ¡ï¼šéctrl + cï¼Œè€Œæ˜¯éœ€è¦å…³é—­è¿›ç¨‹ï¼‰**ï¼Œåˆ†ææ—¥å¿—ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `VLLM_TORCH_PROFILER_DIR` æŒ‡å®šçš„ç›®å½•ï¼ˆæ ¼å¼ä¸º `.pt.trace.json.gz`ï¼‰ã€‚

  - æ­£å¸¸é€€å‡ºæœåŠ¡æ–¹æ³•ï¼š

    ```
    # æŸ¥æ‰¾è¿›ç¨‹IDï¼ˆPIDï¼‰
    ps aux | grep "vllm.entrypoints.openai.api_server"
    # å‘é€ SIGTERM ä¿¡å·ï¼ˆå…è®¸æœåŠ¡å®Œæˆå½“å‰è¯·æ±‚ï¼‰
    kill -15 <PID>
    
    # æˆ–ç›´æ¥é€šè¿‡è¿›ç¨‹åç»ˆæ­¢
    pkill -f "vllm.entrypoints.openai.api_server"
    ```

    

![image-20250220152005780](./assets/image-20250220152005780.png)



æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸ï¼š

```
curl --location 'http://0.0.0.0:8007/v1/chat/completions' \
  --header 'Content-Type: application/json' \
  --data '{
  "model": "Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4",
  "messages": [
    {
      "role": "user",
      "content": "ç®€å•è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—"
    }
  ],
  "temperature": 0.2,
  "stream": true
}'
```



1ã€åŸºå‡†æµ‹è¯•åœ¨çº¿æœåŠ¡çš„ååé‡ï¼š

```
python vllm/benchmarks/benchmark_serving.py --backend vllm --port 8007 --endpoint /v1/completions --model Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --request-rate 10 --num-prompts 1000 --profile
```

æ³¨æ„ï¼š

- æ¨¡å‹æ–‡ä»¶å¤¹çš„å‘½åä¸èƒ½éšä¾¿å‘½åï¼Œæœ€å¥½ç”¨åŸæœ¬çš„åå­—
- è€ç‰ˆæœ¬çš„vllmæ²¡æœ‰--profileåŠŸèƒ½ï¼Œæ–‡ä»¶ä¸Šä¼ https://ui.perfetto.dev/å³å¯çœ‹åˆ°gpuç­‰ä½¿ç”¨è®°å½•

![image-20250220164340303](./assets/image-20250220164340303.png)





2ã€åŸºå‡†æµ‹è¯•ç¦»çº¿æ¨ç†çš„ååé‡ï¼šéœ€è¦å…ˆå…³é—­ä¸Šé¢çš„api_serverï¼Œç›´æ¥è¿è¡Œå¦‚ä¸‹æŒ‡ä»¤å³å¯

```
python vllm/benchmarks/benchmark_throughput.py --backend vllm --model Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --dataset ShareGPT_V3_unfiltered_cleaned_split.json --tensor-parallel-size 4
```

![image-20250220164316209](./assets/image-20250220164316209.png)



3ã€åŸºå‡†æµ‹è¯•ç”¨äºé•¿æ–‡æ¡£QAååé‡

```
python vllm/benchmarks/benchmark_long_document_qa_throughput.py --model Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --enable-prefix-caching --num-documents 8 --repeat-count 2  --tensor-parallel-size 4
```



##### 2ã€CPUã€å†…å­˜å ç”¨

æ–¹æ¡ˆä¸€ï¼šåŸºäºtopæŒ‡ä»¤



```
import os
import time
import signal
import sys

# å®šä¹‰æ—¥å¿—æ–‡ä»¶å
log_file = f"system_resource_log_{time.strftime('%Y%m%d%H%M%S')}.log"

# å¤„ç† Ctrl+C ä¿¡å·ä»¥é€€å‡ºå¹¶ä¿å­˜æ—¥å¿—
def signal_handler(sig, frame):
    print("\nåœæ­¢è®°å½•å¹¶ä¿å­˜æ—¥å¿—ã€‚")
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)

print("å¼€å§‹è®°å½•ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ... æŒ‰ Ctrl+C é€€å‡ºå¹¶ä¿å­˜æ—¥å¿—")
print(f"æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}")

# æŒç»­è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ
try:
    with open(log_file, "a") as f:
        while True:
            # ä½¿ç”¨ top å‘½ä»¤ä»¥æ‰¹å¤„ç†æ¨¡å¼è·å–èµ„æºä½¿ç”¨æƒ…å†µ
            top_output = os.popen("top -b -n 1").read()
            f.write(top_output)
            f.flush()
            time.sleep(1)  # æ¯ç§’è®°å½•ä¸€æ¬¡
except KeyboardInterrupt:
    # æ•è· Ctrl+C åé€€å‡º
    print("\nè®°å½•å·²åœæ­¢ã€‚æ—¥å¿—ä¿å­˜å®Œæˆã€‚")

```

```

```



æ–¹æ¡ˆäºŒï¼šåŸºäºpsutilåº“

ç¤ºä¾‹è„šæœ¬ï¼šæµ‹è¯•ä»£ç æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿcpuåˆ©ç”¨ç‡çš„å¹³å‡å€¼å’Œå³°å€¼ã€å†…å­˜å ç”¨çš„å¹³å‡å€¼å’Œå³°å€¼ï¼Œç”¨æˆ·ç»ˆç«¯è¾“å…¥ctrl+cæ—¶ï¼Œè¾“å‡ºç›¸å…³æŒ‡æ ‡

```
pip install psutil
```

```python
import psutil
import time
import signal
import sys

# ç”¨äºå­˜å‚¨CPUå’Œå†…å­˜çš„å®æ—¶ç»Ÿè®¡æ•°æ®
cpu_usages = []
memory_usages = []
peak_cpu = 0
peak_memory = 0

# æ•è·Ctrl+Cé€€å‡ºæ—¶çš„ä¿¡å·
def signal_handler(sig, frame):
    avg_cpu = sum(cpu_usages) / len(cpu_usages) if cpu_usages else 0
    avg_memory = sum(memory_usages) / len(memory_usages) if memory_usages else 0

    # æ‰“å°CPUå’Œå†…å­˜çš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\nCPUå ç”¨ç‡ï¼ˆå‡å€¼/å³°å€¼ï¼‰: {avg_cpu:.1f}% / {peak_cpu:.1f}%")
    print(f"å†…å­˜å ç”¨ï¼ˆå‡å€¼/å³°å€¼ï¼‰: {avg_memory / 1024:.1f}MB / {peak_memory / 1024:.1f}MB")
    sys.exit(0)

# æ³¨å†ŒCtrl+Cä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)

# æ¨¡æ‹Ÿè¿è¡Œçš„ä»»åŠ¡ï¼Œæ”¶é›†CPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
print("ç¨‹åºæ­£åœ¨è¿è¡Œï¼Œè¯·ç¨å€™...æŒ‰Ctrl+Cé€€å‡ºå¹¶æŸ¥çœ‹ç»“æœ")

try:
    while True:
        # è·å–CPUå ç”¨ç‡ï¼ˆæ¯ç§’ä¸€æ¬¡ï¼‰
        cpu_usage = psutil.cpu_percent(interval=1)
        cpu_usages.append(cpu_usage)
        peak_cpu = max(peak_cpu, cpu_usage)

        # è·å–å†…å­˜å ç”¨æƒ…å†µï¼ˆå½“å‰ä½¿ç”¨çš„ç‰©ç†å†…å­˜ï¼‰
        memory_usage = psutil.virtual_memory().used
        memory_usages.append(memory_usage)
        peak_memory = max(peak_memory, memory_usage)

        # å®æ—¶æ˜¾ç¤ºCPUå’Œå†…å­˜å ç”¨
        print(f"\rå½“å‰CPUå ç”¨ç‡: {cpu_usage:.1f}%  å½“å‰å†…å­˜å ç”¨: {memory_usage / 1024:.1f}MB", end='', flush=True)

        # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—ä»»åŠ¡ï¼Œé¿å…CPUå ç”¨è¿‡ä½
        time.sleep(0.1)

except KeyboardInterrupt:
    pass  # é€šè¿‡Ctrl+Cè§¦å‘çš„å¼‚å¸¸å¤„ç†

```



##### 3ã€GPUç›¸å…³

å‚è€ƒtransformersæ–¹æ³•

æˆ–è€…é‡‡ç”¨vllmè‡ªå¸¦çš„å·¥å…·ï¼ˆæš‚æœªä½¿ç”¨æˆåŠŸï¼‰



#### 003 æ–¹æ¡ˆäºŒã€è‡ªå·±æ’°å†™æµ‹è¯•ç¨‹åº

##### 1ã€tokenè¾“å‡ºé€Ÿåº¦ + cpuå ç”¨ + å†…å­˜å ç”¨

å‚è€ƒå®˜æ–¹ç¤ºä¾‹æ’°å†™çš„æµ‹è¯•æ ·ä¾‹ï¼ˆæ³¨æ„ï¼Œdeepseekç­‰å†™çš„ä»£ç ï¼Œä¼šä¹±ç”¨æ²¡æœ‰çš„å‡½æ•°ï¼‰ï¼štokensè¾“å‡ºé€Ÿåº¦è®¡ç®—ä¸­ï¼Œæ—¶é—´éƒ¨åˆ†æœ‰é—®é¢˜ï¼Œéœ€çº æ­£

```python
import time
import torch
from vllm import LLM, SamplingParams
 
# Step 1: ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
model_name = "model"  # æ¨¡å‹æ‰€åœ¨è·¯å¾„
 
# åˆå§‹åŒ–vLLMæ¨¡å‹
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cpu":
    raise RuntimeError("vLLM currently does not support CPU. Please use a compatible GPU.")
 
llm = LLM(model=model_name, trust_remote_code=True,gpu_memory_utilization=0.9)
 
# Step 2: æ¨¡å‹é¢„çƒ­å‡½æ•°
def warm_up_model(warm_up_tokens=100):
    # ç”Ÿæˆä¸€å®šæ•°é‡çš„tokensæ¥è¿›è¡Œé¢„çƒ­
    prompt = "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»å¤§è¯­è¨€æ¨¡å‹"
    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=warm_up_tokens)
    outputs = llm.generate(prompt, sampling_params)
 
# Step 3: è¿›è¡Œtokenè¾“å‡ºé€Ÿåº¦æµ‹è¯•
def test_token_speed(warm_up_tokens=100, test_tokens=50000):
    # é¢„çƒ­æ¨¡å‹
    print("å¼€å§‹é¢„çƒ­æ¨¡å‹...")
    warm_up_model(warm_up_tokens)
    print("æ¨¡å‹é¢„çƒ­å®Œæˆï¼")
    
    sum_prompt_tokens = []
    sum_generated_text_tokens = []
    
    # åˆ›å»ºæµ‹è¯•ç”¨çš„è¾“å…¥
    prompts = [
        "Hello, my name is",
        "The president of the United States is",
        "The capital of France is",
        "The future of AI is",
    ]
    # Create a sampling params object.
    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
 
    # æµ‹è¯•
    start_time = time.time()
    outputs = llm.generate(prompts, sampling_params)
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    print("######")
    print(outputs)
    print("######")
    
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
 
        tokens_prompt = len(prompt.split())
        tokens_generated = len(generated_text.split())  # ç»Ÿè®¡è¾“å‡ºçš„tokenæ•°é‡
        print(f"tokens_prompt: {tokens_prompt},tokens_generated: {tokens_generated}")
 
        sum_prompt_tokens.append(tokens_prompt)
        sum_generated_text_tokens.append(tokens_generated)
        
    # è®¡ç®—å¹³å‡é€Ÿåº¦
    average_speed = sum(sum_generated_text_tokens) / elapsed_time
    print(f"\nå¹³å‡tokenè¾“å‡ºé€Ÿåº¦: {average_speed:.2f} tokens/ç§’")
    return average_speed
 
# Step 4: è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    test_token_speed()
```

ç¤ºä¾‹è¾“å‡ºï¼š

```
RequestOutput(
	request_id=1, 
	prompt='Hello, my name is', 
	prompt_token_ids=[9707, 11, 847, 829, 374], 
	prompt_logprobs=None, 
	outputs=[
	CompletionOutput(
        index=0, 
        text=' Margo, I have been running for 25 years. I was diagnosed', 
        token_ids=(386, 12088, 11, 358, 614, 1012, 4303, 369, 220, 17, 20, 1635, 13, 358, 572, 28604), 
        cumulative_logprob=-32.0423269867897, 
        logprobs=None, 
        finish_reason=length, 
        stop_reason=None)], 
	finished=True,
	metrics=RequestMetrics(
		arrival_time=1739948349.163026,
		last_token_time=1739948349.163026, 	
		first_scheduled_time=1739948349.1640651,
		first_token_time=1739948349.1878772,
		time_in_queue=0.0010390281677246094,
		finished_time=1739948349.2522297), 
	lora_request=None
)
```





##### 2ã€ååé‡

ç¤ºä¾‹ä»£ç ï¼štokensè¾“å‡ºé€Ÿåº¦è®¡ç®—ä¸­ï¼Œæ—¶é—´éƒ¨åˆ†æœ‰é—®é¢˜ï¼Œéœ€çº æ­£

```python
from vllm import LLM, SamplingParams
import time
 
# 1. åˆå§‹åŒ–æœ¬åœ°å¤§æ¨¡å‹
llm = LLM(
    model="model",  # æœ¬åœ°æ¨¡å‹è·¯å¾„
    # tensor_parallel_size=2,            # GPUå¹¶è¡Œæ•°
    # max_model_len=102400               # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
)
 
# 2. å®šä¹‰æµ‹è¯•å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_k=50,
    max_tokens=512,        # æœ€å¤§ç”Ÿæˆtokenæ•°
    ignore_eos=True        # å¼ºåˆ¶ç”Ÿæˆåˆ°max_tokens
)
 
prompts = ["è¯·è§£é‡Šé‡å­åŠ›å­¦çš„åŸºæœ¬åŸç†"] * 10  # 10ä¸ªé‡å¤è¯·æ±‚ç”¨äºå‹åŠ›æµ‹è¯•
 
# 3. æ‰§è¡Œæ¨ç†å¹¶æµ‹é‡æ€§èƒ½
start_time = time.time()
outputs = llm.generate(prompts, sampling_params)
total_time = time.time() - start_time
 
# 4. è®¡ç®—å…³é”®æŒ‡æ ‡
total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)
avg_tput = total_tokens / total_time  # æ€»ååé‡ï¼ˆtokens/secï¼‰
 
# é¦–tokenå»¶è¿Ÿï¼ˆTTFTï¼‰ç»Ÿè®¡
ttft_list = [output.metrics.first_token_time for output in outputs]
avg_ttft = sum(ttft_list) / len(ttft_list) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
 
# å•tokenå»¶è¿Ÿï¼ˆTPOTï¼‰ç»Ÿè®¡
tpot_list = [
    (output.metrics.time_per_output_token * 1000)  # æ¯«ç§’/token
    for output in outputs if output.metrics.time_per_output_token
]
 
# 5. è¾“å‡ºç»“æœ
print(f"""
========== æ€§èƒ½æµ‹è¯•æŠ¥å‘Š ==========
æ€»è¯·æ±‚æ•°: {len(prompts)}
æ€»ç”Ÿæˆtokenæ•°: {total_tokens}
æ€»è€—æ—¶: {total_time:.2f}s
å¹³å‡ååé‡: {avg_tput:.2f} tokens/sec
å¹³å‡é¦–tokenå»¶è¿Ÿ: {avg_ttft:.2f} ms
å¹³å‡å•tokenå»¶è¿Ÿ: {sum(tpot_list)/len(tpot_list):.2f} ms
==================================
""")
```



##### 3ã€GPUç›¸å…³

å‚è€ƒæµ‹è¯•â€œLLMç¦»çº¿æ¨¡å‹â€



### ä¸‰ã€åŸºäºollama + è¿œç¨‹è¿æ¥Dify + ä¸CloudPSSäº¤äº’

é‡‡ç”¨Ollamaé»˜è®¤é…ç½®æ–‡ä»¶ï¼Œæœªä¿®æ”¹

#### 0ã€ç¯å¢ƒé…ç½®

linuxä¸‹ollamaå®‰è£…æ–¹æ³•ï¼š

```
curl -fsSL https://ollama.com/install.sh | sh
```

æœ‰æ—¶å€™ `wget` å¯èƒ½ä¼šæ¯” `curl` æ›´åŠ ç¨³å®šï¼Œå°¤å…¶åœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œå®ƒèƒ½æ›´å¥½åœ°å¤„ç†ç½‘ç»œä¸­æ–­ã€‚ä½ å¯ä»¥è¯•è¯•è¿™ä¸ªå‘½ä»¤ï¼ˆå®æµ‹è¦å¿«å¾ˆå¤šï¼‰ï¼š

```
wget https://ollama.com/install.sh -O - | sh
```



å¯åŠ¨æœåŠ¡ï¼š`ollama serve`ï¼Œollamaä¸‹è½½å¤§æ¦‚10minï¼Œä¸¤ä¸ª72Bæ¨¡å‹ï¼Œå¤§æ¦‚ä¸‹è½½1h

```
ollama run deepseek-r1:32b-qwen-distill-q4_K_M
ollama run qwen2.5:32b-instruct-q4_K_M

ollama run deepseek-r1:70b-llama-distill-q4_K_M
ollama run qwen2.5:72b-instruct-q4_K_M
ollama run deepseek-r1:32b-qwen-distill-fp16
```

å¦‚ä½•æé«˜ä¸‹è½½é€Ÿåº¦ï¼š

- æ–¹æ³•ä¸€ï¼šä¸‹è½½è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ `Ctrl + C` ä¸­æ–­ `ollama run ***` çš„å‘½ä»¤åï¼Œå†é‡æ–°æ‰§è¡Œåï¼Œé€Ÿåº¦ä¼šå¿«ä¸Šä¸€æ®µæ—¶é—´ã€‚ï¼ˆè¿™ä¸ªè¿‡ç¨‹ä¹Ÿå¯ä»¥å°è£…æˆè„šæœ¬ï¼‰
- æ–¹æ³•äºŒï¼šä¿®æ”¹ollamaæœåŠ¡æ–‡ä»¶ï¼Œæ·»åŠ http_proxyã€https_proxyåŠ é€Ÿåœ°å€ï¼Œå†é‡è½½systemdç®¡ç†å™¨é…ç½®ï¼Œå¹¶é‡å¯ollamaæœåŠ¡





æ³¨æ„ï¼š

- Q1ï¼šå…³äºollamaæ‰€ç”¨gpuç¼“å­˜å¦‚ä½•é‡Šæ”¾ï¼Ÿ
  - A1ï¼šå¦‚æœåˆ‡æ¢æ¨¡å‹ï¼Œollamaç¼“å­˜ä¼šè‡ªåŠ¨é‡Šæ”¾å¹¶åŠ è½½æ–°æ¨¡å‹ï¼Œå¦‚æœæ˜¯æ—§æ¨¡å‹ï¼ŒçŸ­æ—¶é—´å†…ollamaä¸ä¼šè‡ªå·±é‡Šæ”¾æ˜¾å­˜ï¼Œç›®å‰æ²¡æœ‰é€šè¿‡ä»£ç å®ç°è¿™ä¸€è¿‡ç¨‹ï¼Œéœ€è¦æ‰‹åŠ¨é‡å¯ollamaæœåŠ¡å³å¯
- Q2ï¼šå¦‚ä½•æ¸…é™¤ollamaæœªä¸‹è½½å®Œæ¯•çš„æ¨¡å‹å ç”¨çš„ç£ç›˜ç©ºé—´
  - cd ~/.ollamaï¼Œåˆ é™¤modelsç›®å½•




#### 1ã€tokenè¾“å‡ºé€Ÿåº¦ + cpuå ç”¨ + å†…å­˜å ç”¨

ç¤ºä¾‹ä»£ç ï¼š

```python
import os
import time
import threading
import psutil
import numpy as np
import torch
import ollama

# èµ„æºç›‘æ§ç±»
class ResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    def __init__(self, interval=1):
        self.interval = interval  # é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        self.cpu_percentages = []  # CPUä½¿ç”¨ç‡è®°å½•
        self.memory_usages = []  # å†…å­˜å ç”¨è®°å½•ï¼ˆMBï¼‰
        self.gpu_memory_usages = {}  # GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        self.is_monitoring = False
        self.process = psutil.Process(os.getpid())  # ç›‘æ§å½“å‰è¿›ç¨‹

    def _monitor(self):
        """ç›‘æ§çº¿ç¨‹ä¸»å‡½æ•°"""
        while self.is_monitoring:
            # CPUå’Œå†…å­˜ç›‘æ§
            cpu_percent = psutil.cpu_percent(interval=None)
            mem_usage = self.process.memory_info().rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
            self.cpu_percentages.append(cpu_percent)
            self.memory_usages.append(mem_usage)

            # GPUæ˜¾å­˜ç›‘æ§
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    mem_allocated = torch.cuda.memory_allocated(i) / 1024 ** 3  # è½¬æ¢ä¸ºGB
                    if i not in self.gpu_memory_usages:
                        self.gpu_memory_usages[i] = []
                    self.gpu_memory_usages[i].append(mem_allocated)

            time.sleep(self.interval)

    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        self.is_monitoring = True
        self.thread = threading.Thread(target=self._monitor)
        self.thread.start()

    def stop(self):
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        self.is_monitoring = False
        if self.thread.is_alive():
            self.thread.join()

        stats = {
            "cpu_avg": np.mean(self.cpu_percentages) if self.cpu_percentages else 0,
            "cpu_peak": np.max(self.cpu_percentages) if self.cpu_percentages else 0,
            "mem_avg": np.mean(self.memory_usages) if self.memory_usages else 0,
            "mem_peak": np.max(self.memory_usages) if self.memory_usages else 0
        }

        gpu_stats = {}
        if torch.cuda.is_available():
            for i, mem_usages in self.gpu_memory_usages.items():
                gpu_stats[f"gpu_{i}_avg"] = np.mean(mem_usages)
                gpu_stats[f"gpu_{i}_peak"] = np.max(mem_usages)

        stats.update(gpu_stats)
        return stats


# æ€§èƒ½æµ‹è¯•å‡½æ•°
def benchmark_model(
        model_name="Qwen/Qwen2.5-72B",  # ä½¿ç”¨çš„æ¨¡å‹åç§°
        max_new_tokens=512,
        num_runs=3,
):
    if not model_name:
        print("é”™è¯¯: æœªæŒ‡å®šæ¨¡å‹ï¼è¯·ç¡®ä¿è¾“å…¥æœ‰æ•ˆçš„æ¨¡å‹åç§°ã€‚")
        return

    print(f"\n=== è¿è¡Œç¯å¢ƒé…ç½® ===")
    print(f"ä½¿ç”¨çš„æ¨¡å‹: {model_name}")

    print("\n=== æ­£åœ¨åŠ è½½æ¨¡å‹ ===")
    load_start = time.time()

    try:
        response = ollama.chat(model=model_name, messages=[{"role": "user", "content": "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»å¤§è¯­è¨€æ¨¡å‹"}])
        print(f"æ¨¡å‹åŠ è½½è€—æ—¶: {time.time() - load_start:.2f}s")
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        return

    messages = [{"role": "user", "content": "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»å¤§è¯­è¨€æ¨¡å‹"}]

    print("\n=== é¢„çƒ­é˜¶æ®µ ===")
    response = ollama.chat(model=model_name, messages=messages)
    print("é¢„çƒ­é˜¶æ®µå“åº”:", response.message.content)

    print(f"\n=== å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼ˆ{num_runs}æ¬¡å¹³å‡ï¼‰===")
    total_time = 0
    total_tokens = 0

    # å¯åŠ¨èµ„æºç›‘æ§
    monitor = ResourceMonitor(interval=0.5)
    monitor.start()

    for run_idx in range(num_runs):
        start_time = time.time()

        response = ollama.chat(model=model_name, messages=messages)

        elapsed = time.time() - start_time
        new_tokens = len(response.message.content)

        total_time += elapsed
        total_tokens += new_tokens

        print(f"[{run_idx + 1}/{num_runs}] Tokens: {new_tokens} | Time: {elapsed:.2f}s | Speed: {new_tokens / elapsed:.2f}tok/s")

    resource_stats = monitor.stop()

    avg_tps = total_tokens / total_time
    avg_latency = total_time / num_runs

    print("\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    print(f"å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_tps:.2f} tokens/ç§’")
    print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ç§’/è¯·æ±‚")
    print(f"CPUå ç”¨ç‡ï¼ˆå‡å€¼/å³°å€¼ï¼‰: {resource_stats['cpu_avg']:.1f}% / {resource_stats['cpu_peak']:.1f}%")
    print(f"å†…å­˜å ç”¨ï¼ˆå‡å€¼/å³°å€¼ï¼‰: {resource_stats['mem_avg']:.1f}MB / {resource_stats['mem_peak']:.1f}MB")

    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            print(f"GPU-{i} æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼šå‡å€¼: {resource_stats[f'gpu_{i}_avg']:.2f} GB, å³°å€¼: {resource_stats[f'gpu_{i}_peak']:.2f} GB")

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("GPUç¼“å­˜å·²é‡Šæ”¾ã€‚")


if __name__ == "__main__":
    # llm_model_name = "deepseek-r1:32b-qwen-distill-q4_K_M"
    # llm_model_name = "ddeepseek-r1:32b-qwen-distill-fp16"
    # llm_model_name = "deepseek-r1:70b-llama-distill-q4_K_M"
    llm_model_name = "qwen2.5:72b-instruct-q4_K_M"

    benchmark_model(
        model_name=llm_model_name,
        max_new_tokens=128,
        num_runs=3
    )

```





#### 2ã€ååé‡

è°ƒèŠ‚MODEL_NAMEç­‰å››ä¸ªæµ‹è¯•å‚æ•°å³å¯

```python
import ollama
import torch
import time
import threading
from concurrent.futures import ThreadPoolExecutor

# è·å–å¯ç”¨ GPU æ•°é‡
if torch.cuda.is_available():
    num_gpus = torch.cuda.device_count()
else:
    num_gpus = 1  # å¦‚æœæ²¡æœ‰ GPUï¼Œåˆ™é»˜è®¤ä½¿ç”¨ CPU

# æµ‹è¯•å‚æ•°ï¼Œå¯è°ƒæ•´
MODEL_NAME = "deepseek-r1:32b-qwen-distill-q4_K_M"  # éœ€è¦æµ‹è¯•çš„ Ollama æ¨¡å‹åç§°
NUM_CONCURRENT_REQUESTS = num_gpus * 2  # å¹¶å‘è¯·æ±‚æ•°ï¼Œå»ºè®®è®¾ä¸º GPU æ•°é‡çš„å€æ•°
NUM_TEST_ROUNDS = 2  # æµ‹è¯•è½®æ•°
PROMPT = "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»å¤§è¯­è¨€æ¨¡å‹"  # éœ€è¦æµ‹è¯•çš„æç¤ºè¯


def generate_text(model: str, prompt: str):
    """
    è°ƒç”¨ Ollama API ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶è¿”å›ç”Ÿæˆçš„æ–‡æœ¬åŠå…¶ Token æ•°é‡
    :param model: æ¨¡å‹åç§°
    :param prompt: è¾“å…¥æ–‡æœ¬
    :return: ç”Ÿæˆçš„æ–‡æœ¬ä»¥åŠ Token æ•°é‡
    """
    try:
        # è°ƒç”¨ Ollama API è·å–å“åº”
        response = ollama.chat(model=model, messages=[{"role": "user", "content": prompt}])
        text = response.message.content
        # print(f"##################")
        # print(text)
        # print(f"##################")
        # è·å–ç”Ÿæˆæ–‡æœ¬çš„ Token æ•°é‡ï¼ˆä½¿ç”¨æ–‡æœ¬é•¿åº¦ä¼°ç®— Token æ•°é‡ï¼‰
        token_count = len(text)  # ä½¿ç”¨å­—ç¬¦é•¿åº¦æ¥ä¼°ç®— Token æ•°é‡

        return text, token_count  # è¿”å›æ–‡æœ¬å’Œ token æ•°é‡
    except Exception as e:
        return f"Error: {str(e)}", 0  # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯å’Œ0 tokenæ•°


def throughput_test():
    """
    è¿›è¡Œååé‡æµ‹è¯•ï¼Œè®¡ç®— QPSï¼ˆQueries Per Secondï¼‰ä»¥åŠ Tokenååé‡
    """
    print(f"Running throughput test with {NUM_CONCURRENT_REQUESTS} concurrent requests...")

    total_requests = 0
    total_time = 0
    total_tokens = 0  # ç´¯è®¡ Token æ•°é‡

    for round_num in range(NUM_TEST_ROUNDS):
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=NUM_CONCURRENT_REQUESTS) as executor:
            futures = [executor.submit(generate_text, MODEL_NAME, PROMPT) for _ in range(NUM_CONCURRENT_REQUESTS)]
            results = [future.result() for future in futures]

        round_time = time.time() - start_time
        total_time += round_time
        total_requests += NUM_CONCURRENT_REQUESTS

        # ç´¯ç§¯ Token æ•°é‡
        round_tokens = sum([result[1] for result in results])
        total_tokens += round_tokens

        print(f"Round {round_num + 1}: {NUM_CONCURRENT_REQUESTS} requests in {round_time:.2f} seconds")

    avg_qps = total_requests / total_time
    avg_token_throughput = total_tokens / total_time  # è®¡ç®—å¹³å‡æ¯ç§’ Token æ•°é‡

    print(f"\nFinal Throughput: {avg_qps:.2f} QPS")
    print(f"Final Token Throughput: {avg_token_throughput:.2f} Tokens/s")


if __name__ == "__main__":
    throughput_test()

```





#### 3ã€GPUç›¸å…³

å‚è€ƒæµ‹è¯•â€œLLMç¦»çº¿æ¨¡å‹â€



#### 4ã€å…¶ä»–

Difyå¸¸è§è¾“å‡ºé”™è¯¯ï¼š

- `Run failed: Model Parameter num_predict should be less than or equal to 4096.0.`
  - æ¨¡å‹è®¾ç½®æ—¶toekenå¤§å°è®¾ç½®è¿‡å°ï¼Œæé«˜tokenå³å¯
- `Run failed: got invalid json object. error: Expecting ',' delimiter: line 6 column 9 (char 379)`
  - é‡æ–°è¿è¡Œå‡ æ¬¡å°±å¥½äº†
- `Run failed: [xinference] Server Unavailable Error, Failed to create the embeddings, detail: [address=127.0.0.1:60116, pid=15984] Model not found in the model list, uid: abge-large-zh-v1.5`
  - ç”¨åˆ°äº†xinferenceçš„embeddingæ¨¡å‹ï¼Œä½†æ˜¯xinferenceæ²¡å¯åŠ¨



ç§Ÿç”¨å¹³å°ollamaæ¨¡å‹æ¥å…¥åˆ°æ ¡å†…æœåŠ¡å™¨ï¼š

```
ssh -L 8090:localhost:11434 root@ssh.openbayes.com -p31898
```

å‘½ä»¤çš„è§£é‡Šï¼š

- `8090:localhost:11434`ï¼šå°†æœ¬åœ°æœºå™¨çš„ 8090 ç«¯å£æ˜ å°„åˆ°ç›®æ ‡æœåŠ¡å™¨çš„ 11434 ç«¯å£ã€‚
  
- `root@ssh.openbayes.com -p31894`ï¼šç›®æ ‡æœåŠ¡å™¨çš„åœ°å€å’Œç«¯å£ã€‚
  

ç„¶åï¼Œåœ¨æ ¡å†…æœåŠ¡å™¨çš„difyä¸­å¯¼å…¥æ¨¡å‹å³å¯ï¼Œå¦‚`http://host.docker.internal:8090`





### å››ã€æµ‹è¯•æ€»ç»“


ä¸€ã€å®éªŒè®¾è®¡
è§ä¸Šæ–‡

äºŒã€å®éªŒåˆ†æ
ï¼ˆ1ï¼‰é¦–å…ˆï¼ŒåŸºäºtransformersæµ‹è¯•ï¼š
    æ²¡æœ‰å‚è€ƒä»·å€¼ï¼Œtransformersæ¡†æ¶æ¨ç†é€Ÿåº¦å¤ªæ…¢ï¼Œä½¿ç”¨å…¶åšæµ‹è¯•åªæ˜¯ç”¨äºç†Ÿæ‚‰transformersåº“ç›¸å…³APIä¸æ€§èƒ½ã€å­¦ä¹ åŸºå‡†æµ‹è¯•å‚æ•°ã€‚

ï¼ˆ2ï¼‰å…¶æ¬¡ï¼ŒåŸºäºOllamaæµ‹è¯•ï¼š
    ç”±äºOllamaéƒ¨ç½²æ¨¡å‹çš„ç®€æ˜“æ€§ï¼Œå¯ç”¨äºå¿«é€Ÿéƒ¨ç½²æ¨¡å‹ã€äº†è§£æ¨¡å‹å¤§è‡´æƒ…å†µã€‚
    ç”±æµ‹è¯•ç»“æœå¯çŸ¥ï¼ŒåŸºäºOllamaæ–¹æ¡ˆä¸‹ï¼Œ72B-int4å·¦å³çš„æ¨¡å‹ï¼Œå¤§è‡´éœ€è¦4å¡4090 / 2å¡A6000ï¼Œ32B-int4å·¦å³æ¨¡å‹ï¼Œå¤§è‡´éœ€è¦å•å¡4090ï¼Œå„ä¸ªåœºæ™¯ä¸‹æ¨¡å‹çš„tokenè¾“å‡ºé€Ÿåº¦å‡æ»¡è¶³æˆå¹´äººæ­£å¸¸é˜…è¯»é€Ÿåº¦ã€‚
    ä½†æ˜¯å­˜åœ¨ä»¥ä¸‹å‡ ä¸ªé—®é¢˜ï¼š
    1ã€Ollamaå¤šå¡ã€é»˜è®¤æƒ…å†µä¸‹ï¼Œéƒ¨åˆ†åœºæ™¯GPUåˆ©ç”¨ç‡ä½ï¼šå¦‚æœèƒ½å¤Ÿå•å¡è¿è¡ŒæŸæ¨¡å‹ï¼ŒOllamaå°±ä¼šç›´æ¥é—²ç½®åˆ«çš„å¡ï¼Œè¿™ä¸ªéœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œä½†æ˜¯æµ‹è¯•æ—¶æ²¡æœ‰æ³¨æ„æ­¤é—®é¢˜ï¼Œéœ€è¦åœ¨åç»­åŸºäºvLLMæµ‹è¯•æ—¶æ³¨æ„ï¼›
    2ã€ååé‡è¾ƒä½ï¼Œåœ¨å¹¶è¡Œè¯·æ±‚è¾ƒå¤šçš„æƒ…å†µä¸‹tokenè¾“å‡ºé€Ÿåº¦è¾ƒä½ï¼Œä»…é€‚ç”¨äºè‡ªå·±ç©ï¼Œä¸é€‚ç”¨äºç»™ç”¨æˆ·åšéƒ¨ç½²ï¼Œè¿™ä¸ªä»æ ¹æœ¬ä¸Šå¦å®šäº†Ollamaéƒ¨ç½²çš„æ–¹æ¡ˆã€‚
   æ€»çš„æ¥è¯´ï¼ŒOllamaå¸®åŠ©æµ‹è¯•è€…å¿«é€Ÿäº†è§£äº†å„ä¸ªæ¨¡å‹çš„æ™ºåŠ›æ°´å¹³ä¸å¤§è‡´æ€§èƒ½ï¼Œç”±æµ‹è¯•ç»“æœå¯çŸ¥ï¼š
    1ã€æš‚ä¸é‡‡ç”¨deepseek-r1ç³»ç»Ÿï¼šè™½ç„¶å…¶tokenè¾“å‡ºé€Ÿåº¦æ­£å¸¸ï¼Œä½†æ˜¯æ€è€ƒè¿‡ç¨‹ä¸­tokenè¿‡å¤šï¼Œå¯¼è‡´å®Œæˆä»»åŠ¡æ—¶é—´è¶…å‡ºé¢„æœŸï¼Œæ‰€ä»¥ä¸é‡‡ç”¨ï¼›
    2ã€äº¤ä»˜å¿…é¡»é‡‡ç”¨vLLMè€ŒéOllamaï¼šç”±äºè€ƒè™‘æ­é…ç”¨æˆ·ç”¨æ³•ä½¿ç”¨å¤§æ¨¡å‹ï¼ŒOllamaååé‡è¾ƒä½ï¼Œé¡¾è¿˜æ˜¯éœ€è¦vLLMï¼Œå¹¶ä¸”ä»¥åï¼Œåœ¨ç†Ÿæ‚‰vLLMæ¡†æ¶çš„å‰æä¸‹ï¼Œä»¥åè¿˜æ˜¯ç›´æ¥åŸºäºvLLMæµ‹è¯•æ›´æ–¹ä¾¿ï¼›
    3ã€ä¸é‡‡ç”¨qwen-32b-int4ï¼šè™½ç„¶å…¶ä»…éœ€ä¸€å¼ 4090å³å¯ï¼Œä½†è€ƒè™‘é¡¹ç›®çš„ç¨³å®šæ€§ï¼Œæ‰€ä»¥ä¸å†é‡‡ç”¨æ­¤æ¨¡å‹ã€‚

ï¼ˆ3ï¼‰æœ€åï¼ŒåŸºäºvLLMæµ‹è¯•ï¼š
    vLLMç¡®å®æ¯”è¾ƒè€—æ˜¾å­˜ï¼Œä½†æ˜¯ååé‡ä¼˜åŒ–æ•ˆæœæ˜æ˜¾ï¼Œä¸”å¯¹æ¯”å››å¡4090ä¸åŒå¡A6000

ä¸‰ã€å®éªŒæ€»ç»“
   æœ€åå¾—å‡ºç»“è®ºï¼š
    1ã€é‡‡ç”¨åŒå¡A6000æ‰§è¡Œqwen2.5:72b-instruct-q4_K_M,æœ€é€‚åˆå½“å‰é¡¹ç›®ï¼Œå…³äºç§Ÿç”¨å¹³å°çš„A6000æ˜¯å¦é‡‡ç”¨å®‰åŸ¹æ¶æ„ã€æ˜¯å¦é‡‡ç”¨Nvlinkï¼Œæ­£åœ¨å’¨è¯¢å®˜æ–¹
    2ã€ä»¥åæµ‹è¯•æ¨¡å‹ï¼Œç›´æ¥é‡‡ç”¨vLLMæ¡†æ¶ï¼Œæˆ–è€…é‡‡ç”¨Xinferenceè‡ªå¸¦çš„åŸºå‡†æµ‹è¯•æ–¹æ¡ˆï¼Œæ— éœ€å†æµ‹è¯•Ollama/Transformersæ¡†æ¶ï¼ˆä¸”è¿™ä¸¤ä¸ªæ¡†æ¶åŸºå‡†æµ‹è¯•å·¥å…·å‡æ˜¯è‡ªå·±å†™çš„ï¼Œé—®é¢˜å¤šã€éšæ‚£å¤šï¼‰
    

å››ã€å…¶ä»–
   æœ¬å®éªŒæ‰€ç”±è„šæœ¬å‡å·²å¤‡ä»½ï¼Œéƒ¨åˆ†è„šæœ¬æ‰€å¾—è¯„æµ‹æŒ‡æ ‡å¹¶ä¸å‡†ç¡®ï¼Œä»éœ€ä¼˜åŒ–ï¼š
    1ã€cpuä¸å†…å­˜å ç”¨ä¸åº”è¯¥åŸºäºpsutilåº“ï¼Œåœ¨ç§Ÿç”¨å¹³å°ä¸Šï¼Œæµ‹è¯•ç»“æœå­˜åœ¨é—®é¢˜ï¼Œç›´æ¥è§£ælinuxçš„topæŒ‡ä»¤çš„æ—¥å¿—æ‰æ˜¯æœ€å‡†ç¡®çš„ï¼›
    2ã€vLLMåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨çº¿æµ‹è¯•å†…å­˜æ¶ˆè€—ä¸¥é‡ï¼Œå¿…é¡»é™ä½æµ‹è¯•å‹åŠ›ï¼Œç¦»çº¿æµ‹è¯•è¿˜ç®—æ­£å¸¸ï¼Œä¼˜åŒ–ä¸­
    æ­¤å¤–ï¼Œè¿˜æœ‰Xinferenceæµ‹è¯•æ–¹æ¡ˆï¼Œä½†æ¶‰åŠåˆ°æ¨¡å‹å¯¼å…¥å¤±è´¥ï¼Œæ‰€ä»¥æš‚æ—¶æ²¡æœ‰æµ‹è¯•ã€‚ä¸è¿‡Xinferenceä¹Ÿæœ‰å®˜æ–¹çš„åŸºå‡†æµ‹è¯•ç¨‹åºï¼ŒGithubå’¨è¯¢å®˜æ–¹åï¼Œå…¶å¯èƒ½ä¸vLLMå®˜æ–¹æµ‹è¯•ç¨‹åºæœ‰å‡ºå…¥ï¼Œä½†ä¹Ÿç®—å¤šäº†ä¸ªé€‰æ‹©





## 202503

### ä¸€ã€SGLang

#### 1ã€ç¯å¢ƒé…ç½®

å®‰è£…ï¼šå®˜æ–¹æ•™ç¨‹å¾ˆå¤šï¼Œè¿™é‡ŒåŸºäºuvå®‰è£…ï¼šhttps://docs.sglang.ai/start/install.html

```
pip install --upgrade pip
pip install uv
uv pip install "sglang[all]>=0.4.3.post4" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python
```



![image-20250307090126668](./assets/image-20250307090126668.png)

![image-20250307090138433](./assets/image-20250307090138433.png)



å¸¸è§é—®é¢˜ï¼š

Q1ï¼šRuntimeError: CUDA error: invalid device ordinal

A1ï¼šå› ä¸ºæŒ‡å®šäº†æ²¡æœ‰çš„CUDAï¼Œdeviceé‚£é‡Œè®¾ç½®é”™è¯¯ï¼Œè¿™é‡Œæ˜¯å› ä¸ºæœåŠ¡å™¨å¯åŠ¨æ—¶è®¾ç½®äº†å‚æ•°`--tp 8`ï¼Œä½†å®é™…ä¸Šæˆ‘åªæœ‰ä¸¤å¼ å¡ï¼Œæ‰€ä»¥æ”¹ä¸º2å³å¯



#### 2ã€benchmark

æœåŠ¡å™¨å¯åŠ¨å¸¸è§å‚æ•°ï¼šhttps://docs.sglang.ai/backend/server_arguments.html

åŸºå‡†æµ‹è¯•ç›¸å…³ï¼šhttps://docs.sglang.ai/references/benchmark_and_profiling.html



æµ‹è¯•API Serverï¼š

```
usage: bench_serving.py [-h] [--backend {sglang,sglang-native,sglang-oai,vllm,lmdeploy,trt,gserver,truss}] [--base-url BASE_URL] [--host HOST] [--port PORT]
                        [--dataset-name {sharegpt,random,generated-shared-prefix}] [--dataset-path DATASET_PATH] [--model MODEL] [--tokenizer TOKENIZER] [--num-prompts NUM_PROMPTS]
                        [--sharegpt-output-len SHAREGPT_OUTPUT_LEN] [--sharegpt-context-len SHAREGPT_CONTEXT_LEN] [--random-input-len RANDOM_INPUT_LEN] [--random-output-len RANDOM_OUTPUT_LEN]
                        [--random-range-ratio RANDOM_RANGE_RATIO] [--request-rate REQUEST_RATE] [--max-concurrency MAX_CONCURRENCY] [--output-file OUTPUT_FILE] [--disable-tqdm] [--disable-stream]
                        [--return-logprob] [--seed SEED] [--disable-ignore-eos] [--extra-request-body {"key1": "value1", "key2": "value2"}] [--apply-chat-template] [--profile] [--lora-name LORA_NAME]
                        [--prompt-suffix PROMPT_SUFFIX] [--pd-seperated] [--gsp-num-groups GSP_NUM_GROUPS] [--gsp-prompts-per-group GSP_PROMPTS_PER_GROUP] [--gsp-system-prompt-len GSP_SYSTEM_PROMPT_LEN]
                        [--gsp-question-len GSP_QUESTION_LEN] [--gsp-output-len GSP_OUTPUT_LEN]
```

```
python -m sglang.launch_server --model-path ./Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --load-format dummy --tp 2 --disable-radix
 
python3 -m sglang.bench_serving --backend sglang --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompt 1000 --request-rate 10 --host 127.0.0.1 --port 30000
```

æµ‹è¯•ç»“æœï¼šä¸¤å¼ A6000åˆ©ç”¨ç‡å’Œæ˜¾å­˜å·²æ‹‰æ»¡

```
============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    10.0      
Max reqeuest concurrency:                not set   
Successful requests:                     1000      
Benchmark duration (s):                  619.25    
Total input tokens:                      302118    
Total generated tokens:                  195775    
Total generated tokens (retokenized):    195785    
Request throughput (req/s):              1.61      
Input token throughput (tok/s):          487.88    
Output token throughput (tok/s):         316.15    
Total token throughput (tok/s):          804.03    
Concurrency:                             492.06    
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   304707.80 
Median E2E Latency (ms):                 308827.73 
---------------Time to First Token----------------
Mean TTFT (ms):                          172371.83 
Median TTFT (ms):                        145852.25 
P99 TTFT (ms):                           396281.66 
---------------Inter-Token Latency----------------
Mean ITL (ms):                           679.43    
Median ITL (ms):                         394.89    
P95 ITL (ms):                            1673.45   
P99 ITL (ms):                            2490.47   
Max ITL (ms):                            103144.22 
==================================================
```



ç¦»çº¿åˆ†æï¼š

```
# æŸ¥çœ‹å‚æ•°
python -m sglang.bench_offline_throughput -h
```

```
python -m sglang.bench_offline_throughput --model-path ./Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompt 1000 --tensor-parallel-size 2
```

æµ‹è¯•ç»“æœï¼š

```
====== Offline Throughput Benchmark Result =======
Backend:                                 engine    
Successful requests:                     1000      
Benchmark duration (s):                  622.93    
Total input tokens:                      302118    
Total generated tokens:                  195775    
Last generation throughput (tok/s):      27.27     
Request throughput (req/s):              1.61      
Input token throughput (tok/s):          484.99    
Output token throughput (tok/s):         314.28    
Total token throughput (tok/s):          799.27    
==================================================
```



 

### äºŒã€LMDeploy

#### 1ã€ç¯å¢ƒé…ç½®

å®˜æ–¹Githubï¼šhttps://github.com/InternLM/lmdeploy/blob/main/README_zh-CN.md

å®˜æ–¹æ•™ç¨‹ï¼šhttps://lmdeploy.readthedocs.io/zh-cn/latest/get_started/installation.html



å®‰è£…ï¼š

```
pip install lmdeploy
```



#### 2ã€benchmark

å®˜æ–¹æ•™ç¨‹ï¼šhttps://lmdeploy.readthedocs.io/zh-cn/latest/benchmark/benchmark.html

```
git clone https://github.com/InternLM/lmdeploy.git
```



æµ‹è¯•API Serverï¼š

```
lmdeploy serve api_server ./Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --server-port 23333 --tp 2
```

```
python3 lmdeploy/benchmark/profile_restful_api.py --backend lmdeploy  --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompt 1000 --request-rate 10
```

æµ‹è¯•ç»“æœï¼š

```
============ Serving Benchmark Result ============
Backend:                                 lmdeploy  
Traffic request rate:                    10.0      
Successful requests:                     1000      
Benchmark duration (s):                  479.52    
Total input tokens:                      224530    
Total generated tokens:                  193670    
Total generated tokens (retokenized):    194626    
Request throughput (req/s):              2.09      
Input token throughput (tok/s):          468.24    
Output token throughput (tok/s):         403.89    
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   192391.71 
Median E2E Latency (ms):                 189996.24 
---------------Time to First Token----------------
Mean TTFT (ms):                          139146.52 
Median TTFT (ms):                        135900.35 
P99 TTFT (ms):                           308954.79 
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          346.50    
Median TPOT (ms):                        299.05    
P99 TPOT (ms):                           1593.03   
---------------Inter-token Latency----------------
Mean ITL (ms):                           275.07    
Median ITL (ms):                         166.47    
P99 ITL (ms):                            1081.47   
==================================================
```





æµ‹è¯•æ¨ç†å¼•æ“æ¥å£ï¼š

```
python3 lmdeploy/benchmark/profile_throughput.py --model-path ./Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompt 1000 --tensor-parallel-size 2
```

æµ‹è¯•ç»“æœï¼šæŠ¥é”™ï¼Œå¤§æ¦‚ç‡å› ä¸ºgithubä¸Šçš„æµ‹è¯•ç¨‹åºä¸å½“å‰ä¸‹è½½çš„lmdeployç‰ˆæœ¬ï¼ˆpipä¸‹è½½çš„ï¼Œè€Œéæºç ç¼–è¯‘çš„ï¼‰ä¸å¯¹åº”



# å…¶ä»–æµ‹è¯•



ç½‘ä¸Šä¸€äº›å¤§ä½¬çš„æµ‹è¯•ï¼šé‡ç‚¹å…³æ³¨å…¶æµ‹è¯•æŒ‡æ ‡

![image-20250409100637504](./assets/image-20250409100637504.png)



