## Summery

æ¶‰åŠåˆ°æ¨¡å‹éƒ¨ç½²ï¼š

- GPUæ–¹æ¡ˆï¼švLLMã€SGLang
- CPUæ–¹æ¡ˆï¼šllama.cpp

- ç‰¹æ®Šæ–¹æ¡ˆï¼šKtransformer

è¯¦ç»†å†…å®¹ï¼Œå¯å‚è€ƒæœ¬ç›®å½•ç›¸å…³ç¬”è®°



## Others

### é›¶é›¶ç¢ç¢

> å‚è€ƒï¼š[å¤§æ¨¡å‹æ¨ç†å¿…çœ‹ï¼2025æœ€å€¼å¾—è¯»çš„14ç¯‡è®ºæ–‡å’Œ2ç¯‡åšå®¢](https://mp.weixin.qq.com/s/dg8FGNZ0mZOxF4b5fwUxkw)

TODO



> å‚è€ƒï¼š[LLMæ¨ç†æ¡†æ¶å­¦ä¹ ç¬”è®°](https://mp.weixin.qq.com/s/fJ8ElrDfaC7ekqcNTd9paA)

ä¸€äº›åŸºç¡€æ¦‚å¿µ + å‡ ç§å¹¶è¡Œ/åˆ†å¸ƒå¼ç­–ç•¥

- **æ¦‚å¿µ**
  - è¿ç»­æ‰¹å¤„ç†
  - é¡µå¼æ³¨æ„åŠ›
  - é‡åŒ–
  - ç¡¬ä»¶ä¼˜åŒ–
- **åˆ†å¸ƒå¼æ¨ç†ç­–ç•¥**
  - å¼ é‡å¹¶è¡Œ(Tensor Parallelism)ï¼šæŠŠæ¨¡å‹çš„æƒé‡ï¼ˆå¦‚çº¿æ€§å±‚çš„çŸ©é˜µï¼‰æ²¿æŸä¸ªç»´åº¦ï¼ˆè¡Œæˆ–åˆ—ï¼‰åˆ‡åˆ†ï¼Œåˆ†é…åˆ°ä¸åŒçš„GPUä¸Š
  - æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)ï¼šå°†å¤§æ¨¡å‹çš„å¤šå±‚ç»“æ„æŒ‰å±‚åˆ‡åˆ†ï¼Œåˆ†é…åˆ°ä¸åŒçš„è®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ä¸Šã€‚æ¯ä¸ªè®¾å¤‡åªè´Ÿè´£æ¨¡å‹çš„ä¸€éƒ¨åˆ†å±‚ã€‚æ•°æ®ï¼ˆå¦‚è¾“å…¥åºåˆ—ï¼‰åƒâ€œæµæ°´çº¿â€ä¸€æ ·ä¾æ¬¡æµç»è¿™äº›è®¾å¤‡ï¼Œé€æ®µå®Œæˆè®¡ç®—ã€‚
  - æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰ï¼šåœ¨å¤šå¼ æ˜¾å¡ä¸Šéƒ¨ç½²å¤šä¸ªä¸€æ¨¡ä¸€æ ·çš„æ¨¡å‹ï¼Œå½“ä¸€æ¬¡æ€§è¦å¤„ç†å¤šä¸ªç”¨æˆ·è¯·æ±‚æ—¶ï¼Œå¯ä»¥å°†è¿™äº›è¯·æ±‚å¹³å‡åˆ†é…ç»™ä¸åŒçš„GPUå¤„ç†ï¼Œæœ€åå†æŠŠä¸åŒGPUå¤„ç†å¾—åˆ°çš„ç»“æœåˆå¹¶åè¾“å‡º
  - å°†ä¸Šè¿°çš„å‡ ç§ç­–ç•¥è¿›è¡Œç»„åˆ
- **ä¸»è¦çš„é‡åŒ–æŠ€æœ¯**
  - GPTQ
  - AWQ
  - GGUF/GGML



> å‚è€ƒï¼š[LLM æ¨ç†å¼•æ“é€‰å‹æŒ‡å—ï¼šTransformersã€llama.cpp ä¸ vLLM è¯¥æ€ä¹ˆé€‰ï¼Ÿ](https://mp.weixin.qq.com/s/ODIJ2IApGluAhC-76UdEow)

æ¦‚è¿°äº† Transformersã€llama.cpp ä¸ vLLM ï¼Œç®€è¦è¡¥å……äº›æ¦‚å¿µåº”å¯¹é¢è¯•ï¼š

- Transformersï¼šè§£é‡Šå‹è¯­è¨€èˆ¬çš„é€šç”¨åŸºå‡†
  - æ ¸å¿ƒæœºåˆ¶ï¼šEager Execution (åŠ¨æ€å›¾)
  - å†…å­˜æ¨¡å‹ï¼šè¿ç»­åˆ†é…çš„ç—›ç‚¹ï¼ˆå…¶æ€§èƒ½ç“¶é¢ˆå¾€å¾€ä¸åœ¨äºâ€œå†…å­˜å¯¹é½â€ï¼Œè€Œåœ¨äº KV Cache çš„è¿ç»­å†…å­˜åˆ†é…ç­–ç•¥ï¼‰
  - é€‚ç”¨åœºæ™¯ï¼šä»£ç éªŒè¯ä¸åŸå‹å¼€
- llama.cppï¼šåµŒå…¥å¼æ€ç»´ä¸‹çš„â€œè£¸æœºâ€ä¼˜åŒ–
  - æ ¸å¿ƒæŠ€æœ¯ï¼šé‡åŒ– (GGUF) ä¸ å†…å­˜å¢™çªç ´
  - è®¡ç®—ä¼˜åŒ–ï¼šå¼‚æ„åŠ é€Ÿä¸ SIMD
  - é€‚ç”¨åœºæ™¯ï¼šç«¯ä¾§éƒ¨ç½²
- vLLMï¼šå¼•å…¥æ“ä½œç³»ç»Ÿçš„â€œåˆ†é¡µå†…å­˜ç®¡ç†â€
  - æ¶æ„åˆ›æ–°ï¼šPagedAttention
  - æ€§èƒ½è¡¨ç°ï¼šContinuous Batching
  - é€‚ç”¨åœºæ™¯ï¼šç”Ÿäº§çº§é«˜åå API
- å…¶ä»–ï¼š
  - ç®—å­ç¼–ç¨‹ä¸ä¸­é—´ä»¶ Triton (è¯­è¨€/ç¼–è¯‘å™¨)ï¼šç”± OpenAI å¼€å‘ï¼Œå®ƒä¸æ˜¯ vLLM çš„é™„å±ï¼Œè€Œæ˜¯ä¸€ç§ç±»ä¼¼ Python çš„ GPU ç¼–ç¨‹è¯­è¨€
  - å¤æ‚è°ƒåº¦ä¸ Agent ä¼˜åŒ– SGLang (ç»“æ„åŒ–ä¸ç¼“å­˜)ï¼š
    - Radix Attentionï¼šåŸºäºå‰ç¼€æ ‘ï¼ˆRadix Treeï¼‰ç®¡ç† KV Cacheï¼Œå®ç°äº†è·¨è¯·æ±‚çš„ Prompt ç¼“å­˜ï¼ˆè‡ªåŠ¨è¯†åˆ«å¹¶ç¼“å­˜å¤šè½®å¯¹è¯æˆ– Agent ä»»åŠ¡ä¸­çš„å…¬å…±å‰ç¼€ï¼‰
    - ç»“æ„åŒ–ç”Ÿæˆï¼šåŸç”Ÿæ”¯æŒå¼ºåˆ¶æ¨¡å‹è¾“å‡ºç¬¦åˆ JSON Schema æˆ– Regex æ ¼å¼ï¼Œéå¸¸é€‚åˆå·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰åœºæ™¯
  - å¼‚æ„ä¸å›½äº§åŒ–
    - KTransformers (å¼‚æ„å¸è½½)ï¼šå°†æ¨¡å‹çš„å†·æ•°æ®ï¼ˆå¦‚éƒ¨åˆ†æƒé‡ï¼‰Swap åˆ° CPU å†…å­˜ï¼Œçƒ­æ•°æ®ç•™åœ¨ GPU
    - åä¸º MindIE (ç¡¬ä»¶æŠ½è±¡)ï¼šåä¸ºæ˜‡è…¾ï¼ˆAscendï¼‰ç¡¬ä»¶çš„ä¸“ç”¨è¿è¡Œæ—¶ã€‚å®ƒåº•å±‚å¯¹æ¥ CANNï¼ˆå¯¹æ ‡ CUDAï¼‰ï¼Œé’ˆå¯¹ NPU çš„ Cube Unit è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–



### èŠ‚ç‚¹çº§ multi-session ç®¡ç† & è°ƒåº¦

> ï¼ˆâ€œæ¨¡å‹è½åœ°â€ç›¸å…³æ‹›è˜è¦æ±‚ï¼‰å…³äº **èŠ‚ç‚¹çº§ multi-session ç®¡ç† & è°ƒåº¦** è®¾è®¡ï¼Œä»¥ä¸‹ä¸ºå¤§æ¨¡å‹ç»™å‡ºçš„æ–¹æ¡ˆï¼Œä¾›å‚è€ƒ

ç›®æ ‡æ‘˜è¦ï¼š

- æ”¯æŒâ€œä¸€ä¸ªèŠ‚ç‚¹ä¸ŠåŒæ—¶æ‰˜ç®¡å¤šä¸ª sessionï¼ˆmulti-sessionï¼‰â€å¹¶å‘äº¤äº’
- æ”¯æŒæ¨¡å‹å¤ç”¨ã€å¾®æ‰¹ï¼ˆmicro-batchingï¼‰ã€ä¼˜å…ˆçº§ã€å…¬å¹³è°ƒåº¦ã€é¢„å–/çƒ­å¯åŠ¨ä¸ä¼šè¯éš”ç¦»
- ä¸ vLLMï¼ˆé«˜æ•ˆæ‰¹æ¨ç†ï¼‰åŠ LangChainï¼ˆä»»åŠ¡/chain ç¼–æ’ï¼‰å…¼å®¹
- èŠ‚ç‚¹çº§æœ¬åœ°è°ƒåº¦ + å…¨å±€è°ƒåº¦ï¼ˆé›†ç¾¤çº§ï¼‰ååŒ

------

#### 1ã€æ€»ä½“æ¶æ„ï¼ˆé«˜å±‚ï¼‰

1. **Ingress / API ç½‘å…³**ï¼ˆHTTP/gRPC/WSï¼‰
   - æ¥æ”¶ç”¨æˆ·è¯·æ±‚ï¼ˆä¼šè¯åˆ›å»ºã€æ¨ç†è¯·æ±‚ã€æµå¼è¾“å‡ºï¼‰
   - èº«ä»½è®¤è¯ã€é€Ÿç‡é™åˆ¶ã€è·¯ç”±åˆ° Global Scheduler
2. **Global Schedulerï¼ˆå…¨å±€ï¼‰**
   - ç»´æŠ¤é›†ç¾¤èµ„æºè§†å›¾ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ï¼šGPU/CPU/å†…å­˜/å·²åŠ è½½æ¨¡å‹/ä¼šè¯æ•°/é˜Ÿåˆ—é•¿åº¦ï¼‰
   - è´Ÿè´£ä¼šè¯æ”¾ç½®ï¼ˆplacementï¼‰ã€è´Ÿè½½å‡è¡¡ã€è·¨èŠ‚ç‚¹è¿ç§»å†³ç­–
   - æä¾›ç­–ç•¥ï¼šæœ€å°‘è´Ÿè½½ã€bin-packingã€ä¼˜å…ˆçº§/éš”ç¦»/ç§Ÿæˆ·ç­–ç•¥
3. **Node Agentï¼ˆæ¯èŠ‚ç‚¹ï¼‰**
   - **Session Managerï¼ˆèŠ‚ç‚¹çº§ï¼‰**ï¼šç®¡ç†æœ¬èŠ‚ç‚¹ä¸Šçš„ N ä¸ª sessionï¼ˆmulti-sessionï¼‰
   - **Model Pool / Loader**ï¼šè´Ÿè´£æ¨¡å‹åŠ è½½ã€å¸è½½ã€å…±äº«ï¼ˆå•å‰¯æœ¬å¤šä¼šè¯ï¼‰ã€æ¨¡å‹ç¼“å­˜
   - **vLLM Runtime Adapter**ï¼šä¸ vLLM çš„ batch API é›†æˆï¼ˆæ”¶é›† micro-batchesï¼‰
   - **Execution Worker Pool**ï¼šå¤„ç† CPU å‰å¤„ç†ã€åå¤„ç†ä¸ä¸æ¨¡å‹çš„å¼‚æ­¥äº¤äº’
   - **Local Scheduler**ï¼šæ¥æ”¶ Global Scheduler çš„æ”¾ç½®å‘½ä»¤æˆ–ç›´æ¥ä» ingress è·å–è¯·æ±‚ï¼ˆé¢„ç•™ï¼‰
   - **Metrics & Health**ï¼šä¸ŠæŠ¥ Prometheus / OTLP
4. **State Store / Metadata**ï¼ˆetcd / Redis / Postgresï¼‰
   - ä¼šè¯å…ƒä¿¡æ¯ã€æ¨¡å‹å…ƒæ•°æ®ã€å†·å¯åŠ¨æ§½ä½ã€è¿ç§»çŠ¶æ€
   - æ”¯æŒå¿«é€Ÿè¯»å–çš„ä¼šè¯è·¯ç”±è¡¨ï¼ˆgateway -> scheduler -> nodeï¼‰
5. **Control Plane / Operatorï¼ˆå¯é€‰ï¼ŒK8s operatorï¼‰**
   - ç®¡ç†èŠ‚ç‚¹æ‰©ç¼©ã€æ¨¡å‹é•œåƒã€ä½œä¸šç”Ÿå‘½å‘¨æœŸ
6. **Observability & Tracing**
   - Prometheus + Grafana + Jaeger
   - å…³é”®æŒ‡æ ‡ï¼šlatency p50/p95/p99ã€GPU utilã€batch sizeã€ctx reuse ratioã€queue length

------

#### 2ã€èŠ‚ç‚¹çº§ multi-session ç®¡ç†ï¼ˆè¯¦ç»†ï¼‰

**1ï¼‰ä¼šè¯ç”Ÿå‘½å‘¨æœŸ**

- CreateSession(user, model, config)ï¼š
  - Check quota/tenant limits â†’ allocate session_id â†’ decide placement via Global Scheduler
  - Node Agent åœ¨ Model Pool ä¸­ç¡®ä¿ç›®æ ‡æ¨¡å‹å·²åŠ è½½ï¼ˆæˆ–è§¦å‘å¼‚æ­¥åŠ è½½ï¼‰ï¼Œä¸º session åˆ†é… `SessionContext`ï¼ˆåŒ…æ‹¬ max_tokensã€priorityã€warmup_flagsï¼‰
- HandleRequest(session_id, request)ï¼š
  - Local Session Manager æ ¹æ® request ç±»å‹ï¼ˆsync/streamï¼‰å…¥æœ¬åœ°é˜Ÿåˆ—
  - Local Scheduler å†³å®šæ”¾å…¥ micro-batching é›†åˆæˆ–ç«‹å³å¤„ç†ï¼ˆä½å»¶è¿Ÿä¼˜å…ˆï¼‰
- CloseSession(session_id)ï¼š
  - æ¸…ç† contextï¼ˆå¯é€‰æŒä¹…åŒ–çŸ­æœŸå†å²ï¼‰ï¼Œå‡å°‘ä¼šè¯è®¡æ•°
  - è‹¥æ¨¡å‹å†·å´ç­–ç•¥å‘½ä¸­ï¼Œé‡Šæ”¾æ¨¡å‹èµ„æºæˆ–é™çº§ warm-pool
- MigrateSession(session_id, dest_node)ï¼ˆå¯é€‰ï¼‰ï¼š
  - Serialize minimal session stateï¼ˆcontext pointers or embeddingsï¼‰åˆ° State Store
  - åœ¨ç›®æ ‡èŠ‚ç‚¹æ¢å¤ä¼šè¯å¹¶åˆ‡æ¢è·¯ç”±è¡¨ï¼›æºèŠ‚ç‚¹åœæ­¢æ¥æ”¶æ–°è¯·æ±‚ï¼Œå®Œæˆå°¾éƒ¨è¯·æ±‚åç§»é™¤

**2ï¼‰æ•°æ®ç»“æ„ï¼ˆPython é£æ ¼ç¤ºä¾‹ï¼‰**

```python
class SessionContext:
    session_id: str
    model: str
    user_id: str
    priority: int
    max_tokens: int
    token_history_ptr: Optional[str]  # pointer to external cache if large
    state: dict  # ephemeral metadata, e.g., running streaming cursor
    node_id: str  # current placement

class NodeResource:
    node_id: str
    gpus: List[GPUInfo]
    cpu_cores: int
    gpu_free_mem: Dict[int, int]
    models_loaded: Set[str]
    session_count: int
    queue_len: int
```

**3ï¼‰æœ¬åœ°é˜Ÿåˆ—ä¸å¾®æ‰¹ï¼ˆmicro-batchingï¼‰**

- Local Session Manager ç»´æŠ¤å¤šä¸ªä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆp0..pnï¼‰ï¼Œä»¥åŠä¸€ä¸ª `Batcher` è¿›ç¨‹ï¼š
  - æ¯ä¸ª tickï¼ˆä¾‹å¦‚ 1â€“10msï¼‰æ”¶é›†åŒ model çš„è¯·æ±‚ï¼ŒæŒ‰ tokens/latency/priority æ‰“åŒ…æˆ batchï¼Œäº¤ç»™ vLLMã€‚
  - batch size ä¸Šé™ä¾æ® GPU memory åŠ¨æ€è®¡ç®—ï¼ˆæˆ– vLLM æä¾›çš„ APIï¼‰ã€‚
- å¯¹ä½å»¶è¿Ÿè¯·æ±‚ï¼Œæ”¯æŒ `eager_cutoff`ï¼ˆè‹¥ç­‰å¾…è¶…è¿‡ L ms åˆ™ç«‹å³æ½å…¥å½“å‰ batchï¼‰ã€‚
- å¯¹æµå¼è¯·æ±‚ï¼Œæ‹†åˆ†ä¸ºå° batch æˆ–å•ç‹¬ stream è·¯å¾„ï¼ˆä¿è¯ä½å»¶è¿Ÿï¼‰ã€‚

**4ï¼‰vLLM é›†æˆç‚¹ï¼ˆå®è·µè¦ç‚¹ï¼‰**

- ä½¿ç”¨ vLLM çš„æ‰¹å¤„ç†æ¥å£ï¼ˆe.g., `model.generate_batch(prompts)`ï¼‰ï¼Œå¹¶é…åˆå…¶ token-level streamingï¼ˆè‹¥æ”¯æŒï¼‰ã€‚
- vLLM é€šå¸¸ä¼šç®¡ç† GPU å†…å­˜æ± ã€å¹¶æä¾›é«˜ååå¾®æ‰¹ç­–ç•¥ï¼šæŠŠ Local Batcher çš„å¾…å¤„ç† batch å˜ä¸º vLLM çš„ batchã€‚
- é‡è¦ï¼švLLM å¯¹æ˜¾å­˜è¦æ±‚é«˜ï¼Œå»ºè®®ï¼š
  - å…±äº«ä¸€ä»½ model å‰¯æœ¬ç»™å¤šä¸ª sessionï¼ˆé¿å…æ¯ session éƒ½åŠ è½½ï¼‰
  - ä½¿ç”¨ vLLM çš„ quantization / mixed precision æ”¯æŒï¼ˆå¦‚ 4/8-bitï¼‰
  - ä¸ºä¸åŒä¼˜å…ˆçº§è®¾å®šæ˜¾å­˜é…é¢æˆ– QoSï¼ˆé¿å…ä½ä¼˜å…ˆçº§æŠ¢å ï¼‰

------

#### 3ã€è°ƒåº¦ç­–ç•¥ï¼ˆå…¨çƒ + èŠ‚ç‚¹çº§ï¼‰

1ï¼‰å…¨å±€ï¼ˆGlobal Schedulerï¼‰

- **è¾“å…¥**ï¼šä¼šè¯åˆ›å»ºè¯·æ±‚ï¼ˆmodel, priority, tenantï¼‰, cluster resource snapshot

- **ç›®æ ‡å‡½æ•°**ï¼ˆç¤ºä¾‹ï¼‰ï¼š

  ```python
  score(node) = w1 * free_gpu_mem_ratio + w2 * (1 - queue_len_norm)
              + w3 * model_loaded(node, model) + w4 * session_affinity_bonus
  choose node with max(score)
  ```

- æ”¯æŒç­–ç•¥åˆ‡æ¢ï¼šä½å»¶è¿Ÿä¼˜å…ˆ / æˆæœ¬æœ€ä¼˜ / éš”ç¦»æ¨¡å¼

- æ”¯æŒæ¨¡å‹åˆ†å±‚ï¼ˆhot/warm/coldï¼‰ï¼š

  - Hot: å¸¸é©» GPUï¼Œåœ¨å¤šä¸ªèŠ‚ç‚¹ä¿ç•™å‰¯æœ¬
  - Warm: å¯å¿«é€ŸåŠ è½½ï¼ˆç¼“å­˜ï¼‰
  - Cold: S3 å­˜å‚¨ï¼ŒæŒ‰éœ€åŠ è½½

2ï¼‰èŠ‚ç‚¹çº§ï¼ˆLocal Schedulerï¼‰

- è´Ÿè´£å¾®æ‰¹è°ƒåº¦ã€ä¼˜å…ˆçº§é˜Ÿåˆ—ã€é€Ÿç‡é™åˆ¶ã€token å¸¦å®½åˆ†é…
- å¯¹åŒä¸€ model çš„ requests ä¼˜å…ˆæ‰“åŒ…ï¼›å¯¹è·¨æ¨¡å‹åˆ™éš”ç¦» batch

3ï¼‰å…¬å¹³ä¸ä¼˜å…ˆçº§

- ä½¿ç”¨å¤šçº§åé¦ˆé˜Ÿåˆ—ï¼ˆMLFQï¼‰æˆ–ä»¤ç‰Œæ¡¶æ¥ä¿è¯å…¬å¹³ä¸QoS
- é«˜ä¼˜å…ˆçº§è¯·æ±‚ï¼ˆå¦‚å¯¹è¯äº¤äº’ï¼‰è®¾ç½®çŸ­ç­‰å¾…ä¸Šé™å¹¶å¯ preemptï¼ˆä¸­æ–­ä½ä¼˜å…ˆçº§ batchï¼‰
- ä½ä¼˜å…ˆçº§æ‰¹å¤„ç†èµ° background queueï¼ˆå°½é‡å¤§æ‰¹ä»¥æé«˜ååï¼‰



ç¤ºä¾‹ä»£ç ç»“æ„ï¼šæ•´ä¸ªå®ç°ä¸º **å¯è¿è¡Œçš„ Python/FastAPI + Redis** ç‰ˆæœ¬

```
global_scheduler/
  â”œâ”€â”€ scheduler.py        # GlobalScheduler ä¸»ç±»
  â”œâ”€â”€ models.py           # æ•°æ®ç»“æ„ï¼šNodeInfo, SessionMetadata
  â”œâ”€â”€ storage.py          # Redis åç«¯ - å…ƒä¿¡æ¯å­˜å‚¨
  â”œâ”€â”€ api.py              # FastAPI æ¥å£å…¥å£ï¼ˆ/v1/sessions/createï¼‰
  â””â”€â”€ strategy/
        â””â”€â”€ placement.py  # èŠ‚ç‚¹é€‰æ‹©ç­–ç•¥
```



ğŸ“Œ **1. `models.py` â€” æ•°æ®æ¨¡å‹å®šä¹‰**

```
from pydantic import BaseModel
from typing import Dict, Optional, List


class NodeInfo(BaseModel):
    node_id: str
    address: str
    gpu_free_mem: int
    gpu_total_mem: int
    running_sessions: int
    loaded_models: List[str]
    queue_len: int
    last_heartbeat_ts: float


class SessionMetadata(BaseModel):
    session_id: str
    model: str
    user_id: Optional[str] = None
    node_id: Optional[str] = None
    priority: int = 5
    created_ts: float = 0.0
```



ğŸ“Œ **2. `storage.py` â€” Redis å­˜å‚¨å±‚**

```
import time
import json
import redis
from typing import Dict, List, Optional
from models import NodeInfo, SessionMetadata


class RedisStore:
    def __init__(self, url="redis://localhost:6379/0"):
        self.r = redis.from_url(url)

    ############################################################
    # Node Info
    ############################################################
    def get_all_nodes(self) -> List[NodeInfo]:
        keys = self.r.keys("node:*")
        result = []
        for k in keys:
            data = json.loads(self.r.get(k))
            result.append(NodeInfo(**data))
        return result

    def save_node(self, node: NodeInfo):
        self.r.set(f"node:{node.node_id}", node.json())

    ############################################################
    # Session Info
    ############################################################
    def save_session(self, session: SessionMetadata):
        self.r.set(f"session:{session.session_id}", session.json())

    def get_session(self, session_id: str) -> Optional[SessionMetadata]:
        data = self.r.get(f"session:{session_id}")
        return SessionMetadata(**json.loads(data)) if data else None

    def update_session_node(self, session_id: str, node_id: str):
        s = self.get_session(session_id)
        if not s:
            return
        s.node_id = node_id
        self.save_session(s)
```



ğŸ“Œ **3. `strategy/placement.py` â€” èŠ‚ç‚¹è°ƒåº¦ç­–ç•¥**

> æ ¸å¿ƒï¼š
>
> - ä¼˜å…ˆé€‰ **å·²åŠ è½½ç›®æ ‡æ¨¡å‹** çš„èŠ‚ç‚¹ï¼ˆå‡å°‘å†·å¯åŠ¨ï¼‰
> - session affinityï¼ˆå¦‚æœ session å·²ç»‘å®šèŠ‚ç‚¹ï¼‰
> - èŠ‚ç‚¹è´Ÿè½½ï¼ˆqueue_len / running_sessionsï¼‰
> - GPU å‰©ä½™æ˜¾å­˜

```
from typing import List
from models import NodeInfo, SessionMetadata


def score_node(node: NodeInfo, session: SessionMetadata) -> float:
    """èŠ‚ç‚¹æ‰“åˆ†å‡½æ•°ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•"""

    # æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    model_loaded_bonus = 1.0 if session.model in node.loaded_models else 0.0

    # è´Ÿè½½æŒ‡æ ‡ï¼ˆä»»åŠ¡è¶Šå°‘è¶Šå¥½ï¼‰
    load_factor = (node.queue_len + node.running_sessions) + 1

    # GPU ç©ºé—²æ¯”ä¾‹
    gpu_free_ratio = node.gpu_free_mem / max(node.gpu_total_mem, 1)

    return (
        3.0 * model_loaded_bonus +
        2.0 * gpu_free_ratio +
        1.0 / load_factor
    )


def choose_best_node(nodes: List[NodeInfo], session: SessionMetadata) -> NodeInfo:
    """æ ¹æ®è¯„åˆ†æ‰¾åˆ°æœ€ä¼˜èŠ‚ç‚¹"""
    ranked = sorted(nodes, key=lambda n: score_node(n, session), reverse=True)
    return ranked[0]
```



ğŸ“Œ **4. `scheduler.py` â€” Global Scheduler ä¸»é€»è¾‘**

```
import time
import uuid
from typing import Optional
from storage import RedisStore
from models import SessionMetadata, NodeInfo
from strategy.placement import choose_best_node


class GlobalScheduler:

    def __init__(self, store: RedisStore):
        self.store = store

    ####################################################################
    # Create a new session and find node placement
    ####################################################################
    def create_session(
        self,
        model: str,
        user_id: str,
        priority: int = 5,
        session_id: Optional[str] = None
    ) -> SessionMetadata:

        session_id = session_id or str(uuid.uuid4())
        session = SessionMetadata(
            session_id=session_id,
            model=model,
            user_id=user_id,
            priority=priority,
            created_ts=time.time(),
        )

        nodes = self.store.get_all_nodes()
        if not nodes:
            raise RuntimeError("No available nodes for scheduling")

        # æŒ‰ç­–ç•¥æ‰¾èŠ‚ç‚¹
        best_node = choose_best_node(nodes, session)

        # è®°å½• session â†’ node æ˜ å°„
        session.node_id = best_node.node_id
        self.store.save_session(session)

        return session

    ####################################################################
    # æŸ¥è¯¢å·²æœ‰ session åº”æ”¾ç½®çš„èŠ‚ç‚¹ï¼ˆç”¨äºè·¯ç”±ï¼‰
    ####################################################################
    def route_session(self, session_id: str) -> str:
        session = self.store.get_session(session_id)
        if not session:
            raise ValueError("Session not found")
        if not session.node_id:
            raise ValueError("Session is not assigned yet")
        return session.node_id
```



ğŸ“Œ **5. `api.py` â€” FastAPI å¯¹å¤–æ¥å£**

ä½ å¯ä»¥å°†è¯¥ API éƒ¨ç½²ä¸ºâ€œå…¨å±€è°ƒåº¦æœåŠ¡â€ã€‚

```
import uvicorn
from fastapi import FastAPI, HTTPException
from storage import RedisStore
from scheduler import GlobalScheduler

app = FastAPI()
store = RedisStore()
scheduler = GlobalScheduler(store)


class CreateSessionReq(BaseModel):
    user_id: str
    model: str
    priority: int = 5


@app.post("/v1/sessions/create")
def create_session(req: CreateSessionReq):
    try:
        sess = scheduler.create_session(
            model=req.model,
            user_id=req.user_id,
            priority=req.priority,
        )
        return {
            "session_id": sess.session_id,
            "node_id": sess.node_id,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/sessions/{session_id}/route")
def route_session(session_id: str):
    try:
        node = scheduler.route_session(session_id)
        return {"node_id": node}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=9000)
```



ğŸ“Œ 6. Global Scheduler çš„å…³é”®ç‰¹æ€§ï¼ˆå·²åŒ…å«ï¼‰

- âœ” **session â†’ node æ˜ å°„ç¼“å­˜**
- âœ” **æ¨¡å‹çƒ­åº¦æ„ŸçŸ¥ï¼ˆå·²åŠ è½½æ¨¡å‹ä¼˜å…ˆï¼‰**
- âœ” **è´Ÿè½½å‡è¡¡ï¼ˆqueue_len / session_countï¼‰**
- âœ” **GPU æ˜¾å­˜æƒé‡**
- âœ” **session affinity**
- âœ” **å¯æ‰©å±•æ‰“åˆ†ç­–ç•¥**
- âœ” **æ”¯æŒ Redis é›†ç¾¤æ¨¡å¼**



ğŸ“Œ 7. å¦‚ä½•ä¸ Node Agent ç»“åˆï¼Ÿ

Node Agent å®šæœŸä¸ŠæŠ¥ï¼š

```
POST /v1/node/heartbeat
{
  "node_id": "...",
  "address": "...",
  "gpu_free_mem": 20000,
  "gpu_total_mem": 40000,
  "queue_len": 3,
  "running_sessions": 12,
  "loaded_models": ["gpt-4o-mini", "llama3-8b"]
}
```

Global Scheduler æ¥æ”¶å¹¶å†™å…¥ Redisï¼ˆstore.save_node(node)ï¼‰ï¼Œå°±å¯ä»¥è®©èŠ‚ç‚¹å‚ä¸èµ„æºæ± ã€‚



ğŸ“Œ 8. å¦‚ä½•ä¸ LangChain Wrapper è¿æ¥ï¼Ÿ

ä½ çš„ LangChain Wrapper åªéœ€è¦é€šè¿‡ï¼šè·å– session æ‰€åœ¨èŠ‚ç‚¹

```
GET /v1/sessions/{session_id}/route
```

ç„¶åå°†æ‰€æœ‰ LLM è¯·æ±‚è·¯ç”±åˆ°è¿”å›çš„ node åœ°å€å³å¯ã€‚

------

#### 4ã€ä¼šè¯éš”ç¦»ä¸ä¸Šä¸‹æ–‡ç®¡ç†

- ä¸Šä¸‹æ–‡ä¿å­˜åœ¨ **SessionContext**ï¼Œå¯¹é•¿å¯¹è¯é‡‡ç”¨**å¤–éƒ¨æŒä¹…åŒ–ï¼ˆRedis/Vector DBï¼‰**ï¼Œåªåœ¨å†…å­˜ä¸­ä¿ç•™ sliding windowï¼ˆe.g., last 2048 tokensï¼‰
- å¯¹äº memory-heavy åŠŸèƒ½ï¼ˆå¦‚å·¥å…·è°ƒç”¨ã€æ£€ç´¢å¢å¼ºï¼‰ï¼š
  - æŠŠé•¿æœŸè®°å¿†/embeddings å­˜åœ¨ vector DBï¼ˆe.g., Milvus, Pineconeï¼‰ï¼Œè¯·æ±‚æ—¶æ£€ç´¢å¹¶ä½œä¸º prompt ç‰‡æ®µ
- é‡‡ç”¨ **prompt chunking + chunk cache**ï¼Œå¯¹é‡å¤æ£€ç´¢è¿›è¡Œç¼“å­˜

------

#### 5ã€ä¼šè¯è¿ç§»ä¸æ•…éšœæ¢å¤

- **ä¼˜å…ˆæ–¹æ¡ˆ**ï¼šå°½é‡é¿å…é¢‘ç¹è¿ç§»ï¼ˆsession affinityï¼‰ï¼Œåªæœ‰åœ¨èŠ‚ç‚¹è¿‡è½½æˆ–ç»´æŠ¤æ—¶è¿ç§»
- è¿ç§»æ­¥éª¤ï¼š
  1. Global Scheduler ä¸‹è¾¾è¿ç§»å‘½ä»¤
  2. æºèŠ‚ç‚¹å°† session çš„ minimal stateï¼ˆrecent tokensï¼Œbuffersï¼‰å†™å…¥ State Storeï¼ˆå¯å‹ç¼©ï¼‰
  3. ç›®æ ‡èŠ‚ç‚¹ä» State Store è¯»å–å¹¶æ¢å¤ sessionï¼›Global Scheduler æ›´æ–°è·¯ç”±è¡¨
  4. æºèŠ‚ç‚¹åœ¨å®Œæˆ inflight è¯·æ±‚åé‡Šæ”¾èµ„æº
- æ•…éšœæ¢å¤ï¼šèŠ‚ç‚¹çªé™ï¼ˆOOM/æ‰çº¿ï¼‰æ—¶ï¼ŒGlobal Scheduler å°† session æ ‡è®°ä¸º `orphaned` å¹¶è§¦å‘è¿ç§»/é‡å»ºï¼›å¯¹äºæœ‰æŒä¹…åŒ–çš„ sessionï¼Œä¸Šä¸‹æ–‡å¯ä»¥é‡æ–°åŠ è½½

------

#### 6ã€å…¸å‹ API è®¾è®¡ï¼ˆREST/gRPCï¼‰

- `POST /v1/sessions` -> åˆ›å»ºä¼šè¯ï¼ˆè¿”å› session_id, nodeï¼‰
- `POST /v1/sessions/{id}/messages` -> å‘é€è¯·æ±‚ï¼ˆæ”¯æŒ streaming via websocketï¼‰
- `GET /v1/sessions/{id}/state` -> æŸ¥è¯¢çŠ¶æ€
- `DELETE /v1/sessions/{id}` -> å…³é—­
- `GET /health`ï¼Œ`GET /metrics`

ç¤ºä¾‹è¯·æ±‚å­—æ®µï¼š

```python
{
  "model": "gpt-4o-mini",
  "priority": 10,
  "max_tokens": 1024,
  "stream": true,
  "tenant_id": "team-abc"
}
```

------

ä¼ªä»£ç ï¼šNode Session Manager + Batcherï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰

```python
import asyncio
from collections import defaultdict, deque
from time import time

class LocalBatcher:
    def __init__(self, model_name, vllm_adapter, tick_ms=5, max_batch_tokens=8192):
        self.model = model_name
        self.vllm = vllm_adapter
        self.queues = defaultdict(deque)  # priority -> deque of (session, request)
        self.tick_ms = tick_ms / 1000

    async def submit(self, session_ctx, request, priority=5):
        self.queues[priority].append((session_ctx, request))

    async def run(self):
        while True:
            batch = []
            batch_tokens = 0
            start = time()
            # collect from high->low priority
            for p in sorted(self.queues.keys()):
                q = self.queues[p]
                while q and batch_tokens < MAX_TOKENS:
                    session, req = q.popleft()
                    est = estimate_tokens(req)
                    if batch_tokens + est > MAX_TOKENS:
                        # put back for next round
                        q.appendleft((session, req))
                        break
                    batch.append((session, req))
                    batch_tokens += est

            if batch:
                # transform to vLLM batch and call
                prompts = [prepare_prompt(s, r) for s,r in batch]
                outputs = await self.vllm.generate_batch(prompts)
                # dispatch outputs back to sessions
                for (session, req), out in zip(batch, outputs):
                    session.on_response(req, out)
            # sleep until next tick or when small wait triggered
            await asyncio.sleep(self.tick_ms - (time()-start) if time()-start < self.tick_ms else 0)

```

------

#### 7ã€ä¸ LangChain çš„é›†æˆ

- LangChain ä¸»å¯¼ä¸Šå±‚ç¼–æ’ï¼ˆchains, agents, toolsï¼‰ã€‚å°† LLM è°ƒç”¨æ›¿æ¢ä¸ºå‘è°ƒåº¦æœåŠ¡å‘å‡ºçš„ HTTP/gRPC è¯·æ±‚ï¼š
  - å†™ä¸€ä¸ª `LLMWrapper`ï¼ˆLangChain çš„ `BaseLLM`ï¼‰ï¼Œå†…éƒ¨æŠŠè¯·æ±‚å‘é€åˆ°æˆ‘ä»¬çš„è°ƒåº¦ APIï¼Œå¹¶æ”¯æŒæµå¼å“åº”è½¬ callbackã€‚
- å¯¹é•¿é“¾è·¯ï¼ˆtool callsã€retrievalï¼‰ï¼š
  - åœ¨ chain ä¸­æ˜¾å¼å£°æ˜æ˜¯å¦éœ€è¦â€œä½å»¶è¿Ÿæ¨¡å¼â€ï¼ˆinteractiveï¼‰æˆ–â€œbatch modeâ€ï¼ˆbackgroundï¼‰
- LangChain çš„å¹¶è¡ŒåŒ–ï¼ˆmap/asyncï¼‰å¯ä»¥è¢«è°ƒåº¦æœåŠ¡è§†ä¸ºä½ä¼˜å…ˆçº§ batch ä½œä¸šï¼Œäº¤ç”± background queue å¤„ç†

ç¤ºä¾‹ä»£ç ï¼š

```python
from __future__ import annotations
import json
import httpx
from typing import Any, Dict, List, Optional, Iterator

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
    SystemMessage,
    AIMessage,
)
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks.manager import CallbackManagerForLLMRun

##############################################################
#                       Transport Layer
##############################################################

class SchedulerLLMTransport:
    """HTTP client for the Scheduler LLM Service."""

    def __init__(self, base_url: str, api_key: Optional[str] = None, timeout: int = 120):
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.timeout = timeout

        self.client = httpx.Client(
            timeout=timeout,
            headers={"Authorization": f"Bearer {api_key}"} if api_key else {},
        )

    def _messages_to_payload(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """Convert LangChain messages -> è°ƒåº¦æœåŠ¡æ”¯æŒçš„ç»“æ„."""
        result = []
        for m in messages:
            if isinstance(m, HumanMessage):
                result.append({"role": "user", "content": m.content})
            elif isinstance(m, SystemMessage):
                result.append({"role": "system", "content": m.content})
            elif isinstance(m, AIMessage):
                result.append({"role": "assistant", "content": m.content})
            else:
                raise ValueError(f"Unsupported message type: {type(m)}")
        return result

    ##############################################################
    #               Sync / Non-streaming request
    ##############################################################
    def chat(self, session_id: str, messages: List[BaseMessage], **kwargs) -> str:
        payload = {
            "session_id": session_id,
            "messages": self._messages_to_payload(messages),
            "stream": False,
            **kwargs,
        }

        resp = self.client.post(f"{self.base_url}/v1/chat/completions", json=payload)
        resp.raise_for_status()

        data = resp.json()
        return data["output"]

    ##############################################################
    #               Streaming request (server-sent events)
    ##############################################################
    def chat_stream(self, session_id: str, messages: List[BaseMessage], **kwargs) -> Iterator[str]:
        payload = {
            "session_id": session_id,
            "messages": self._messages_to_payload(messages),
            "stream": True,
            **kwargs,
        }

        with self.client.stream(
            "POST", f"{self.base_url}/v1/chat/completions", json=payload
        ) as resp:
            resp.raise_for_status()

            for line in resp.iter_lines():
                if not line:
                    continue
                try:
                    event = json.loads(line)
                except Exception:
                    continue

                if "delta" in event:  # Streaming delta token
                    yield event["delta"]


##############################################################
#                       LangChain Wrapper
##############################################################

class SchedulerChatModel(BaseChatModel):
    """
    LangChain ChatModel wrapper for LLM Scheduler.
    """

    def __init__(
        self,
        base_url: str,
        session_id: str,
        api_key: Optional[str] = None,
        model: str = "gpt-4o-mini",
        streaming: bool = False,
        timeout: int = 120,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.session_id = session_id
        self.streaming = streaming
        self.model = model
        self.transport = SchedulerLLMTransport(base_url, api_key, timeout)

    ##############################################################
    #                LangChain Core Implementation
    ##############################################################

    @property
    def _llm_type(self) -> str:
        return "scheduler_chat_model"

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs
    ) -> ChatResult:

        # streaming mode
        if self.streaming:
            final_text = ""
            for token in self.transport.chat_stream(self.session_id, messages, model=self.model):
                final_text += token
                if run_manager:
                    run_manager.on_llm_new_token(token)
            return ChatResult(
                generations=[ChatGeneration(message=AIMessage(content=final_text))]
            )

        # non-streaming
        output = self.transport.chat(self.session_id, messages, model=self.model)
        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=output))]
        )

```

ä½¿ç”¨ç¤ºä¾‹ï¼š

- åˆå§‹åŒ– Wrapper

  ```python
  from scheduler_llm import SchedulerChatModel
  
  llm = SchedulerChatModel(
      base_url="https://YOUR_SCHEDULER_URL",
      session_id="session-abc123",
      model="gpt-4o-mini",
      streaming=False,
  )
  ```

- åœ¨ LangChain ä¸­å¯¹è¯ï¼ˆéæµå¼ï¼‰

  ```python
  result = llm.invoke("ç»™æˆ‘ä»‹ç»ä¸€ä¸‹ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ")
  print(result.content)
  ```

- æµå¼å¯¹è¯

  ```python
  llm_stream = SchedulerChatModel(
      base_url="https://YOUR_SCHEDULER_URL",
      session_id="session-xyz999",
      streaming=True,
  )
  
  for chunk in llm_stream.stream("è®²ä¸ªç¬‘è¯"):
      print(chunk, end="", flush=True)
  ```

------

Node Agentï¼ˆèŠ‚ç‚¹ä»£ç†ï¼‰æ•´ä½“ç»“æ„

```
node_agent/
  â”œâ”€â”€ agent.py               # NodeAgent ä¸»å…¥å£
  â”œâ”€â”€ session_manager.py     # ä¼šè¯ç®¡ç†ï¼ˆåˆ›å»º/ç»‘å®š/çŠ¶æ€ï¼‰
  â”œâ”€â”€ local_scheduler.py     # æœ¬åœ°è°ƒåº¦å™¨ï¼ˆä¼˜å…ˆçº§é˜Ÿåˆ—/Micro-Batchingï¼‰
  â”œâ”€â”€ vllm_adapter.py        # vLLM æ¨ç†æ¥å£
  â”œâ”€â”€ worker.py              # æ‰§è¡Œæ¨ç†ä»»åŠ¡ï¼ˆbatch workerï¼‰
  â”œâ”€â”€ heartbeat.py           # å‘å…¨å±€è°ƒåº¦å™¨ä¸ŠæŠ¥èµ„æº
  â”œâ”€â”€ models.py              # æ•°æ®ç»“æ„
  â””â”€â”€ api.py                 # FastAPI: /v1/chat/completions
```

ğŸ§© 1. æ•°æ®ç»“æ„ï¼ˆmodels.pyï¼‰

```
from pydantic import BaseModel
from typing import List, Optional


class InferenceRequest(BaseModel):
    session_id: str
    messages: List[dict]
    model: str
    stream: bool = False


class InferenceResponse(BaseModel):
    output: str


class SessionContext(BaseModel):
    session_id: str
    model: str
    last_access_ts: float
```

------

ğŸ§© 2. Session Managerï¼ˆsession_manager.pyï¼‰

è´Ÿè´£ï¼š

- Session æ˜¯å¦å±äºæœ¬èŠ‚ç‚¹
- Session æ˜¯å¦éœ€è¦ç»‘å®šæ¨¡å‹
- æ›´æ–° last_access timeï¼ˆç”¨äº LRU æˆ– evictionï¼‰
- ä¸ Global Scheduler çš„ session ç»‘å®šä¿æŒä¸€è‡´

```
import time
from typing import Dict
from models import SessionContext


class SessionManager:
    def __init__(self):
        self.sessions: Dict[str, SessionContext] = {}

    def ensure_session(self, session_id: str, model: str):
        if session_id not in self.sessions:
            self.sessions[session_id] = SessionContext(
                session_id=session_id,
                model=model,
                last_access_ts=time.time(),
            )
        else:
            self.sessions[session_id].last_access_ts = time.time()

        return self.sessions[session_id]
```

------

ğŸ§© 3. vLLM Adapterï¼ˆvllm_adapter.pyï¼‰

å°† batch è¾“å…¥è½¬ç»™ vLLM å®ä¾‹ã€‚

```
from typing import List
from vllm import LLM, SamplingParams


class VLLMAdapter:
    def __init__(self, model_path: str):
        self.llm = LLM(model=model_path)

    def run_batch(self, prompts: List[str], max_tokens=256) -> List[str]:
        params = SamplingParams(max_tokens=max_tokens)
        outputs = self.llm.generate(prompts, sampling_params=params)
        return [out.outputs[0].text for out in outputs]
```

------

ğŸ§© 4. Local Schedulerï¼ˆlocal_scheduler.pyï¼‰

> è¿™æ˜¯ Node Agent çš„æ ¸å¿ƒï¼š
>
> - ç»´æŠ¤å¤šä¸ªä¼˜å…ˆçº§é˜Ÿåˆ—
> - æ”¶é›† micro-batches
> - æäº¤ç»™ Worker æ‰§è¡Œ
> - å¯æ”¯æŒå¹¶è¡Œ worker

```
import asyncio
from collections import defaultdict, deque
from typing import List, Tuple
from models import SessionContext


class LocalScheduler:

    def __init__(self, vllm_adapter, max_batch_tokens=4096, tick_ms=10):
        self.vllm = vllm_adapter
        self.queues = defaultdict(deque)  # priority -> deque[(session, message)]
        self.max_batch_tokens = max_batch_tokens
        self.tick_ms = tick_ms / 1000
        self.loop = asyncio.get_event_loop()

    async def submit(self, session: SessionContext, message: dict, priority: int = 5):
        self.queues[priority].append((session, message))

    async def run(self):
        """Batch Loop"""
        while True:
            batch_sessions = []
            batch_prompts = []
            total_tokens = 0

            # æŒ‰ä¼˜å…ˆçº§æ”¶é›† batch
            for p in sorted(self.queues.keys()):
                q = self.queues[p]
                while q:
                    session, req = q[0]
                    prompt = self._messages_to_prompt(req["messages"])

                    est_tokens = len(prompt)  # ç®€åŒ–ä¼°è®¡ï¼Œå¯å®Œå–„
                    if total_tokens + est_tokens > self.max_batch_tokens:
                        break

                    q.popleft()
                    batch_sessions.append(session)
                    batch_prompts.append(prompt)
                    total_tokens += est_tokens

            if batch_prompts:
                outputs = await self.loop.run_in_executor(
                    None, self.vllm.run_batch, batch_prompts
                )
                # æŠŠè¾“å‡ºå†™å› sessionï¼ˆè¿™é‡Œåªåšç¤ºä¾‹ï¼‰
                for s, out in zip(batch_sessions, outputs):
                    s.last_output = out

            await asyncio.sleep(self.tick_ms)

    def _messages_to_prompt(self, messages):
        return "\n".join([f"{m['role']}: {m['content']}" for m in messages])
```

------

ğŸ§© 5. Workerï¼ˆworker.pyï¼‰

å¦‚æœå°†å¤æ‚é€»è¾‘è§£è€¦ä¸º workerï¼Œå¯ä»¥è¿™æ ·ï¼š

```
# ç”¨ run_batch å³å¯ï¼Œæ— éœ€å•ç‹¬æ¨¡å—
```

Node Agent ç®€åŒ– worker â€” å›  vLLM å·²å¼‚æ­¥åŒ–

------

ğŸ§© 6. Heartbeatï¼ˆheartbeat.pyï¼‰

å®šæœŸå‘å…¨å±€è°ƒåº¦å™¨ä¸ŠæŠ¥èŠ‚ç‚¹èµ„æºï¼š

```
import time
import psutil
import requests

class Heartbeat:
    def __init__(self, scheduler_url, node_id, agent_ref):
        self.scheduler_url = scheduler_url
        self.node_id = node_id
        self.agent_ref = agent_ref

    def send(self):
        gpu_free = self.agent_ref.get_gpu_free_mem()
        payload = {
            "node_id": self.node_id,
            "address": self.agent_ref.address,
            "gpu_free_mem": gpu_free,
            "gpu_total_mem": self.agent_ref.total_gpu_mem,
            "running_sessions": len(self.agent_ref.session_manager.sessions),
            "queue_len": self.agent_ref.local_scheduler.queue_length(),
            "loaded_models": [self.agent_ref.model_name],
            "last_heartbeat_ts": time.time()
        }
        requests.post(f"{self.scheduler_url}/v1/node/heartbeat", json=payload)

    async def run(self):
        while True:
            self.send()
            await asyncio.sleep(3)
```

------

ğŸ§© 7. Node Agent ä¸»ç¨‹åºï¼ˆagent.pyï¼‰

```
import asyncio
from session_manager import SessionManager
from local_scheduler import LocalScheduler
from vllm_adapter import VLLMAdapter
from heartbeat import Heartbeat


class NodeAgent:
    def __init__(self, node_id, address, model_path, scheduler_url):
        self.node_id = node_id
        self.address = address
        self.model_name = model_path

        self.session_manager = SessionManager()
        self.vllm = VLLMAdapter(model_path)
        self.local_scheduler = LocalScheduler(self.vllm)

        self.heartbeat = Heartbeat(scheduler_url, node_id, self)

    def get_gpu_free_mem(self):
        # ç®€åŒ–ï¼šå®é™…åº”è¯»å– NVML
        return 16000

    def start(self):
        loop = asyncio.get_event_loop()
        loop.create_task(self.local_scheduler.run())
        loop.create_task(self.heartbeat.run())
        loop.run_forever()
```

------

ğŸ§© 8. Node Agent APIï¼ˆapi.pyï¼‰

ç”¨äºæ¥æ”¶ LangChain Wrapper çš„æ¨ç†è¯·æ±‚ã€‚

```
from fastapi import FastAPI
from models import InferenceRequest
from agent import agent_instance   # NodeAgent singleton

app = FastAPI()


@app.post("/v1/chat/completions")
async def chat(req: InferenceRequest):
    session = agent_instance.session_manager.ensure_session(req.session_id, req.model)
    await agent_instance.local_scheduler.submit(session, req.dict())

    # ç­‰å¾…æ¨ç†å®Œæˆï¼ˆç®€åŒ–å®ç°ï¼‰
    while not getattr(session, "last_output", None):
        await asyncio.sleep(0.005)

    return {"output": session.last_output}
```

------

ğŸ§© Node Agent å•èŠ‚ç‚¹éƒ¨ç½²ï¼š

```
# å¯åŠ¨ Node Agent
agent_instance = NodeAgent(
    node_id="node-1",
    address="http://127.0.0.1:5001",
    model_path="gpt-4o-mini",
    scheduler_url="http://scheduler:9000"
)
agent_instance.start()
```

------

#### 8ã€å„æ¨¡å—ååŒå·¥ä½œçš„å…¨è¿‡ç¨‹

æ•´æ—¶åºå›¾ï¼ˆLangChain â†’ Global Scheduler â†’ Node Agentï¼‰ï¼š

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Client / App      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘  è¯·æ±‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     LangChain Wrapper  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘¡ è¯·æ±‚ session æ”¾ç½®
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Global Scheduler    â”‚
        â”‚  (session æ”¾ç½® / é€‰èŠ‚ç‚¹ ) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘¢ è¿”å› session â†’ node æ˜ å°„
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   LangChain Wrapper    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘£ ç›´æ¥æ‰“åˆ°ç›®æ ‡ node
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Node Agent       â”‚
        â”‚  (session + batch æ¨ç†)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘¤ è¯·æ±‚å…¥é˜Ÿ (Local Scheduler)
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Local Scheduler     â”‚
        â”‚ (ä¼˜å…ˆçº§é˜Ÿåˆ— + microbatch) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘¥ batch æ”¶é›†
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       vLLM Engine      â”‚
        â”‚  (çœŸæ­£åšæ¨ç†çš„ GPU æ¨¡å‹)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘¦ æ¨ç†è¾“å‡º
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Node Agent       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘§ è¿”å›ç»“æœ
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  LangChain Wrapper     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ â‘¨ è¿”å› client
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Client / App      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

------

ç®€è¦ç‰ˆï¼š

- è°ƒåº¦å™¨è´Ÿè´£å…¨å±€åˆ†é…
- Node Agent è´Ÿè´£æœ¬åœ°æ‰¹å¤„ç†ã€é«˜æ•ˆæ¨ç†
- vLLM è´Ÿè´£é«˜é€Ÿ GPU batch æ¨ç†

```
è°ƒåº¦å™¨å†³å®š session ç»™å“ªä¸ªèŠ‚ç‚¹
â†“
Node Agent æ”¶åˆ°è¯·æ±‚
â†“
Session Manager ç®¡ session
â†“
Local Scheduler æŠŠå¤šä¸ª session åˆæˆ batch
â†“
vLLM åšæ‰¹é‡æ¨ç†
â†“
Node Agent è¿”å›ç»“æœç»™ LangChain
â†“
LangChain è¿”å›ç»™ Client
```

å…·ä½“è€Œè¨€ï¼š

------

â‘  LangChain Wrapper è°ƒç”¨è°ƒåº¦æœåŠ¡åˆ›å»º Sessionï¼šLangChain Wrapper è´Ÿè´£ç»Ÿä¸€ä»£ç†æ‰€æœ‰æ¨¡å‹è°ƒç”¨

```
Client â†’ Global Scheduler:
  POST /v1/sessions/create
    { model: "gpt-4o-mini" }

Global Scheduler:
  - æŸ¥ Redis èŠ‚ç‚¹èµ„æº
  - é€‰æ‹© node-3
  - ä¿å­˜ session â†’ node æ˜ å°„
  - è¿”å› { session_id, node_id: "node-3" }
```

------

â‘¡ LangChain Wrapper è·å–è·¯ç”±ï¼šLangChain Wrapper å…ˆé—®è°ƒåº¦å™¨ï¼Œâ€œæˆ‘è¿™ä¸ª session åº”è¯¥å»å“ªå°æœåŠ¡å™¨æ¨ç†ï¼Ÿâ€

```
Client â†’ Global Scheduler:
  GET /v1/sessions/<id>/route
â†’ node-3
```

------

â‘¢ LangChain Wrapper æŠŠæ¨ç†è¯·æ±‚æ‰“åˆ° Node-3

```
POST http://node-3/v1/chat/completions
{
  "session_id": "...",
  "model": "gpt-4o-mini",
  "stream": false,
  "messages": [ ... ]
}
```

------

â‘£ Node Agent çš„ SessionManager

Session Manager çš„å·¥ä½œéå¸¸ç®€å•ï¼šå®ƒå­˜å‚¨ session çš„ä¸Šä¸‹æ–‡å’ŒçŠ¶æ€ã€‚

> â€œè¿™ä¸ª session æˆ‘è®¤è¯†ï¼Œè¿˜æ˜¯ç¬¬ä¸€æ¬¡æ¥ï¼Ÿâ€
>  â€œæŠŠå®ƒè®°ä¸‹æ¥ï¼Œä¸‹æ¬¡ç»§ç»­ç”¨ã€‚â€

```
session_manager.ensure_session(session_id, model)
```

å¦‚æœ session ç¬¬ä¸€æ¬¡åˆ°æ¥ â†’ åˆ›å»º session ç»‘å®šæœ¬èŠ‚ç‚¹ã€‚

------

â‘¤ LocalScheduler å°†è¯·æ±‚æ”¾å…¥é˜Ÿåˆ—ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

```
queues[priority].append((session, req))
```

è¡¥å……ï¼šLocal Scheduler = **æœ¬åœ°çš„å°å‹è°ƒåº¦å™¨**ï¼ˆå†³å®šå“ªäº›è¯·æ±‚åˆå¹¶æˆ batch å»è·‘ï¼‰ï¼Œå®ƒç»´æŠ¤å¤šæ¡é˜Ÿåˆ—ï¼š

```
ä¼˜å…ˆçº§1é˜Ÿåˆ—: [S1, S4, S9]
ä¼˜å…ˆçº§2é˜Ÿåˆ—: [S3, S7]
ä¼˜å…ˆçº§5é˜Ÿåˆ—: [S2]
```

------

â‘¥ LocalScheduler é€šè¿‡ batch loop æ”¶é›†å¤šä¸ª session è¯·æ±‚ â†’ é€ vLLM

```
batch = [(sessionA, promptA), (sessionB, promptB)]
outputs = vllm.run_batch(prompts)
```

è¡¥å……ï¼šLocal Scheduler å®šæœŸæ‰¹å¤„ç†ï¼ˆmicro-batchingï¼‰ï¼Œä¾‹å¦‚æ¯ 10ms çœ‹ä¸€çœ¼ï¼š

```
èƒ½ä¸èƒ½æŠŠè¿™ 6 ä¸ª session çš„ prompt åˆæˆ 1 ä¸ª batchï¼Ÿ
èƒ½ï¼Œé‚£å°±é€ç»™ vLLM æ¨ç†ã€‚
```

------

â‘¦ vLLM ç”Ÿæˆç»“æœ â†’ å†™å› SessionContext

```
session.last_output = "ç”Ÿæˆå†…å®¹..."
```

å…·ä½“æ¥è¯´ï¼švLLM ä¼šè¿”å›ä¸€ä¸ª batch ç»“æœï¼Œä¾‹å¦‚ï¼š

```
[S1 è¾“å‡ºå†…å®¹, S2 è¾“å‡ºå†…å®¹, S3 è¾“å‡ºå†…å®¹ ...]
```

Node Agent æ ¹æ®é¡ºåºæŠŠç»“æœæ”¾å›å„è‡ªçš„ sessionã€‚

------

â‘§ API è¿”å›æ¨ç†ç»“æœç»™ LangChain Wrapper

LangChain Wrapper å†è§£æä¸º `AIMessage` å¡«å……ç»„ä»¶é“¾ã€‚

æ¢å¥è¯è¯´ï¼Œå°±æ˜¯LangChain åˆæŠŠ response å°è£…æˆ`AIMessage` è¿”å›ç»™ clientã€‚

------

â‘¨ Node Agent æ¯ 3 ç§’ä¸ŠæŠ¥ Heartbeat ç»™ Global Scheduler

```
POST /v1/node/heartbeat
{
  node_id: node-3,
  gpu_free_mem: ...,
  running_sessions: ...,
  queue_len: ...,
}
```

Global Scheduler ç»´æŠ¤å…¨å±€èµ„æºè§†å›¾ï¼Œè°ƒåº¦å™¨æ®æ­¤çŸ¥é“æ¯å°æœºå™¨çš„çŠ¶æ€ã€‚

```
GPU ç©ºé—²ï¼š15000MB
æ’é˜Ÿè¯·æ±‚ï¼š12
æ´»è·ƒä¼šè¯æ•°ï¼š53
å·²åŠ è½½æ¨¡å‹ï¼šgpt-4o-mini
```

ç”¨äºåç»­ session çš„æ”¾ç½®ã€‚



#### 9ã€æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼ˆå®è·µæ¸…å•ï¼‰

1. **æœ€å¤§åŒ–å¾®æ‰¹**ï¼šæŠŠç­‰å¾…çª—å£æ§åˆ¶åœ¨ 5â€“20ms ä»¥å…¼é¡¾ååå’Œå»¶è¿Ÿ
2. **æ¨¡å‹å…±äº« & quantization**ï¼š4/8-bitï¼Œfloat16ï¼›é¿å…å¤åˆ¶ model weights
3. **Session affinity**ï¼šå°½é‡åœ¨åŒèŠ‚ç‚¹é‡ç”¨ sessionï¼Œé¿å…é¢‘ç¹åŠ è½½
4. **Prompt cache & result cache**ï¼šå¯¹é‡å¤ prompt å‘½ä¸­ç¼“å­˜
5. **é›¶æ‹·è´**ï¼šåœ¨ node å†…ä½¿ç”¨é›¶æ‹·è´ buffer ä¼ é€’ token / logits
6. **Avoid head-of-line blocking**ï¼šä¼˜å…ˆçº§åŒ–è°ƒåº¦ï¼Œä½ä¼˜å…ˆçº§åœ¨ä¸å½±å“é«˜ä¼˜å…ˆçº§ä¸‹æ‰©å¤§ batch
7. **Backpressure**ï¼šå½“ queue è¶…é˜ˆå€¼æ—¶è¿”å› 429 æˆ–åœ¨ ingress åšé€Ÿç‡é™åˆ¶
8. **Memory safety**ï¼šç›‘æ§ OOMï¼Œä½¿ç”¨ per-model memory accounting