GPU方案 - 96G显存（如 双卡Ampere A6000 / 单卡 RTX Pro 6000 Blackwell）

- Qwen/Qwen3-32B
- Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4

GPU方案 - 其他：

- [Qwen/Qwen3-235B-A22B](https://modelscope.cn/models/Qwen/Qwen3-235B-A22B)：
  - [阿里云官方 - 在GPU实例上部署Qwen3-235B-A22B](https://help.aliyun.com/zh/ecs/user-guide/deploy-qwen3-235b-a22b-on-gpu-accelerated-instances)：720+GB 显存（8 * 96 = 768）
  - [阿里云官方 - Qwen3系列模型部署、微调、评测](https://www.alibabacloud.com/help/zh/pai/use-cases/deploy-fine-tune-and-evaluate-qwen3-in-quickstart)：Qwen3-235B-A22B → 8 * 96 GB 显存，Qwen3-235B-A22B-FP8 → 4 * 96 GB 显存（四卡RTX Pro 6000，完整服务器40w内）

Ktransformers方案：

- [DeepSeek-V3/R1-671B-Q4_K_M](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md)：`14G 显存` + `382+GB 内存`
- [Qwen3-235B-A22B（4-bit）](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/AMX.md)：`13G 显存` + `160+GB 内存`
- [moonshotai/Kimi-K2.5](moonshotai/Kimi-K2.5)：`48G 显存` + `600+GB 内存` （完整服务器20w内）

华为昇腾方案：

- [Qwen3-32B](https://www.hiascend.com/software/modelzoo/models/detail/4d4ddb25b60d47bd880d4a2b1a218435)：一台 [800I A2服务器](https://e.huawei.com/cn/products/computing/ascend/atlas-800i-a2)，单台京东约25w？
- [DeepSeek-R1](https://www.hiascend.com/software/modelzoo/models/detail/68457b8a51324310aad9a0f55c3e56e3)：部署DeepSeek-R1模型用BF16权重进行推理至少需要4台Atlas 800I A2（8 * 64G）服务器，用W8A8量化权重进行推理则至少需要2台Atlas 800I A2 (8 * 64G)

------

如果使用 RoPE 扩展（如 NTK scaling、YaRN） 增加上下文长度是否会导致显存占用增加？

- 本身几乎不会新增显存占用；
- 真正导致显存上涨的是你“真的用了更长的上下文”，而不是位置编码方法。



# 01 推理场景

### 1、基础知识

> 参考：[LLM 大模型训练-推理显存占用分析](https://yuanchaofa.com/post/llm-train-infer-memoery-usage-calculation.html)

模型推理一共有两部分内容会占用 GPU 显存，**模型参数和 KV cache**。

- 其中假设模型的参数是 θ，那么推理时候占用的显存是 2Φ，这是因为现在 HuggingFace 中大部分模型的参数都保存为 BF16，如果没有特殊的必要，不会用 fp32 加载。

- KV cache，假设输入序列的长度为 s ，输出序列的长度为 n ，以float16来保存KV cache，那么**KV cache的峰值显存占用大小为** 
  $$
   
  b
  (
  s
  +
  n
  )
  h
  ∗
  l
  ∗
  2
  ∗
  2
  =
  4
  b
  l
  h
  (
  s
  +
  n
  )
  b(s+n)h∗l∗2∗2=4blh(s+n) 
  $$
  这里第一个2 表示K/V cache，第二个2表示float16占2个bytes。

粗略的预估，模型推理需要的内存为： **1.2 倍的模型参数内存** = 1.2×2Φ=2.4Φ1.2×2Φ=2.4Φ，以 7B 模型为例，那么推理需要的内存大概是： 16.8 GB。 相对精确的预估：按照上面的公式进行计算

注意：

- 推理的时候并不需要保存激活值，看到有的博客说需要保存激活值是错的



> 参考：
>
> - [大模型显存计算公式与优化](https://zhuanlan.zhihu.com/p/687226668)
> - [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/)

推理显存可以粗略设计为：
$$
\text{InferMemory} \approx 1.2 * \text{ModelMemory}
$$


### 2、推理实践

> 以下为ChatGPT生成，未验证

以 **Qwen2.5-72B Int4** 为例：

一、仅模型权重需要多少显存？

- 理论权重大小
  - 72B 参数 × 4 bit = 72B × 0.5 Byte ≈ **36 GB**
  - 但**实际部署不是 36GB**，原因：Int4 通常是 **GPTQ / AWQ / SmoothQuant**，需要额外 scale / zero-point、对齐 padding、CUDA kernel buffer
- 实际显存占用（经验值）：`模型权重（Int4）40–44 GB` + `CUDA / framework buffer 2–4 GB` = `42–48 GB 显存`

二、推理时额外显存消耗（关键）

- `推理显存 = KV Cache + 临时激活 + batch buffer`，其中 **KV Cache 是最大头**
- KV Cache 显存计算：
  - Qwen2.5-72B 结构（近似）：
    - 层数：80
    - hidden size：8192
    - heads：64
    - head_dim：128
    - KV 精度：FP16（2 bytes）
  - KV Cache 公式：`KV ≈ 2 × layers × seq_len × hidden_size × bytes`
- 换算结果，8k上下文约16GB，16k上下文约32GB

三、完整显存需求汇总

1、最小可跑：Int4 + batch=1

| 上下文 | 总显存                            |
| ------ | --------------------------------- |
| 8k     | **≈ 60–65 GB**                    |
| 16k    | **≈ 75–80 GB**                    |
| 32k    | **≈ 105–115 GB** ❌ 单卡基本不现实 |

2、生产推理（batch=4）

| 上下文 | 总显存           |
| ------ | ---------------- |
| 8k     | **≈ 100–110 GB** |
| 16k    | **≈ 150 GB+**    |



# 02 训练场景

> 参考：[快手二面拷打：训练100B模型要多少显存？](https://mp.weixin.qq.com/s/tdPrtsxOfnpyQzE25psdUQ)

概述：

- **训练显存**消耗（可估算部分）主要包括：模型参数（Model）+ 优化器状态（Optimizer status）+梯度值（Gradient）+激活值（Activation）
- **显存优化**的过程一般是从模型算法本身到底层，可以参考的优化路径：多卡并行 -> 算子/数据类型 -> 消除框架副本 -> 显存管理 -> 底层 API