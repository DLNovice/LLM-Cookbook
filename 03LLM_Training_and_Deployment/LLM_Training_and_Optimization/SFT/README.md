框架的使用方法参考本目录对应笔记，此文档主要记录一些SFT方案。



> 参考：[大模型微调记录 - 挺逗的汪 | 小红书](https://www.xiaohongshu.com/discovery/item/688081910000000012016c8a)

笔者面向智能销售业务：

- 从0到1：PromptEngineer、ContextEngineer（动态示例、Memory压缩检索等），但BadCase依旧高
- 尝试训练：核心目标，即让模型像真人一样跟客户聊天
  - 尝试RL：
    - ppo、grpo等方法需要一个明确的 Reward Model（奖励模型）。
    - 笔者先尝试标注了5k不算纯净的数据，基于 Reasoning Reward Model（推理奖励模型）方案，微调GRPO，但是效果很差，但是对“差”的判断还是挺准的。
    - 修改标签，将“好、中、差”改为了“合理、不合理”，基于ppo训练，结果效果更差了。
  - 尝试SFT：
    - 做好BadCase：人工标注badcase，做badecase分类系统，详细的归纳出badcase原因，并根据badcase找解法
    - 让模型评估出“中好差”很难，但是让模型做对比判断准确率高，就是谁比谁好：于是，研发一个reasoning的偏序rm，没做ppo和grpo，选择了做online dpo
    - 少量高质量的样本，模型效果就很不错。
- 总结：
  - 算法方面：普通业务中，ppo和grpo等RL算法很厉害，但是sft和dpo或许是最好的选择。当基准模型在本身领域有一定能力时，lorarank=8就可以通杀了。
  - 数据方便：数据是王道，大部分时间都在标注，提高质量。
  - 先活下去，再讲故事。



> 参考：
>
> - [为什么我放弃235B/671B，转而训练8B？](https://mp.weixin.qq.com/s/x_I08orDgrMihDvha15Zog)
> - [用8B模型训练垂直Agent全部细节（完结篇）](https://mp.weixin.qq.com/s/6x84K25-aD5zXMXhDGxIkA)

一、问题背景

在垂直领域Agent落地时，最大的挑战不是“模型不够聪明”，而是**工具调用的不稳定性**。具体表现为：

- 该调用工具时不调用
- 调用工具但参数错误
- 多轮对话中直接编造工具返回结果 这导致业务流程的端到端一次通过率很难超过50%。

二、解决方案

作者放弃了使用Qwen3 235B、DeepSeek V3.1 671B等超大模型配合Prompt工程的方法，转而采用**Qwen3-8B小模型进行针对性训练**。

**指标含义定义：**

- **tool_call_accuracy：**是否调用工具（与期望一致）；若调用，则函数名与参数必须完全正确才算对（严格口径）。
- **tool_name_accuracy：**在“模型发生工具调用”的情况下，函数名正确率。
- **tool_args_accuracy：**在“模型发生工具调用”的情况下，参数与预期一致的比例。
- **response_quality：**回答是否存在超短、结尾重复、或不匹配的 <think> 标签等格式问题（值越高越好）。

**训练策略（两阶段）**

1. **SFT（监督微调）**：注入垂直领域背景知识、业务流程，让模型“懂业务”
2. **DPO（直接偏好优化）**：专门对齐工具调用偏好，让模型“守规矩”

关键发现

- SFT后工具调用指标短期没有改善，但回答更贴近业务领域
- **扩大DPO数据规模（v2版本数据集扩大5倍）**后，工具调用准确率从约30%提升到97%-99%
- 多轮对话和推理能力未出现明显损坏

三、核心结论与观点

1. **长期最优解**：如果合规允许，将私域数据贡献给上游开源生态是长期收益最大的选择。但在无法开源的情况下，后训练是可行解。
2. **微调目标**：微调应是“注入”私域术语和业务流程，而非“覆盖”基座模型的通用能力。
3. **稳定性**：后训练提供的稳定性优于提示工程（Prompt Engineering），在多轮复杂对话中更可控、健壮。
4. **对小模型（8B）的乐观态度**：8B模型已能支撑不少复杂私域场景，关键在于扎实的评估体系、数据策略和训练节奏，无需一味追求大算力。

四、实践“避坑指南”

1. **警惕“脑损伤”SFT**：监督微调（SFT）易破坏原始模型能力，一旦出现功能丧失、乱码等问题，应及早终止并回滚。
2. **训练次序**：先通过SFT注入知识并保留模型“智商”（对话和思考能力），再进行DPO训练对齐行为偏好。DPO无法修复已被SFT破坏的能力。
3. **评估大于训练**：建立高效的评估体系，进行数据审计和快速回归测试，及时淘汰无效训练分支，这比盲目跑大量数据更有效。