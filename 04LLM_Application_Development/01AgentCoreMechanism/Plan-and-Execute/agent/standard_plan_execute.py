"""
æ ‡å‡† Plan-and-Execute æ¶æ„ï¼ˆLangGraph å¼‚æ­¥å®ç°ï¼‰
=================================================
åŒ…å«æ¨¡å—ï¼š
1. Plannerï¼šç”Ÿæˆç»“æ„åŒ–æ‰§è¡Œè®¡åˆ’ï¼ˆæ”¯æŒå·¥å…·æ„ŸçŸ¥ï¼‰
2. Executorï¼šå¸¦å·¥å…·è°ƒç”¨èƒ½åŠ›çš„æ‰§è¡Œå™¨
3. Judgeï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ Replan
4. Replannerï¼šåŠ¨æ€è°ƒæ•´è®¡åˆ’
5. Finalizerï¼šæ±‡æ€»æœ€ç»ˆç»“æœ
6. Memoryï¼šåŸºäº Mem0 çš„è®°å¿†ç³»ç»Ÿ

æ ¸å¿ƒç‰¹æ€§ï¼š
- âœ… å¼‚æ­¥æ‰§è¡Œ
- âœ… MCP å·¥å…·é›†æˆ
- âœ… æ­¥éª¤é—´ä¾èµ–ç®¡ç†
- âœ… å…±äº«ä¸Šä¸‹æ–‡ä¼ é€’
- âœ… æ‰§è¡Œå†å²è¿½è¸ª
- âœ… å¤±è´¥è‡ªåŠ¨é‡è§„åˆ’
- âœ… é˜²æ­¢æ— é™å¾ªç¯
- âœ… è®°å¿†ç³»ç»Ÿæ”¯æŒ
"""

import json
import time
import logging
from typing import List, Dict, Any, Optional, Literal
from typing_extensions import TypedDict
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import BaseTool
from langgraph.graph import StateGraph, END

from app.agent.config import PlanExecuteConfig, default_config
from app.agent.memory import AgentMemory, create_memory, generate_session_id
from app.prompt.planning import (
    TOOL_ENHANCED_PLANNER_PROMPT,
    TOOL_ENHANCED_EXECUTOR_PROMPT,
    JUDGE_PROMPT,
    REPLANNER_PROMPT,
    FINALIZER_PROMPT,
    TOOL_SELECTOR_PROMPT,
    EXPERIENCE_SUMMARY_PROMPT
)

logger = logging.getLogger(__name__)


# =========================================================
# 1. æ•°æ®ç»“æ„å®šä¹‰
# =========================================================

class PlanStep(BaseModel):
    """å•ä¸ªè®¡åˆ’æ­¥éª¤"""
    step_id: str = Field(description="æ­¥éª¤å”¯ä¸€ID")
    description: str = Field(description="æ­¥éª¤æè¿°")
    expected_output: str = Field(description="æœŸæœ›è¾“å‡ºç±»å‹")
    dependencies: List[str] = Field(default=[], description="ä¾èµ–çš„å‰ç½®æ­¥éª¤ID")
    suggested_tools: List[str] = Field(default=[], description="å»ºè®®ä½¿ç”¨çš„å·¥å…·")


class ExecutionRecord(BaseModel):
    """æ‰§è¡Œè®°å½•"""
    step_id: str = Field(description="å¯¹åº”çš„æ­¥éª¤ID")
    input_snapshot: Dict[str, Any] = Field(default={}, description="æ‰§è¡Œæ—¶çš„è¾“å…¥å¿«ç…§")
    output: str = Field(description="æ‰§è¡Œç»“æœ")
    status: str = Field(description="success/failed/skipped")
    tool_calls: List[Dict[str, Any]] = Field(default=[], description="å·¥å…·è°ƒç”¨è®°å½•")
    error_message: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")
    timestamp: float = Field(default_factory=time.time, description="æ‰§è¡Œæ—¶é—´æˆ³")


class PlannerOutput(BaseModel):
    """Planner ç»“æ„åŒ–è¾“å‡º"""
    steps: List[PlanStep] = Field(description="æ‰§è¡Œæ­¥éª¤åˆ—è¡¨")


class ExecutorOutput(BaseModel):
    """Executor ç»“æ„åŒ–è¾“å‡º"""
    action: Literal["tool_call", "direct_response"] = Field(description="åŠ¨ä½œç±»å‹")
    tool_name: Optional[str] = Field(default=None, description="å·¥å…·åç§°")
    tool_input: Optional[Dict[str, Any]] = Field(default=None, description="å·¥å…·å‚æ•°")
    result: Optional[str] = Field(default=None, description="ç›´æ¥å“åº”ç»“æœ")
    shared_updates: Dict[str, Any] = Field(default={}, description="å…±äº«ä¸Šä¸‹æ–‡æ›´æ–°")
    status: str = Field(default="success", description="æ‰§è¡ŒçŠ¶æ€")
    error_message: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")


class JudgeOutput(BaseModel):
    """Judge ç»“æ„åŒ–è¾“å‡º"""
    decision: Literal["CONTINUE", "REPLAN", "END"] = Field(description="åˆ¤æ–­ç»“æœ")
    reason: str = Field(description="åˆ¤æ–­ç†ç”±")


class ReplannerOutput(BaseModel):
    """Replanner ç»“æ„åŒ–è¾“å‡º"""
    reuse_steps: List[str] = Field(description="å¯å¤ç”¨çš„æ­¥éª¤ID")
    new_steps: List[PlanStep] = Field(description="æ–°çš„æ‰§è¡Œæ­¥éª¤")
    adjustment_summary: str = Field(description="è°ƒæ•´è¯´æ˜")


class StandardPlanExecuteState(TypedDict):
    """æ ‡å‡† Plan-and-Execute æ¶æ„çš„å…¨å±€ State"""

    # ============ ç”¨æˆ·è¾“å…¥å±‚ ============
    original_input: str              # ç”¨æˆ·åŸå§‹ç›®æ ‡ï¼ˆä¸å¯å˜ï¼‰
    session_id: str                  # ä¼šè¯ ID

    # ============ è§„åˆ’å±‚ ============
    current_plan: List[Dict[str, Any]]  # å½“å‰æ‰§è¡Œè®¡åˆ’
    plan_version: int                # è®¡åˆ’ç‰ˆæœ¬å·

    # ============ æ‰§è¡Œå±‚ ============
    execution_history: List[Dict[str, Any]]  # å®Œæ•´æ‰§è¡Œå†å²
    current_step_index: int          # å½“å‰æ‰§è¡Œåˆ°ç¬¬å‡ æ­¥

    # ============ å·¥å…·å±‚ ============
    tools: List[BaseTool]            # å¯ç”¨å·¥å…·åˆ—è¡¨
    tools_description: str           # å·¥å…·æè¿°æ–‡æœ¬

    # ============ ä¸Šä¸‹æ–‡å…±äº«å±‚ ============
    shared_context: Dict[str, Any]   # æ­¥éª¤é—´å…±äº«çš„æ•°æ®

    # ============ è®°å¿†å±‚ ============
    memory_context: str              # æ£€ç´¢åˆ°çš„ç›¸å…³è®°å¿†
    compressed_history: str          # å‹ç¼©åçš„å¯¹è¯å†å²

    # ============ åˆ¤æ–­ä¸åé¦ˆå±‚ ============
    last_execution_status: str       # ä¸Šæ¬¡æ‰§è¡ŒçŠ¶æ€
    judge_decision: str              # Judge çš„åˆ¤æ–­ç»“æœ
    replan_reason: Optional[str]     # è§¦å‘ Replan çš„åŸå› 
    replan_count: int                # Replan æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰

    # ============ è¾“å‡ºå±‚ ============
    final_response: str              # æœ€ç»ˆè¾“å‡º
    metadata: Dict[str, Any]         # å…ƒæ•°æ®


# =========================================================
# 2. å·¥å…·ç®¡ç†
# =========================================================

async def load_mcp_tools(config: PlanExecuteConfig) -> tuple[List[BaseTool], str]:
    """
    åŠ è½½ MCP å·¥å…·

    Returns:
        (å·¥å…·åˆ—è¡¨, å·¥å…·æè¿°æ–‡æœ¬)
    """
    try:
        from langchain_mcp_adapters.client import MultiServerMCPClient

        client_config = config.mcp.to_client_config()
        client = MultiServerMCPClient(client_config)

        tools = await client.get_tools()
        logger.info(f"å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·")

        # ç”Ÿæˆå·¥å…·æè¿°
        descriptions = []
        for tool in tools:
            desc = f"- **{tool.name}**: {tool.description}"
            descriptions.append(desc)

        tools_description = "\n".join(descriptions) if descriptions else "æ— å¯ç”¨å·¥å…·"

        return tools, tools_description

    except Exception as e:
        logger.warning(f"MCP å·¥å…·åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä»¥æ— å·¥å…·æ¨¡å¼è¿è¡Œ")
        return [], "æ— å¯ç”¨å·¥å…·ï¼ˆMCP è¿æ¥å¤±è´¥ï¼‰"


def format_tools_for_prompt(tools: List[BaseTool]) -> str:
    """æ ¼å¼åŒ–å·¥å…·ä¿¡æ¯ç”¨äº Prompt"""
    if not tools:
        return "æ— å¯ç”¨å·¥å…·"

    lines = []
    for tool in tools:
        lines.append(f"- **{tool.name}**: {tool.description}")
        # å¦‚æœæœ‰å‚æ•° schemaï¼Œä¹Ÿå¯ä»¥æ·»åŠ 
    return "\n".join(lines)


# =========================================================
# 3. LLM å·¥å‚
# =========================================================

def get_llm(config: PlanExecuteConfig, temperature: float = 0.3):
    """è·å– LLM å®ä¾‹"""
    return ChatOpenAI(
        model=config.llm.model,
        temperature=temperature,
        api_key=config.llm.api_key,
        base_url=config.llm.base_url
    )


# =========================================================
# 4. Planner èŠ‚ç‚¹ï¼ˆå¼‚æ­¥ï¼‰
# =========================================================

async def planner_node(
    state: StandardPlanExecuteState,
    config: PlanExecuteConfig = default_config
) -> Dict[str, Any]:
    """
    ç”Ÿæˆåˆå§‹æ‰§è¡Œè®¡åˆ’ï¼ˆå·¥å…·æ„ŸçŸ¥ç‰ˆæœ¬ï¼‰
    """
    logger.info("ğŸ§  [Planner] æ­£åœ¨ç”Ÿæˆæ‰§è¡Œè®¡åˆ’...")

    llm = get_llm(config, temperature=0.3).with_structured_output(PlannerOutput)

    prompt = TOOL_ENHANCED_PLANNER_PROMPT.format(
        objective=state["original_input"],
        available_tools=state.get("tools_description", "æ— å¯ç”¨å·¥å…·"),
        memory_context=state.get("memory_context", "æ— ç›¸å…³å†å²è®°å¿†")
    )

    result = await llm.ainvoke([HumanMessage(content=prompt)])

    # å°† Pydantic å¯¹è±¡è½¬æ¢ä¸º dict
    plan_dicts = [step.model_dump() for step in result.steps]

    logger.info(f"âœ… [Planner] å·²ç”Ÿæˆ {len(plan_dicts)} ä¸ªæ­¥éª¤")
    for i, step in enumerate(plan_dicts, 1):
        tools_hint = f" (å·¥å…·: {', '.join(step['suggested_tools'])})" if step['suggested_tools'] else ""
        logger.info(f"   {i}. [{step['step_id']}] {step['description']}{tools_hint}")

    return {
        "current_plan": plan_dicts,
        "plan_version": state["plan_version"] + 1,
        "current_step_index": 0
    }


# =========================================================
# 5. Executor èŠ‚ç‚¹ï¼ˆå¼‚æ­¥ + å·¥å…·è°ƒç”¨ï¼‰
# =========================================================

def format_plan_overview(plan: List[Dict[str, Any]]) -> str:
    """æ ¼å¼åŒ–è®¡åˆ’æ¦‚è§ˆ"""
    lines = []
    for i, step in enumerate(plan, 1):
        deps = ", ".join(step["dependencies"]) if step["dependencies"] else "æ— "
        tools = ", ".join(step.get("suggested_tools", [])) or "æ— "
        lines.append(f"{i}. [{step['step_id']}] {step['description']} (ä¾èµ–: {deps}, å·¥å…·: {tools})")
    return "\n".join(lines)


def get_dependency_results(
    step: Dict[str, Any],
    execution_history: List[Dict[str, Any]]
) -> str:
    """è·å–ä¾èµ–æ­¥éª¤çš„æ‰§è¡Œç»“æœ"""
    if not step["dependencies"]:
        return "æ— å‰ç½®æ­¥éª¤"

    lines = []
    for dep_id in step["dependencies"]:
        record = next(
            (r for r in execution_history if r["step_id"] == dep_id),
            None
        )
        if record:
            lines.append(f"[{dep_id}]:")
            lines.append(record["output"])
            lines.append("")

    return "\n".join(lines) if lines else "ä¾èµ–æ­¥éª¤æœªæ‰¾åˆ°æ‰§è¡Œè®°å½•"


async def execute_tool(
    tool_name: str,
    tool_input: Dict[str, Any],
    tools: List[BaseTool]
) -> tuple[str, bool]:
    """
    æ‰§è¡Œå·¥å…·è°ƒç”¨

    Returns:
        (ç»“æœæ–‡æœ¬, æ˜¯å¦æˆåŠŸ)
    """
    tool = next((t for t in tools if t.name == tool_name), None)

    if not tool:
        return f"å·¥å…· '{tool_name}' ä¸å­˜åœ¨", False

    try:
        result = await tool.ainvoke(tool_input)
        return str(result), True
    except Exception as e:
        return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}", False


async def executor_node(
    state: StandardPlanExecuteState,
    config: PlanExecuteConfig = default_config
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå½“å‰æ­¥éª¤ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
    """
    current_index = state["current_step_index"]
    current_step = state["current_plan"][current_index]

    logger.info(
        f"âš™ï¸  [Executor] æ‰§è¡Œæ­¥éª¤ {current_index + 1}/{len(state['current_plan'])}: "
        f"[{current_step['step_id']}] {current_step['description']}"
    )

    llm = get_llm(config, temperature=0.5).with_structured_output(ExecutorOutput)

    # æ„é€ æ‰§è¡Œ Prompt
    prompt = TOOL_ENHANCED_EXECUTOR_PROMPT.format(
        objective=state["original_input"],
        plan_overview=format_plan_overview(state["current_plan"]),
        current_step_id=current_step["step_id"],
        current_step_description=current_step["description"],
        expected_output=current_step["expected_output"],
        suggested_tools=", ".join(current_step.get("suggested_tools", [])) or "æ— ",
        dependency_results=get_dependency_results(current_step, state["execution_history"]),
        shared_context=json.dumps(state["shared_context"], ensure_ascii=False, indent=2),
        available_tools=state.get("tools_description", "æ— å¯ç”¨å·¥å…·")
    )

    tool_calls = []

    try:
        result = await llm.ainvoke([HumanMessage(content=prompt)])

        # å¤„ç†å·¥å…·è°ƒç”¨
        if result.action == "tool_call" and result.tool_name:
            logger.info(f"ğŸ”§ [Executor] è°ƒç”¨å·¥å…·: {result.tool_name}")

            tool_result, tool_success = await execute_tool(
                result.tool_name,
                result.tool_input or {},
                state.get("tools", [])
            )

            tool_calls.append({
                "tool_name": result.tool_name,
                "tool_input": result.tool_input,
                "tool_output": tool_result,
                "success": tool_success
            })

            output = tool_result
            status = "success" if tool_success else "failed"
            error_msg = None if tool_success else tool_result

        else:
            # ç›´æ¥å“åº”
            output = result.result or ""
            status = result.status
            error_msg = result.error_message

        # åˆ›å»ºæ‰§è¡Œè®°å½•
        execution_record = {
            "step_id": current_step["step_id"],
            "input_snapshot": {
                "description": current_step["description"],
                "dependencies": current_step["dependencies"]
            },
            "output": output,
            "status": status,
            "tool_calls": tool_calls,
            "error_message": error_msg,
            "timestamp": time.time()
        }

        # æ›´æ–°å…±äº«ä¸Šä¸‹æ–‡
        updated_context = {**state["shared_context"], **result.shared_updates}

        status_icon = "âœ…" if status == "success" else "âŒ"
        logger.info(f"{status_icon} [Executor] æ­¥éª¤ {current_step['step_id']} æ‰§è¡Œ{status}")

        return {
            "execution_history": state["execution_history"] + [execution_record],
            "shared_context": updated_context,
            "current_step_index": current_index + 1,
            "last_execution_status": status
        }

    except Exception as e:
        logger.error(f"âŒ [Executor] æ­¥éª¤ {current_step['step_id']} æ‰§è¡Œå¼‚å¸¸: {str(e)}")

        execution_record = {
            "step_id": current_step["step_id"],
            "input_snapshot": {},
            "output": "",
            "status": "failed",
            "tool_calls": tool_calls,
            "error_message": str(e),
            "timestamp": time.time()
        }

        return {
            "execution_history": state["execution_history"] + [execution_record],
            "current_step_index": current_index + 1,
            "last_execution_status": "failed"
        }


# =========================================================
# 6. Judge èŠ‚ç‚¹ï¼ˆå¼‚æ­¥ï¼‰
# =========================================================

async def judge_node(
    state: StandardPlanExecuteState,
    config: PlanExecuteConfig = default_config
) -> Dict[str, Any]:
    """
    åˆ¤æ–­ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå°†åˆ¤æ–­ç»“æœå­˜å…¥ state
    """
    logger.info("âš–ï¸  [Judge] è¯„ä¼°å½“å‰æ‰§è¡ŒçŠ¶æ€...")

    # 1. æ£€æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰æ­¥éª¤
    if state["current_step_index"] >= len(state["current_plan"]):
        logger.info("âœ… [Judge] æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ â†’ END")
        return {"judge_decision": "end"}

    # 2. æ£€æŸ¥æ˜¯å¦è¾¾åˆ° Replan ä¸Šé™
    max_replan = config.max_replan_count
    if state["replan_count"] >= max_replan:
        logger.warning(f"âš ï¸  [Judge] å·²è¾¾åˆ°æœ€å¤§ Replan æ¬¡æ•° ({max_replan}) â†’ END")
        return {"judge_decision": "end"}

    # 3. å¦‚æœæœ€åä¸€æ­¥å¤±è´¥ï¼Œè°ƒç”¨ LLM åˆ¤æ–­
    if state["last_execution_status"] == "failed":
        logger.info("âš ï¸  [Judge] æ£€æµ‹åˆ°æ­¥éª¤å¤±è´¥ï¼Œè°ƒç”¨ LLM åˆ¤æ–­...")

        llm = get_llm(config, temperature=0.2).with_structured_output(JudgeOutput)

        last_record = state["execution_history"][-1] if state["execution_history"] else {}

        prompt = JUDGE_PROMPT.format(
            objective=state["original_input"],
            current_plan=format_plan_overview(state["current_plan"]),
            execution_history="\n".join([
                f"[{r['step_id']}] {r['status']}: {r['output'][:100]}..."
                for r in state["execution_history"]
            ]),
            completed_count=len([r for r in state["execution_history"] if r["status"] == "success"]),
            total_count=len(state["current_plan"]),
            last_status=last_record.get("status", "unknown")
        )

        try:
            result = await llm.ainvoke([HumanMessage(content=prompt)])
            decision = result.decision.lower()

            logger.info(f"ğŸ¤” [Judge] LLM åˆ¤æ–­: {result.decision}")
            logger.info(f"   ç†ç”±: {result.reason}")

            return {"judge_decision": decision}

        except Exception as e:
            logger.warning(f"âš ï¸  [Judge] LLM åˆ¤æ–­å¼‚å¸¸: {e}ï¼Œé»˜è®¤ç»§ç»­æ‰§è¡Œ")

    # 4. é»˜è®¤ç»§ç»­æ‰§è¡Œ
    logger.info("â¡ï¸  [Judge] ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥ â†’ CONTINUE")
    return {"judge_decision": "continue"}


def route_after_judge(state: StandardPlanExecuteState) -> str:
    """
    Judge åçš„è·¯ç”±å‡½æ•°ï¼ˆåªè¯»å– stateï¼Œä¸é‡å¤è°ƒç”¨ judge_nodeï¼‰
    """
    return state.get("judge_decision", "continue")


# =========================================================
# 7. Replanner èŠ‚ç‚¹ï¼ˆå¼‚æ­¥ï¼‰
# =========================================================

async def replanner_node(
    state: StandardPlanExecuteState,
    config: PlanExecuteConfig = default_config
) -> Dict[str, Any]:
    """
    é‡æ–°è§„åˆ’ï¼ˆä¿ç•™æˆåŠŸæ­¥éª¤çš„å®Œæ•´ä¿¡æ¯ï¼‰
    """
    logger.info(f"ğŸ”„ [Replanner] å¼€å§‹é‡æ–°è§„åˆ’ï¼ˆç¬¬ {state['replan_count'] + 1} æ¬¡ï¼‰...")

    llm = get_llm(config, temperature=0.4).with_structured_output(ReplannerOutput)

    # è·å–æˆåŠŸçš„æ­¥éª¤
    completed_steps = [
        r for r in state["execution_history"]
        if r["status"] == "success"
    ]
    completed_step_ids = [r["step_id"] for r in completed_steps]

    # è·å–å¤±è´¥ä¿¡æ¯
    last_failed = next(
        (r for r in reversed(state["execution_history"]) if r["status"] == "failed"),
        None
    )
    failure_info = (
        f"æ­¥éª¤ [{last_failed['step_id']}] å¤±è´¥: {last_failed.get('error_message', 'æœªçŸ¥é”™è¯¯')}"
        if last_failed else "æœªçŸ¥å¤±è´¥"
    )

    prompt = REPLANNER_PROMPT.format(
        objective=state["original_input"],
        plan_version=state["plan_version"],
        old_plan=format_plan_overview(state["current_plan"]),
        completed_steps="\n".join([
            f"[{r['step_id']}]: {r['output'][:100]}..."
            for r in completed_steps
        ]) or "æ— å·²å®Œæˆæ­¥éª¤",
        failure_info=failure_info,
        shared_context=json.dumps(state["shared_context"], ensure_ascii=False, indent=2)
    )

    try:
        result = await llm.ainvoke([HumanMessage(content=prompt)])

        # ä¿ç•™å¯å¤ç”¨æ­¥éª¤çš„æ‰§è¡Œè®°å½•
        retained_history = [
            r for r in state["execution_history"]
            if r["step_id"] in result.reuse_steps
        ]

        # æ„å»ºæ–°è®¡åˆ’ï¼šå¤ç”¨çš„æ—§æ­¥éª¤ + æ–°æ­¥éª¤
        # ä»æ—§è®¡åˆ’ä¸­æå–è¢«å¤ç”¨æ­¥éª¤çš„å®Œæ•´å®šä¹‰
        reused_step_defs = [
            step for step in state["current_plan"]
            if step["step_id"] in result.reuse_steps
        ]

        # æ–°æ­¥éª¤è½¬æ¢ä¸º dict
        new_step_defs = [step.model_dump() for step in result.new_steps]

        # åˆå¹¶è®¡åˆ’
        new_plan = reused_step_defs + new_step_defs

        logger.info(f"âœ… [Replanner] æ–°è®¡åˆ’å·²ç”Ÿæˆ:")
        logger.info(f"   - å¤ç”¨æ­¥éª¤: {', '.join(result.reuse_steps) or 'æ— '}")
        logger.info(f"   - æ–°å¢æ­¥éª¤: {len(new_step_defs)} ä¸ª")
        logger.info(f"   - è°ƒæ•´è¯´æ˜: {result.adjustment_summary}")

        return {
            "current_plan": new_plan,
            "plan_version": state["plan_version"] + 1,
            "execution_history": retained_history,
            "current_step_index": len(retained_history),
            "replan_count": state["replan_count"] + 1,
            "replan_reason": result.adjustment_summary,
            "last_execution_status": "success",
            "judge_decision": ""  # æ¸…ç©ºåˆ¤æ–­ç»“æœ
        }

    except Exception as e:
        logger.error(f"âŒ [Replanner] é‡æ–°è§„åˆ’å¤±è´¥: {e}")
        # å¦‚æœ Replan å¤±è´¥ï¼Œç›´æ¥ç»“æŸ
        return {
            "current_step_index": len(state["current_plan"]),
            "judge_decision": "end"
        }


# =========================================================
# 8. Finalizer èŠ‚ç‚¹ï¼ˆå¼‚æ­¥ + è®°å¿†å­˜å‚¨ï¼‰
# =========================================================

async def finalizer_node(
    state: StandardPlanExecuteState,
    config: PlanExecuteConfig = default_config,
    memory: Optional[AgentMemory] = None
) -> Dict[str, Any]:
    """
    æ±‡æ€»æœ€ç»ˆç»“æœå¹¶å­˜å‚¨æ‰§è¡Œç»éªŒ
    """
    logger.info("ğŸ“ [Finalizer] æ±‡æ€»æœ€ç»ˆç»“æœ...")

    llm = get_llm(config, temperature=0.3)

    # æ ¼å¼åŒ–æ‰§è¡Œå†å²
    history_text = []
    for i, record in enumerate(state["execution_history"], 1):
        status_icon = "âœ…" if record["status"] == "success" else "âŒ"
        history_text.append(f"{status_icon} Step {i} [{record['step_id']}]:")
        history_text.append(record["output"])
        if record.get("tool_calls"):
            for tc in record["tool_calls"]:
                history_text.append(f"   ğŸ”§ å·¥å…·è°ƒç”¨: {tc['tool_name']}")
        history_text.append("")

    prompt = FINALIZER_PROMPT.format(
        objective=state["original_input"],
        execution_history="\n".join(history_text)
    )

    result = await llm.ainvoke([HumanMessage(content=prompt)])

    # è®¡ç®—å…ƒæ•°æ®
    successful_steps = len([r for r in state["execution_history"] if r["status"] == "success"])
    failed_steps = len([r for r in state["execution_history"] if r["status"] == "failed"])
    total_steps = len(state["execution_history"])
    success = failed_steps == 0

    metadata = {
        "total_steps": total_steps,
        "successful_steps": successful_steps,
        "failed_steps": failed_steps,
        "replan_count": state["replan_count"],
        "plan_version": state["plan_version"],
        "success": success
    }

    # å­˜å‚¨æ‰§è¡Œç»éªŒåˆ°è®°å¿†ç³»ç»Ÿ
    if memory and memory.is_available:
        try:
            # ç”Ÿæˆç»éªŒæ€»ç»“
            summary_prompt = EXPERIENCE_SUMMARY_PROMPT.format(
                objective=state["original_input"],
                plan_summary=format_plan_overview(state["current_plan"])[:500],
                execution_result=result.content[:500],
                success="æˆåŠŸ" if success else "å¤±è´¥"
            )
            summary_result = await llm.ainvoke([HumanMessage(content=summary_prompt)])

            await memory.store_execution_result(
                session_id=state.get("session_id", "default"),
                objective=state["original_input"],
                plan_summary=format_plan_overview(state["current_plan"])[:200],
                success=success,
                key_insights=summary_result.content
            )
            logger.info("ğŸ’¾ [Finalizer] æ‰§è¡Œç»éªŒå·²å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ")

        except Exception as e:
            logger.warning(f"âš ï¸  [Finalizer] å­˜å‚¨æ‰§è¡Œç»éªŒå¤±è´¥: {e}")

    logger.info("âœ… [Finalizer] ç»“æœæ±‡æ€»å®Œæˆ")

    return {
        "final_response": result.content,
        "metadata": metadata
    }


# =========================================================
# 9. æ„å»º LangGraphï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
# =========================================================

def create_standard_plan_execute_graph(
    config: PlanExecuteConfig = default_config,
    memory: Optional[AgentMemory] = None
):
    """
    åˆ›å»ºæ ‡å‡† Plan-and-Execute å·¥ä½œæµï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    """
    graph = StateGraph(StandardPlanExecuteState)

    # åˆ›å»ºç»‘å®šé…ç½®çš„èŠ‚ç‚¹å‡½æ•°
    async def _planner(state):
        return await planner_node(state, config)

    async def _executor(state):
        return await executor_node(state, config)

    async def _judge(state):
        return await judge_node(state, config)

    async def _replanner(state):
        return await replanner_node(state, config)

    async def _finalizer(state):
        return await finalizer_node(state, config, memory)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("planner", _planner)
    graph.add_node("executor", _executor)
    graph.add_node("judge", _judge)
    graph.add_node("replanner", _replanner)
    graph.add_node("finalizer", _finalizer)

    # å®šä¹‰è¾¹
    graph.add_edge("__start__", "planner")
    graph.add_edge("planner", "executor")
    graph.add_edge("executor", "judge")

    # Judge çš„æ¡ä»¶åˆ†æ”¯ï¼ˆä¿®å¤ï¼šä½¿ç”¨è·¯ç”±å‡½æ•°è€Œéé‡å¤è°ƒç”¨ judge_nodeï¼‰
    graph.add_conditional_edges(
        "judge",
        route_after_judge,
        {
            "continue": "executor",
            "replan": "replanner",
            "end": "finalizer"
        }
    )

    # Replanner â†’ Executor
    graph.add_edge("replanner", "executor")

    # Finalizer â†’ END
    graph.add_edge("finalizer", END)

    return graph.compile()


# =========================================================
# 10. é«˜çº§ API
# =========================================================

class StandardPlanExecuteAgent:
    """
    æ ‡å‡† Plan-and-Execute Agent å°è£…ç±»

    æä¾›ç®€æ´çš„ API æ¥è¿è¡Œ Agent
    """

    def __init__(self, config: PlanExecuteConfig = default_config):
        self.config = config
        self.memory: Optional[AgentMemory] = None
        self.tools: List[BaseTool] = []
        self.tools_description: str = "æ— å¯ç”¨å·¥å…·"
        self._graph = None
        self._initialized = False

    async def initialize(self) -> "StandardPlanExecuteAgent":
        """
        å¼‚æ­¥åˆå§‹åŒ– Agentï¼ˆåŠ è½½å·¥å…·å’Œè®°å¿†ç³»ç»Ÿï¼‰
        """
        if self._initialized:
            return self

        # åŠ è½½ MCP å·¥å…·
        self.tools, self.tools_description = await load_mcp_tools(self.config)

        # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        self.memory = await create_memory(self.config)

        # åˆ›å»ºå›¾
        self._graph = create_standard_plan_execute_graph(self.config, self.memory)

        self._initialized = True
        logger.info("ğŸš€ StandardPlanExecuteAgent åˆå§‹åŒ–å®Œæˆ")

        return self

    async def run(
        self,
        user_input: str,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è¿è¡Œ Agent

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„ç›®æ ‡
            session_id: ä¼šè¯ IDï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰

        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        if not self._initialized:
            await self.initialize()

        session_id = session_id or generate_session_id(user_input)

        # è·å–ç›¸å…³è®°å¿†
        memory_context = "æ— ç›¸å…³å†å²è®°å¿†"
        if self.memory and self.memory.is_available:
            memory_context = await self.memory.get_relevant_context(
                user_input,
                session_id
            )

        # æ„é€ åˆå§‹çŠ¶æ€
        initial_state: StandardPlanExecuteState = {
            "original_input": user_input,
            "session_id": session_id,
            "current_plan": [],
            "plan_version": 0,
            "execution_history": [],
            "current_step_index": 0,
            "tools": self.tools,
            "tools_description": self.tools_description,
            "shared_context": {},
            "memory_context": memory_context,
            "compressed_history": "",
            "last_execution_status": "success",
            "judge_decision": "",
            "replan_reason": None,
            "replan_count": 0,
            "final_response": "",
            "metadata": {}
        }

        logger.info("=" * 60)
        logger.info("ğŸš€ æ ‡å‡† Plan-and-Execute Agent å¯åŠ¨")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Œ ç”¨æˆ·ç›®æ ‡: {user_input}")
        logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·: {len(self.tools)} ä¸ª")
        logger.info(f"ğŸ’¾ è®°å¿†ç³»ç»Ÿ: {'å¯ç”¨' if self.memory and self.memory.is_available else 'ä¸å¯ç”¨'}")

        # è¿è¡Œå›¾
        result = await self._graph.ainvoke(
            initial_state,
            config={"recursion_limit": self.config.recursion_limit}
        )

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 60)

        return result


# =========================================================
# 11. ä¾¿æ·å‡½æ•°
# =========================================================

async def run_plan_execute(
    user_input: str,
    config: Optional[PlanExecuteConfig] = None
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¿è¡Œ Plan-and-Execute Agent
    """
    agent = StandardPlanExecuteAgent(config or default_config)
    return await agent.run(user_input)


# =========================================================
# 12. è¿è¡Œç¤ºä¾‹
# =========================================================

if __name__ == "__main__":
    import asyncio

    async def main():
        user_input = "åˆ†æ Python å’Œ Go è¯­è¨€åœ¨å¹¶å‘ç¼–ç¨‹æ–¹é¢çš„ä¼˜åŠ£ï¼Œå¹¶ç»™å‡ºé€‰å‹å»ºè®®"

        agent = StandardPlanExecuteAgent()
        result = await agent.run(user_input)

        print("\n" + "=" * 60)
        print("ğŸ‰ æœ€ç»ˆç»“æœ")
        print("=" * 60)
        print(result["final_response"])

        print("\n" + "=" * 60)
        print("ğŸ“Š æ‰§è¡Œç»Ÿè®¡")
        print("=" * 60)
        for key, value in result["metadata"].items():
            print(f"  {key}: {value}")

    asyncio.run(main())
