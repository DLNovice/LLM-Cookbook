OpenAI 姚顺雨在25年4月发表的论文《The Second Half》中，宣告AI主线程已经进入了下半场。

上半场以模型和方法为中心，核心构建了模型和训练方法。下半场的重点转向如何定义有现实意义的任务、如何有效评估AI系统的表现。



### 招聘要求

AI云原生测试开发工程师

- 输出**测试用例和测试计划**，对测试任务进行合理规划并保证有效落地执行，通过自动化用例设计和实现、工具开发等，提升测试效率和标准化；
- 参与大模型、AI相关产品**端到端客户场景测试**（训练、推理、微调、量化、Agent开发等），对模型性能、准确率等进行测试评价，输出**体系化的评价标准**，并构建自动化测试能力。
- 有较强的自动化用例设计能力，能够熟练使用至少**一种自动化框架**（如Robot Framework、WebdriverIO、Citrus、Cypress、Selenium、Ginkgo/Gomage、Pytest等）；
- 在RAG、多模态Agent、Function call上的**评估能力前沿进展，并实现系统化评估能力建设**
- Agent实验及**评测体系**、**Agent Tracing**等能力建设



Agent评测产品经理

- **Agent能力评测体系设计**：拆解智能体核心能力（意图理解与澄清、多轮规划、工具调用、环境反馈、指令遵循），设计分层评测方案，建立端到端产品评测框架。 基于Prompt Engineering等调整打分策略，优化评测置信度与场景适配性。
- **评测集全生命周期管理**：主导高复杂度评测集设计（如多智能体协作、长程规划任务），构建覆盖通用场景化和重点专项场景；设计自动化数据筛选与合成策略，提升数据规模与覆盖度。
- **标注规范设计与团队搭建**：建立智能体人工评测标注规范（如规划步骤合理性、工具调用准确性），保障数据质量。 搭建标注人员能力矩阵模型，扩充团队，实施分层培训与动态考核。
- **评测结果分析与方案迭代**：基于评测结果，监控端到端指标与过程指标，输出归因分析报告。 基于评测结果反哺训练方案、训练数据迭代，建立发现问题到解决问题的闭环机制。
- 定义行业评测标准：主导构建覆盖"认知-决策-执行"全链路的通用智能体评测体系。



### 评测指标

之前写过一些，忘记写在哪个文件夹下了，以下是新收集的一些。



**请介绍几个目前行业内广泛使用的 LLM 综合性基准测试，并说明它们各自的侧重点。（例如：MMLU, Big-Bench, HumanEval）**

- **参考答案：** 为了更全面地评估LLM的能力，学术界和工业界开发了许多综合性基准测试。其中，MMLU、Big-Bench和HumanEval是最具代表性的几个，它们各自有不同的侧重点：

  1. **MMLU (Massive Multitask Language Understanding)**
     - **侧重点：** **知识的广度与学科问题解决能力**。
     - **简介：** MMLU是一个大规模的多任务测试集，旨在衡量模型在各种学科领域的知识水平。它包含57个不同的科目，涵盖了从初等数学、美国历史、计算机科学到专业级别的法律、市场营销和医学等。
     - **形式：** 所有问题都是**四选一的单项选择题**。
     - **评估目的：** 检验模型是否具备渊博的、跨学科的知识储备和应用这些知识解决问题的能力。一个在MMLU上得分高的模型，通常被认为是一个“知识渊博”的模型。
  2. **Big-Bench (Beyond the Imitation Game Benchmark)**
     - **侧重点：** **探索LLM的能力边界和未来潜力**。
     - **简介：** Big-Bench是一个由社区协作创建的、极其多样化的基准，包含了超过200个任务。这些任务被设计得非常有挑战性，旨在测试当前LLM难以解决的能力，如常识推理、逻辑、物理直觉、创造性任务等。
     - **形式：** 任务形式非常多样，包括选择题、生成题、比较题等。
     - **评估目的：** Big-Bench的目标是“预测未来”。它试图找到那些一旦模型规模或技术发展到某个临界点就可能“涌现”出的新能力。它衡量的是模型的**通用智能水平和前沿能力**。
  3. **HumanEval (Human-Labeled Evaluation)**
     - **侧重点：** **代码生成与编程能力**。
     - **简介：** HumanEval是一个由OpenAI创建的、专门用于评估代码生成能力的基准。它包含164个手写的编程问题，每个问题都提供了函数签名、文档字符串（docstring）、以及几个单元测试（unit tests）。
     - **形式：** 模型需要根据函数签名和文档字符串，生成完整的Python函数体。
     - **评估方法：** 采用 **pass@k** 指标。即模型生成k个代码样本，只要其中至少有一个能够通过所有的单元测试，就算通过。这衡量了模型**编写正确、可用代码**的能力。

  **其他重要基准：**

  - **GSM8K:** 专注于评估**小学水平的数学应用题**的推理能力，需要模型进行多步的思维链推理。
  - **ARC (AI2 Reasoning Challenge):** 专注于评估需要**科学常识和推理**的、有挑战性的选择题。
  - **HellaSwag:** 专注于评估**常识推理**，任务是选择一个最合理的句子来续写一个给定的情景。



（TODO）其他参考：[【闲聊AI】大模型的排行榜都在比什么？→MMLU、GPQA、Chiikawa、SWE、LiveCodeBench](https://www.bilibili.com/video/BV1zBndz4Eqq?spm_id_from=333.1245.0.0)



### 单Agent评估

#### 一些综述

> 参考：综述论文 [Survey on Evaluation of LLM-based Agents](https://arxiv.org/pdf/2503.16416) 的[解读](https://mp.weixin.qq.com/s/-23RzNZ36F7qTv3xEFYaWA)

一、将 LLM Agent 评测的方法分为四大维度：

- **智能体的基础能力（Agent Capabilities）**：
  - 规划与多步推理（Planning & Reasoning）
  - 工具调用与函数使用（Tool Use）
  - 自我反思与自我纠错（Self-Reflection）
  - 长期记忆（Memory）
- **特定应用领域的智能体（Application-Specific Agents）**：
  - 网页智能体（Web Agents）
  - 软件开发智能体（Software Engineering Agents）
  - 科学研究智能体（Scientific Agents）
  - 对话式智能体（Conversational Agents）
- **通用型智能体（Generalist Agents）**：
  - 综合多项能力，适用于多种复杂环境
- **开发与评测工具框架（Evaluation Frameworks）**：
  - 各类辅助开发者持续评测智能体性能的工具与平台



二、**基础能力评测**

- 规划与多步骤推理：
  - 评测主要关注的维度有：
    - **任务分解（Task Decomposition）**：将复杂问题逐步拆分。
    - **状态追踪（State Tracking）**：记录问题求解过程中的状态变化。
    - **自我纠错（Self-Correction）**：在出错时及时调整策略。
    - **因果推理（Causal Reasoning）**：理解行为与结果之间的因果关系。
  - 常见的评测集：略
  - **长远规划与动态适应能力测试**：略
- **工具调用与函数使用**：
  - 要点包括
    - **意图识别 (Intent Recognition)** ：Agent 需判断什么场合需要调用哪个工具，何时调用。
    - **工具选择 (Tool Selection)** ：一旦知道肯定要调用，Agent 必须能正确挑选对应功能的 API。
    - **参数匹配 (Slot Filling)** ：从用户输入里解析出需要的参数，赋予工具的调用函数。
    - **执行工具调用（Execution）** ：正确地调用工具。
    - **结果整合输出（Response Generation）** ：调用完成后，还要把输出转成可读的文本回复给用户，或继续内部推理。
- 自我反思：
  - 评测要点：
    - **感知新信息**：能否理解反馈。
    - **信念更新**：根据反馈调整想法。
    - **决策调整**：改变行动策略。
  - 评测思路：
    - **单步校验**：向 Agent 提供「你在上一步可能有错误」之类的反馈，看它能否更新推理并调整下一步输出。
    - **多轮对话反馈**：把原本的测试问题改成多轮版本，让外部系统或用户告诉 Agent 某些信息有矛盾，测试它如何进行自我修正。
- 记忆机制：
  - 常见评测方法：
    - **长文本阅读场景**：如 ReadAgent 对多段文档进行摘要与问答，评测在多次对话或分块阅读时的正确性。
    - **多回合信息集成**：如 LoCoMo 和 StreamBench，要求 Agent 能在多次调用和外部反馈之间积累记忆、逐渐改进表现。
    - **动作 - 状态跟踪**：常用于交互式游戏或仿真环境，评测对先前执行的动作、世界状态的正确掌控。如 MemGPT，评测多会话聊天中的记忆保持。



三、**特定领域的 Agent 评测**

有些 Agent 专门应用在特定领域，如 Web 导航 Agent、软件工程辅助、科学研究助手，以及客服对话系统。它们各自有不同的典型任务、环境模拟方式和评测指标。



**四、通用型 Agent 评测**

这种通才型 Agent 需要覆盖更广的能力评测，出现了一些「综合性」的 Benchmark：

- **GAIA**：侧重多模态理解、网页导航、工具调用；
- **Galileo’s Agent Leaderboard**：聚焦真实应用场景下的 API 调用与复杂指令执行；
- **AgentBench**：在一个统一环境里提供操作系统命令行、SQL 数据库、游戏和家政等多种任务，测试 Agent 的「跨领域」适应性；
- **OSWorld、OmniACT、AppWorld**：直接让 Agent 在真实或准真实的操作系统环境里执行多步操作，对其完整操作系统级能力做评测。

还有一些平台，如**TheAgentCompany**、**CRMArena**等，会模拟一个「企业内部」场景，让 Agent 扮演不同角色去解决软件开发或客户管理等综合任务。它们不仅评测 Agent 的多技能协作，也注重安全策略、操作顺序等。在此基础上，又有**Holistic Agent Leaderboard (HAL)** 等平台整合多项 Benchmark，形成「一个榜单统考」的模式。



五、评测框架

略



> 参考：[Agent评估「必知」的核心方法论和8个工具，揭示如何评估Agent？](https://mp.weixin.qq.com/s/kI-3zgtkQz9qNQN6N4JE4A)

四大核心能力：

- 规划与多步推理：Agent的思考框架
- 函数调用与工具使用：Agent的交互能力
- 自反思：Agent的自我进化能力
- 记忆：Agent的持久化基础

应用场景特定评估：从专业领域看Agent性能

- Web Agent
- 对话Agent
- 科研Agent
- 略

通用Agent评估：全能型助手的综合测试

- 略

Agent评估框架：开发者的必备工具箱

- LangSmith、Langfuse、Google Vertex AI等框架

当前趋势与未来方向：Agent评估的进化路径

- 趋势一：更加真实和具有挑战性的评估
- 趋势二：实时基准与持续更新



> 参考：[Agent评估体系：12种LangSmith评估方法详解](https://mp.weixin.qq.com/s/4DWfSzS-cT1Rr1ZsDYALzg)

有参考价值



> 参考：[吴恩达 - Evaluating AI Agents](https://www.bilibili.com/video/BV1AjPVe5E3M)

结合了一些架构图，对一些概念的描述较为清晰



#### 一些测试工具

[agent常用的评测数据集](https://mp.weixin.qq.com/s/lcx5Uwr-unZk3mF_z6PxSQ)：

| 数据集        | 语言 | 难度 | 数量           | 模态          | 领域                                                         | 评测方式 |
| :------------ | :--- | :--- | :------------- | :------------ | :----------------------------------------------------------- | :------- |
| GAIA          | 英文 | 高   | 166dev+300test | 多模态        | 涵盖个人日常任务，科学问题，以及通用信息查询                 | 可自动化 |
| BrowseComp    | 英文 | 高   | 1266           | 文本          | 多领域                                                       | 可自动化 |
| BrowseComp-ZH | 中文 | 高   | 289            | 文本          | 11个领域                                                     | 可自动化 |
| HLE           | 英文 | 高   | 2700+          | 13%多模态问题 | 数学、人文科学、自然科学等数十个不同的学科                   | 可自动化 |
| GPQA          | 英语 | 中   | 448道多选题    | 文本          | 生物学、物理学和化学                                         | 可自动化 |
| ScholarSearch | 英文 | 中   | 223            | 文本          | 覆盖Science & Engineering和Social Sciences & Humanities两大门类，共15个细分学科 | 可自动化 |

[Agent的三种测试方案](https://mp.weixin.qq.com/s/0FZrgFosHzzYFBRiV3ba2g)：

- AgentBeach：一个测试工具，包括八种测试环境（操作系统、数据库操作、知识图谱、卡牌对战、情景猜谜、居家场景、网络购物、网页浏览）
- ToolEmu：一个仿真框架，通过模拟多样化的工具集，检测 LLM-Base Agent 在各种场景下的表现，旨在自动化地发现真实世界中的故障场景，为 Agent 执行提供了一个高效的沙盒环境。
- Agent 执行轨迹评估：如果说 AgentBeach 是对基于大模型的 Agent 通用能力测试，则 Agent 执行轨迹评估（Agent Trajectory Evaluation）通过观察基于大模型的 Agent 在执行任务过程中所采取的一系列动作及其响应，来全面评价 Agent 的表现。这种方法用于评估 Agent 在解决问题时的逻辑和效率，以及它是否选择了正确的工具和步骤来完成任务。



[红杉中国 测试工具 xbench](https://mp.weixin.qq.com/s/FnWo3gd2hrPWjB2DL358ig)：

- xbench采用双轨评估体系，构建多维度测评数据集，旨在同时追踪模型的理论能力上限与Agent的实际落地价值。
- xbench采用长青评估（Evergreen Evaluation）机制，通过持续维护并动态更新测试内容，以确保时效性和相关性。
- 发布了一些核心评估集



### MAS评估

参考：[2025 AI Agent（多智能体系统）评估和优化指南](https://mp.weixin.qq.com/s/YUWgU_MfMHpRvh1gSleH4A)，**摘要**

1 **评估流程**：从数据集中取样本，输入到应用，获得输出，然后由评估器（可结合真实答案）对输出进行打分，从而完成对产品的评估。

2 **评估指标**：主要介绍了**任务成功率，正确的函数调用使用，协作指标等。**

3 **评估工具**：介绍了多种多Agent系统（MAS）的评估工具，包括：**DeepEval，LangSmith，MultiAgentBench**等**。**

4 **优化方法**：从工程和算法两个角度介绍了多 Agent 系统的优化方法。



4 月份 Hugging Face上热度最高的论文[《ADVANCES AND CHALLENGES IN FOUNDATION AGENTS》解读](https://mp.weixin.qq.com/s/HNREg8mtVrLoGn7NM-aegQ)：

- 核心： 多Agent系统的设计、协作与评估
- **多Agent系统设计**
- **多Agent间通信**
- **Agent协作范式与机制**
- **多Agent系统的评估**





### 其他

> 参考：[大模型面试题：使用大模型进行打分时为何要掉换位置多次打分呢？](https://mp.weixin.qq.com/s/-DEE6ibJ4eq6rcdMPC9RJw)

给定问题 Q 回答 A：…… 回答 B：…… 请判断哪个更好

大量实证发现：

- 排在前面的回答更容易被判为更好
- 即使两个回答质量接近，甚至后者更优，模型仍倾向选择前者

这不是偶然噪声，而是稳定、可复现的系统性偏差。

GPT 在评测时存在稳定、可复现的位置偏差，其根源来自自回归建模、训练先验和注意力非对称性；通过交换回答顺序并聚合判断，可以在统计意义上抵消该偏差，因此这是 LLM-as-a-judge 的标准做法，而非工程技巧。