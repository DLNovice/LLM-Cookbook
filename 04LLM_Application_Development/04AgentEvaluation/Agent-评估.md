> 参考：https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents
>
> 其他：[Anthropic 再发长文：首次详细揭秘Agent的评估全过程](https://mp.weixin.qq.com/s/XAagCdKLqj47z-9kwTaqHg)

Agent评估通常结合三种评分器：

1. **基于代码的评分器**： **方法**：字符串匹配、单元测试、静态分析、结果/工具调用验证、记录分析等。 **优点**：快速、廉价、客观、可复现、易于调试。 **缺点**：对有效变体可能过于脆弱，缺乏细微差别判断。
2. **基于模型的评分器**： **方法**：基于规则打分、自然语言断言、成对比较、多评委共识等。 **优点**：灵活、可扩展、能捕捉细微差别、处理开放式任务。 **缺点**：非确定性、更昂贵、需与人工评分器校准。
3. **人工评分器**： **方法**：领域专家审查、众包判断、抽样检查等。 **优点**：黄金标准质量，能匹配专家判断，用于校准模型评分器。 **缺点**：昂贵、缓慢、需要规模化专家。

评估策略与不同类型Agent的实践

- **能力评估 vs. 衰退评估**： 
  - **能力评估**：回答“Agent擅长做什么？”，初始通过率应较低。
  - **衰退评估**：回答“Agent是否还能处理好旧任务？”，通过率应接近100%。高通过率的能力评估可“毕业”为衰退评估套件。
- **针对不同类型Agent的评估方法**： 
  - **编码Agent（如Claude Code）**：侧重确定性评分（如单元测试），结合代码质量检查（LLM评分器）和工具调用验证。 
  - **对话Agent（如客服）**：评估交互质量本身，常需另一个LLM模拟用户，结合状态检查、记录约束和LLM评分规则。 
  - **研究Agent**：评估信息收集与综合，组合**Groundedness检查**（声明有来源）、**Coverage检查**（覆盖关键事实）、**Source quality检查**（信源权威性），LLM评分器需与人工频繁校准。 
  - **计算机使用Agent（如通过GUI交互）**：在真实或沙盒环境运行，需平衡token效率与延迟，检查最终环境状态（如URL、文件系统）。

从0到1构建评估的路线图

1. **第一阶段：收集初始评估数据集** 尽早开始（20-50个任务）。 从手动测试和真实失败案例（bug报告、用户反馈）中转化任务。 编写无歧义的任务和参考解决方案。 构建平衡的问题集（测试应发生和不应发生的行为）。
2. **第二阶段：设计评估框架和评分器** 
   - 构建稳健的评估框架和稳定的环境。
   -  深思熟虑设计评分器：**评判结果而非具体路径**；设计部分得分；校准LLM评分器；避免评分陷阱（如过度严格的任务描述）。
3. **第三阶段：长期维护和使用评估** **检查记录**：
   - 验证评估有效性，区分Agent真错还是评分器问题。 
   - **监控能力评估饱和度**：通过率接近100%时需开发更难的评估。 
   - **推行评估驱动开发**：先定义评估（期望能力），再迭代Agent。鼓励开放贡献和维护评估套件。



