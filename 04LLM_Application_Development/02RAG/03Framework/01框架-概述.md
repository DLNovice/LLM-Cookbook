目前LangGraph应该还是主流，lightrag等适合做案例。

## RAG常用开发框架

除了Dify、RagFlow，一些专用Rag开发框架

### Langchain

TODO



### LlamaIndex

官网：https://github.com/run-llama/llama_index

```
# custom selection of integrations to work with core
pip install llama-index-core
pip install llama-index-llms-openai
pip install llama-index-llms-replicate
pip install llama-index-embeddings-huggingface
pip install llama_index.llms.openrouter
```



#### 简易示例代码

```
import logging
import sys
import os

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
# from llama_index.llms.openai import OpenAI
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# # 配置日志，以便查看 LlamaIndex 的详细运行过程
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))

# 设置你的 OpenAI API 密钥
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY" # 请替换为你的实际 API 密钥
# with your actual OpenRouter API key.
os.environ["OPENROUTER_API_KEY"] = "sk-or-v1-***"


def main():
    # 1. 加载数据
    # SimpleDirectoryReader 会从指定目录加载所有文件
    documents = SimpleDirectoryReader("data").load_data()
    print(f"已加载 {len(documents)} 个文档。")

    llm = OpenRouter(model="google/gemini-2.0-flash-001")
    embed_model = HuggingFaceEmbedding(model_name="all-MiniLM-L6-v2")

    # 2. 创建索引
    # VectorStoreIndex 是 LlamaIndex 中常用的索引类型，它将文档转换为向量嵌入
    # 并存储在一个向量存储中，以便进行相似性搜索。
    index = VectorStoreIndex.from_documents(documents, llm=llm, embed_model=embed_model)
    print("索引已创建。")

    # 3. 创建查询引擎
    # 查询引擎允许你对索引进行自然语言查询。
    query_engine = index.as_query_engine(llm=llm)
    print("查询引擎已就绪。")

    # 4. 进行查询
    query = "LlamaIndex 是用来做什么的？"
    print(f"\n正在查询: '{query}'")
    response = query_engine.query(query)

    print("\n--- 回答 ---")
    print(response)

    query = "它支持哪些数据源？"
    print(f"\n正在查询: '{query}'")
    response = query_engine.query(query)

    print("\n--- 回答 ---")
    print(response)

if __name__ == "__main__":
    main()

```

示例文档：

```
LlamaIndex 是一个用于连接 LLM 和外部数据的框架。
它使开发者能够轻松地为 LLM 提供私有数据上下文。
LlamaIndex 支持多种数据源，包括文档、数据库和 API。
```

示例输出：

```
已加载 3 个文档。
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2
Load pretrained SentenceTransformer: all-MiniLM-L6-v2
索引已创建。
查询引擎已就绪。

正在查询: 'LlamaIndex 是用来做什么的？'
INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"

--- 回答 ---
LlamaIndex 是一个框架，旨在连接 LLM 和外部数据，使开发者能够轻松地为 LLM 提供私有数据上下文，并支持多种数据源，包括文档、数据库和 API。


正在查询: '它支持哪些数据源？'
INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"

--- 回答 ---
它支持多种数据源，包括文档、数据库和 API。
```



#### 结合ChromaDB示例

```
pip install llama-index openai chromadb
```

示例代码：

```
import logging
import sys
import os

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings # Import Settings

# # 配置日志，以便查看 LlamaIndex 的详细运行过程
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))

# 设置你的 OpenAI API 密钥
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY" # 请替换为你的实际 API 密钥
# with your actual OpenRouter API key.
os.environ["OPENROUTER_API_KEY"] = "sk-or-v1-***"

def main():
    # 1. 加载数据
    documents = SimpleDirectoryReader("data").load_data()
    print(f"已加载 {len(documents)} 个文档。")

    # 2. 初始化 ChromaDB 客户端
    # 这将在当前目录下创建一个名为 "chroma_db" 的文件夹来存储数据
    # 如果该目录不存在，ChromaDB 会自动创建它
    db = chromadb.PersistentClient(path="./chroma_db2")

    # 获取或创建 ChromaDB 集合
    # 集合是 ChromaDB 中存储向量和元数据的地方
    # 如果 'my_llama_collection' 集合不存在，它会被创建
    # 如果已经存在，你可以选择清空它或继续使用现有数据
    try:
        chroma_collection = db.get_or_create_collection("my_llama_collection")
        print(f"已连接到 ChromaDB 集合: '{chroma_collection.name}'")
    except Exception as e:
        print(f"连接或创建 ChromaDB 集合失败: {e}")
        sys.exit(1) # 退出程序如果无法连接数据库

    # 3. 创建 ChromaVectorStore 实例
    # 将 ChromaDB 集合传递给 LlamaIndex
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

    # 4. 创建 OpenAI LLM 和 Embedding 模型实例
    # 确保同时指定了 LLM 和嵌入模型，特别是如果你想使用特定的模型版本
    llm = OpenRouter(model="google/gemini-2.0-flash-001")
    embed_model = HuggingFaceEmbedding(model_name="all-MiniLM-L6-v2")

    Settings.llm = llm
    Settings.embed_model = embed_model

    # 5. 构建 LlamaIndex 索引
    # 这里的 `service_context` 参数是关键，它允许你指定 LlamaIndex 使用的
    # LLM、嵌入模型和向量存储等组件。
    # 从 LlamaIndex 0.10.0 版本开始，建议直接将 llm, embed_model 和 vector_store 传递给 VectorStoreIndex.from_documents
    index = VectorStoreIndex.from_documents(
        documents,
        llm=llm,
        embed_model=embed_model,
        vector_store=vector_store,
        show_progress=True # 显示构建索引的进度
    )
    print("索引已使用 ChromaDB 作为向量存储创建。")

    # 6. 创建查询引擎
    query_engine = index.as_query_engine()
    print("查询引擎已就绪。")

    # 7. 进行查询
    query1 = "LlamaIndex 的主要用途是什么？"
    print(f"\n正在查询: '{query1}'")
    response1 = query_engine.query(query1)
    print("\n--- 回答 ---")
    print(response1)

    query2 = "ChromaDB 在这里有什么作用？"
    print(f"\n正在查询: '{query2}'")
    response2 = query_engine.query(query2)
    print("\n--- 回答 ---")
    print(response2)

    # 你可以尝试关闭程序，再次运行，会发现数据仍然存在于 ChromaDB 中
    # 因为我们使用了 PersistentClient。
    print("\n查询完成。ChromaDB 数据已持久化存储在 './chroma_db2' 目录中。")

if __name__ == "__main__":
    main()
```

示例输出：

```
已加载 3 个文档。
INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
已连接到 ChromaDB 集合: 'my_llama_collection'
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2
Load pretrained SentenceTransformer: all-MiniLM-L6-v2
Parsing nodes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1814.67it/s]
Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.41it/s]
索引已使用 ChromaDB 作为向量存储创建。
查询引擎已就绪。

正在查询: 'LlamaIndex 的主要用途是什么？'
INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"

--- 回答 ---
LlamaIndex 主要用于连接 LLM 和外部数据，使开发者能够轻松地为 LLM 提供私有数据上下文。


正在查询: 'ChromaDB 在这里有什么作用？'
INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"

--- 回答 ---
Chroma is an open-source embedding database.


查询完成。ChromaDB 数据已持久化存储在 './chroma_db2' 目录中。
```



#### LlamaIndex通常如何接入Agent项目？

LlamaIndex自己内封了Agent框架

```
from llama_index.llms import OpenAI
from llama_index.agent import Tool
```

当然，也可以把相关的函数封装入FunctionCall或MCP，套个壳子而已，数据库的增删改查还是直接套用Langchain或者LlamaIndex或者数据库官方MCP。



### Milvus

`docker compose up -d`一键安装Milvus：

```
version: '3.5'
services:
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      ETCD_AUTO_COMPACTION_RETENTION: "1"
      ETCD_QUOTA_BACKEND_BYTES: "4294967296"
    volumes:
      - etcd_data:/etcd
    ports:
      - "2379:2379"

  milvus:
    image: milvusdb/milvus:v2.3.3
    command: ["milvus", "run", "standalone"]
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - etcd
    volumes:
      - milvus_data:/var/lib/milvus

volumes:
  etcd_data:
  milvus_data:

```

测试：略



环境配置：

```
pymilvus==2.4.4
transformers==4.41.1
torch
requests
sentence-transformers==2.7.0
```

项目结构：

```
rag_milvus_project/
├── main.py                  # 主程序
├── milvus_client.py         # 封装 Milvus 操作
├── embedding.py             # 文本向量化处理
├── llm_openrouter.py        # 调用 OpenRouter 接口
├── config.py                # 配置文件
├── requirements.txt
└── sample_docs.txt          # 示例文档
```



使用方法：

- 先向量化 + 建库：`python main.py build`
- 后提问（RAG 模式）：`python main.py`



milvus容器存在问题，导致案例暂时未运行成功，重新docker安装一下milvus：todo



### mem0

官网链接

- https://github.com/mem0ai/mem0
- https://docs.mem0.ai/integrations/langgraph

Mem0: Building Production- Ready AI Agents with Scalable Long-Term Memory



示例教程：

- https://www.cnblogs.com/xiaoqi/p/18315502/mem0
- https://zhuanlan.zhihu.com/p/710032476



### DSPy 