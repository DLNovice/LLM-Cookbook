## LLM常用向量数据库

下面是几个常见数据库，star数都很可观：

- ChromaDB
- Pinecone
- Faiss
- Weaviate
- Qdrant



### ChromaDB

安装：

- 直接 pip 安装即可运行（默认本地模式）：
  - 这会启动一个本地的嵌入式数据库（DuckDB + Parquet 存储），对于绝大多数**个人项目、小型应用或开发测试**场景
  - 限制：
    - **单线程**、不适合高并发写入
    - 无 REST API（只能通过 Python 客户端调用）
    - 部署在云端、集群环境下可扩展性较差
- 使用 Docker 部署 Chroma 服务器：
  - Chroma 提供了服务端版本，可通过 Docker 运行（带 REST API），适合需要**远程访问、跨进程/跨语言支持、多用户部署**的场景。

| 方式           | pip 安装（本地模式）            | Docker 服务端模式            |
| -------------- | ------------------------------- | ---------------------------- |
| 安装复杂度     | ✅ 简单                          | ⚠️ 需要 Docker                |
| 启动方式       | 本地 Python 直接运行            | 启动容器服务，提供 REST API  |
| 适合场景       | 本地测试、小规模应用            | 多语言访问、远程部署、服务化 |
| 并发能力       | 低（单进程）                    | 更强，支持多用户/多进程      |
| API 接口       | 仅限 Python                     | 提供 HTTP REST API           |
| 数据持久化支持 | ✅（通过设置 persist_directory） | ✅（挂载 volume 即可）        |
| 生产环境推荐   | ❌（仅原型/开发）                | ✅（适合服务部署）            |

环境配置：

```python
pip install chromadb openai tiktoken
```

示例代码：

```python
import chromadb


# setup Chroma in-memory, for easy prototyping. Can add persistence easily!
# 客户端代码
client = chromadb.Client()

# Create collection. get_collection, get_or_create_collection, delete_collection also available!
# 创建实例：collection（数据库名称）是您存储嵌入、文档和任何其他元数据的地方。您可以创建一个具有以下名称的集合（相当于关系数据库mysql里面的数据库名称）
collection = client.create_collection("all-my-documents")

# Add docs to the collection. Can also update and delete. Row-based API coming soon!
collection.add(
    documents=["This is document1", "This is document2"], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well
    metadatas=[{"source": "notion"}, {"source": "google-docs"}], # filter on these!
    ids=["doc1", "doc2"], # unique for each doc
)

# Query/search 2 most similar results. You can also .get by id
results = collection.query(
    query_texts=["This is a query document"],
    n_results=2,
    # where={"metadata_field": "is_equal_to_this"}, # optional filter
    # where_document={"$contains":"search_string"}  # optional filter
)

print(results)


# 本地持久化

# 增删改查

```

示例输出：

```python
{'ids': [['doc1', 'doc2']], 'embeddings': None, 'documents': [['This is document1', 'This is document2']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'source': 'notion'}, {'source': 'google-docs'}]], 'distances': [[0.9026353359222412, 1.035815954208374]]}
```



结合langchain：

```python
pip install -U langchain
pip install -U langchain-community
pip install langchain_huggingface
pip install langchain_openai
```

示例代码：

```python
from langchain_community.document_loaders import TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import OpenAI
from langchain_openai import ChatOpenAI

import os

# ✅ 使用 OpenRouter 的配置
os.environ["OPENAI_API_KEY"] = "sk-or-v1-***"
os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"

# ✅ 加载文档
loader = TextLoader("data/docs2.txt")
documents = loader.load()

# ✅ 使用 huggingface 本地模型做 embedding
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# ✅ 构建 Chroma 向量库（文档自动持久化，无需 persist）
vectorstore = Chroma.from_documents(documents, embedding_model, persist_directory="chroma_db")
vectorstore.persist()  # 清空数据库中以前所有已存数据

# ✅ 获取检索器
retriever = vectorstore.as_retriever()

# ✅ 创建 LLM 实例，使用 OpenRouter 提供的模型
# llm = OpenAI(
#     model="openai/gpt-4o-mini",  # 你也可以用 "anthropic/claude-3-haiku" 等
#     temperature=0,
# )

llm = ChatOpenAI(
    model="openai/gpt-4o-mini",
    api_key="sk-or-v1-***",  # ✅ 明确传入
    base_url="https://openrouter.ai/api/v1",
    temperature=0
)

# ✅ 构建 RetrievalQA 链
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# ✅ 发起问题查询（注意用 invoke 替代 __call__）
query = "What is LangChain?"
result = qa_chain.invoke({"query": query})

# ✅ 打印结果
print("Answer:", result["result"])
print("Source documents:")
for doc in result["source_documents"]:
    print("-", doc.page_content)
print("############################")
print(result)

```

示例结果：

```python
Answer: LangChain is a framework for developing applications powered by language models.
Source documents:
- LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.

- LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.

- LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.

- LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.

############################
{'query': 'What is LangChain?', 'result': 'LangChain is a framework for developing applications powered by language models.', 'source_documents': [Document(metadata={'source': 'data/docs2.txt'}, page_content='LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.\n'), Document(metadata={'source': 'data/docs2.txt'}, page_content='LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.\n'), Document(metadata={'source': 'data/docs2.txt'}, page_content='LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.\n'), Document(metadata={'source': 'data/docs2.txt'}, page_content='LangChain is a framework for developing applications powered by language models. Chroma is an open-source embedding database.\n')]}
```

为什么召回结果中，同一个文档，同样的内容，被多次召回？

- 原因 1：**文档被切分成多个重叠 chunk**
- 原因 2：**Embedding 结果非常相似，距离计算难区分**
- 原因 3：**数据库中重复插入了文档**
- 原因 4：**召回数量设置太高**

解决方案很多，可行。



### Faiss

>简介：faiss是一个Facebook AI团队开源的库，全称为Facebook AI Similarity Search，该开源库针对高维空间中的海量数据（稠密向量），提供了高效且可靠的相似性聚类和检索方法，可支持十亿级别向量的搜索，是目前最为成熟的近似近邻搜索库。
>
>官方资源地址：https://github.com/facebookresearch/faiss

#### 环境配置

官方安装指南：https://github.com/facebookresearch/faiss/blob/main/INSTALL.md

环境配置（官方环境安装比较简略）：

```
conda install -c pytorch -c nvidia faiss-gpu=1.11.0
conda install pytorch torchvision torchaudio cudatoolkit -c pytorch
pip install -U sentence-transformers
```

剩余一些版本冲突问题：

```
# RuntimeError: operator torchvision::nms does not exist，即 torchvision 与 torch 版本不兼容，或者安装不完全导致的
pip install torch torchvision --force-reinstall

# 一些版本冲突问题
pip install "numpy<2.0"
pip install openai==0.28
```



示例代码：

```python
import faiss
import openai
import torch
import numpy as np
from sentence_transformers import SentenceTransformer


# 使用 GPU 加载嵌入模型
device = 'cuda' if torch.cuda.is_available() else 'cpu'
embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)  # 加载嵌入模型 'all-MiniLM-L6-v2'，并指定运行设备（GPU 或 CPU）

# 载入文档
with open('data/docs.txt', 'r', encoding='utf-8') as f:
    documents = [line.strip() for line in f if line.strip()]

# 嵌入向量（GPU 加速）
doc_embeddings = embedder.encode(documents, convert_to_numpy=True, device=device)  # 使用嵌入模型将所有文档转换为向量（嵌入），并转为 NumPy 格式

# 构建 FAISS GPU 索引
dimension = doc_embeddings.shape[1]  # 获取嵌入向量的维度（例如：384）
cpu_index = faiss.IndexFlatL2(dimension)  # 在 CPU 上创建一个 L2 距离的平面索引
gpu_res = faiss.StandardGpuResources()  # 初始化 GPU 资源管理器
gpu_index = faiss.index_cpu_to_gpu(gpu_res, 0, cpu_index)  # 将 CPU 索引迁移到 GPU 上以实现加速
gpu_index.add(doc_embeddings)  # 向 GPU 索引中添加文档嵌入向量

# 检索函数
def retrieve(query, top_k=2):
    query_vec = embedder.encode([query], convert_to_numpy=True, device=device)  # 对查询进行向量编码
    D, I = gpu_index.search(query_vec, top_k)  # 在 FAISS 索引中进行向量搜索
    return [documents[i] for i in I[0]]  # 根据索引返回对应的文档内容

# 生成回答（默认使用 OpenAI，也可替换为本地模型）
def generate_answer(query):
    context = "\n".join(retrieve(query))  # 通过检索函数获取与查询相关的上下文文档
    prompt = f"根据以下内容回答问题：\n\n{context}\n\n问题：{query}\n回答："

    # 设置 OpenRouter API
    openai.api_key = "sk-or-v1-***"
    openai.api_base = "https://openrouter.ai/api/v1"

    response = openai.ChatCompletion.create(
        model="google/gemini-2.0-flash-001",  # OpenRouter 上的模型名
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    return response.choices[0].message['content'].strip()  # 提取并返回模型生成的回答文本


# 主流程
if __name__ == "__main__":
    query = input("请输入你的问题：")
    answer = generate_answer(query)
    print("\n💡 回答：", answer)

```

示例效果：

```python
$ python rag_faiss_demo.py 
请输入你的问题：faiss

💡 回答： 是一个Facebook AI团队开源的库，全称为Facebook AI Similarity Search，该开源库针对高维空间中的海量数据（稠密向量），提供了高效且可靠的相似性聚类和检索方法，可支持十亿级别向量的搜索，是目前最为成熟的近似近邻搜索库。
```



#### 使用方法

faiss 三个最常用的索引是：IndexFlatL2, IndexIVFFlat,IndexIVFPQ

参考：https://cloud.tencent.com/developer/article/2425179



### Weaviate

参考：https://cloud.tencent.com/developer/article/2490861



# 如何选择向量库

之前有听说向量库虽然实现方法不同，但是从应用上来看大差不差，所以不同向量库一般不影响RAG效果。

但是这里还是想查查是否有些区别。



AWS：[Choosing an AWS vector database for RAG use cases](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/choosing-an-aws-vector-database-for-rag-use-cases/choosing-an-aws-vector-database-for-rag-use-cases.pdf)
Google：[Vertex AI RAG Engine 中的矢量数据库选项](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/vector-db-choices?hl=zh-cn)



其他：

- https://www.digitalocean.com/community/conceptual-articles/how-to-choose-the-right-vector-database
- https://www.aimon.ai/posts/comparison-of-vector-databases-for-retrieval-augmented-generation-rag-systems/



> 参考：https://zhuanlan.zhihu.com/p/29263667589

下面列出十个目前主流的向量数据库，展示其数据库链接、介绍、优点与缺点。根据开发者具体的使用场景和技术需求，选择最适合的向量数据库解决方案是关键。

| 数据库名称                           | 介绍                                                     | 优点                             | 缺点                                   | 主站链接      |
| ------------------------------------ | -------------------------------------------------------- | -------------------------------- | -------------------------------------- | ------------- |
| Pinecone                             | 专为机器学习应用设计的向量数据库，支持高效的相似性搜索。 | 高性能、易于集成、支持实时更新   | 价格较高，适合企业级应用               | Pinecone      |
| Weaviate                             | 开源的向量搜索引擎，支持语义搜索和混合搜索。             | 开源、支持多种数据格式、易于扩展 | 需要一定的技术背景进行部署和维护       | Weaviate      |
| Milvus                               | 开源的向量数据库，专注于AI和机器学习场景。               | 高性能、可扩展性强、社区活跃     | 部署和配置较为复杂                     | Milvus        |
| FAISS                                | Facebook开源的向量相似性搜索库，适用于大规模数据集。     | 高效、灵活、支持GPU加速          | 需要编程集成，不适合直接作为数据库使用 | FAISS         |
| Annoy                                | Spotify开源的近似最近邻搜索库，适用于高维向量搜索。      | 轻量级、易于使用、内存占用低     | 功能相对单一，适合小规模应用           | Annoy         |
| Vespa                                | 开源的全文搜索和向量搜索引擎，支持大规模数据。           | 支持多种搜索模式、可扩展性强     | 配置复杂，学习曲线较高                 | Vespa         |
| Qdrant                               | 开源的向量搜索引擎，支持过滤和混合搜索。                 | 易于使用、支持多种语言、性能优异 | 社区相对较小，文档较少                 | Qdrant        |
| Redis Vector Similarity Search (VSS) | Redis模块，支持向量相似性搜索。                          | 高性能、与Redis生态系统无缝集成  | 功能相对基础，适合简单场景             | Redis VSS     |
| Elasticsearch with Vector Search     | Elasticsearch的向量搜索插件，支持高维向量搜索。          | 强大的全文搜索能力、易于扩展     | 配置复杂，资源消耗较大                 | Elasticsearch |
| Zilliz                               | 基于Milvus的云原生向量数据库，提供托管服务。             | 高性能、易于使用、支持大规模数据 | 价格较高，适合企业级应用               | Zilliz        |

这些向量数据库各有特点，适用于不同的应用场景。根据具体需求选择合适的数据库可以提高开发效率和系统性能。而对于追求高性能和可扩展性的企业级应用，可以考虑 Milvus/Zilliz。FAISS 是适合对性能有极致要求、不要求持久化和数据管理的场景。Weaviate 在处理多模态数据方面表现突出，适用于需要管理多种数据类型（如图像、文本、音频等）的 AI 应用。如果需要无缝集成现有数据库并进行向量搜索，Elasticsearch、Redis 是理想的方案。而不希望管理基础设施的用户则可以选择 Pinecone 这样的全托管服务。



>https://blog.csdn.net/dengdeng333/article/details/145873345

| 数据库   | 开源性 | 分布式 | 索引类型        | 实时更新 | 适用场景           |
| -------- | ------ | ------ | --------------- | -------- | ------------------ |
| Pinecone | 否     | 是     | HNSW等（黑盒）  | 是       | 快速部署、实时应用 |
| Milvus   | 是     | 是     | HNSW、IVF、PQ等 | 是       | 大规模分布式系统   |
| Weaviate | 是     | 否     | HNSW            | 是       | 语义搜索、多模态   |
| Faiss    | 是     | 否     | IVF、PQ、HNSW等 | 否       | 本地高性能计算     |

![image-20250804131955449](./assets/image-20250804131955449.png)

![image-20250804132011315](./assets/image-20250804132011315.png)



>https://www.53ai.com/news/RAG/2025010902789.html

**Pinecone**：

- **特点**：云服务型向量数据库，易于扩展，支持在线部署。
- **适用场景**：适合需要快速部署和扩展的项目。

**Milvus**：

- **特点**：开源、高效、支持大规模数据存储与检索，适用于超大规模数据集。
- **适用场景**：适用于需要高吞吐量和低延迟的生产环境，如大型推荐系统和知识库检索。

**FAISS**：

- **特点**：由Facebook开发，适合小到中规模的数据集，支持多种索引算法。
- **适用场景**：适用于中小型项目或快速原型开发。

**Qdrant**：

- **特点**：支持高效向量搜索，具有良好的扩展性和灵活性。
- **适用场景**：适用于需要灵活部署和高度定制化的项目。

**Postgres pgvector**：

- **特点**：基于PostgreSQL的插件，支持SQL与向量搜索结合。
- **适用场景**：适合需要SQL查询支持的项目，特别是与现有PostgreSQL数据库集成的场景。

**Weaviate：**

- **特点：**丰富的元数据处理：高级过滤和混合搜索功能。灵活的数据模型设计。支持通过自定义模块实现额外功能。

- **适合场景：**处理文本、图像、视频、音频、代码或其他结构化或非结构化信息。
